# This file should contain all the record creation needed to seed the database with its default values.
# The data can then be loaded with the rails db:seed command (or created alongside the database with db:setup).
#
# Examples:
#
#   movies = Movie.create([{ name: 'Star Wars' }, { name: 'Lord of the Rings' }])
#   Character.create(name: 'Luke', movie: movies.first)

# require 'open-uri'

# demo_user = User.create(username: "Andy2", email: "andy2@email.com", password: "password2")

# demo_user.photo.attach(io: file, filename: 'dancer.jpg')

User.destroy_all
Article.destroy_all

articles = [
    {
    title: 'IT’S TIME TO BUILD',
    author:  'Marc Andreesen',
    url: 'a16z.com',
    full_url: 'https://a16z.com/2020/04/18/its-time-to-build/', 
    reading_time: '8 min',
    featured: false, 
    # cover_img: 'https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fi0.wp.com%2Fa16z.com%2Fwp-content%2Fuploads%2F2020%2F04%2Fbuildingfuture.jpg%3Ffit%3D600%252C440%26ssl%3D1%22)/https%3A%2F%2Fi0.wp.com%2Fa16z.com%2Fwp-content%2Fuploads%2F2020%2F04%2Fbuildingfuture.jpg%3Ffit%3D600%252C440%26ssl%3D1',
    content: 
        "<div class='article-content'>
            <p>Every Western institution was unprepared for the coronavirus pandemic, despite many prior warnings. This monumental failure of institutional effectiveness will reverberate for the rest of the decade, but it’s not too early to ask why, and what we need to do about it.</p>
            <p>Many of us would like to pin the cause on one political party or another, on one government or another. But the harsh reality is that it all failed — no Western country, or state, or city was prepared — and despite hard work and often extraordinary sacrifice by many people within these institutions. So the problem runs deeper than your favorite political opponent or your home nation.</p>
            <p>Part of the problem is clearly foresight, a failure of imagination. But the other part of the problem is what we didn’t *do* in advance, and what we’re failing to do now. And that is a failure of action, and specifically our widespread inability to *build*.</p>
            <p>We see this today with the things we urgently need but don’t have. We don’t have enough coronavirus tests, or test materials — including, amazingly, cotton swabs and common reagents. We don’t have enough ventilators, negative pressure rooms, and ICU beds. And we don’t have enough surgical masks, eye shields, and medical gowns — as I write this, New York City has put out a desperate call for rain ponchos to be used as medical gowns. Rain ponchos! In 2020! In America!</p>
            <p>We also don’t have therapies or a vaccine — despite, again, years of advance warning about bat-borne coronaviruses. Our scientists will hopefully invent therapies and a vaccine, but then we may not have the manufacturing factories required to scale their production. And even then, we’ll see if we can deploy therapies or a vaccine fast enough to matter — it took scientists 5 years to get regulatory testing approval for the new Ebola vaccine after that scourge’s 2014 outbreak, at the cost of many lives.</p>
            <p>In the U.S., we don’t even have the ability to get federal bailout money to the people and businesses that need it. Tens of millions of laid off workers and their families, and many millions of small businesses, are in serious trouble *right now*, and we have no direct method to transfer them money without potentially disastrous delays. A government that collects money from all its citizens and businesses each year has never built a system to distribute money to us when it’s needed most.</p>
            <p>Why do we not have these things? Medical equipment and financial conduits involve no rocket science whatsoever. At least therapies and vaccines are hard! Making masks and transferring money are not hard. We could have these things but we chose not to — specifically we chose not to have the mechanisms, the factories, the systems to make these things. We chose not to *build*.</p>
            <p>You don’t just see this smug complacency, this satisfaction with the status quo and the unwillingness to build, in the pandemic, or in healthcare generally. You see it throughout Western life, and specifically throughout American life.</p>
            <p>You see it in housing and the physical footprint of our cities. We can’t build nearly enough housing in our cities with surging economic potential — which results in crazily skyrocketing housing prices in places like San Francisco, making it nearly impossible for regular people to move in and take the jobs of the future. We also can’t build the cities themselves anymore. When the producers of HBO’s “Westworld” wanted to portray the American city of the future, they didn’t film in Seattle or Los Angeles or Austin — they went to Singapore. We should have gleaming skyscrapers and spectacular living environments in all our best cities at levels way beyond what we have now; where are they?</p>
            <p>You see it in education. We have top-end universities, yes, but with the capacity to teach only a microscopic percentage of the 4 million new 18 year olds in the U.S. each year, or the 120 million new 18 year olds in the world each year. Why not educate every 18 year old? Isn’t that the most important thing we can possibly do? Why not build a far larger number of universities, or scale the ones we have way up? The last major innovation in K-12 education was Montessori, which traces back to the 1960s; we’ve been doing education research that’s never reached practical deployment for 50 years since; why not build a lot more great K-12 schools using everything we now know? We know one-to-one tutoring can reliably increase education outcomes by two standard deviations (the Bloom two-sigma effect); we have the internet; why haven’t we built systems to match every young learner with an older tutor to dramatically improve student success?</p>
            <p>You see it in manufacturing. Contrary to conventional wisdom, American manufacturing output is higher than ever, but why has so much manufacturing been offshored to places with cheaper manual labor? We know how to build highly automated factories. We know the enormous number of higher paying jobs we would create to design and build and operate those factories. We know — and we’re experiencing right now! — the strategic problem of relying on offshore manufacturing of key goods. Why aren’t we building Elon Musk’s “alien dreadnoughts” — giant, gleaming, state of the art factories producing every conceivable kind of product, at the highest possible quality and lowest possible cost — all throughout our country?</p>
            <p>You see it in transportation. Where are the supersonic aircraft? Where are the millions of delivery drones? Where are the high speed trains, the soaring monorails, the hyperloops, and yes, the flying cars?</p>
            <p>Is the problem money? That seems hard to believe when we have the money to wage endless wars in the Middle East and repeatedly bail out incumbent banks, airlines, and carmakers. The federal government just passed a $2 trillion coronavirus rescue package in two weeks! Is the problem capitalism? I’m with Nicholas Stern when he says that capitalism is how we take care of people we don’t know — all of these fields are highly lucrative already and should be prime stomping grounds for capitalist investment, good both for the investor and the customers who are served. Is the problem technical competence? Clearly not, or we wouldn’t have the homes and skyscrapers, schools and hospitals, cars and trains, computers and smartphones, that we already have.</p>
            <p>The problem is desire. We need to *want* these things. The problem is inertia. We need to want these things more than we want to prevent these things. The problem is regulatory capture. We need to want new companies to build these things, even if incumbents don’t like it, even if only to force the incumbents to build these things. And the problem is will. We need to build these things.</p>
            <p>And we need to separate the imperative to build these things from ideology and politics. Both sides need to contribute to building.</p>
            <p>The right starts out in a more natural, albeit compromised, place. The right is generally pro production, but is too often corrupted by forces that hold back market-based competition and the building of things. The right must fight hard against crony capitalism, regulatory capture, ossified oligopolies, risk-inducing offshoring, and investor-friendly buybacks in lieu of customer-friendly (and, over a longer period of time, even more investor-friendly) innovation.</p>
            <p>It’s time for full-throated, unapologetic, uncompromised political support from the right for aggressive investment in new products, in new industries, in new factories, in new science, in big leaps forward.</p>
            <p>The left starts out with a stronger bias toward the public sector in many of these areas. To which I say, prove the superior model! Demonstrate that the public sector can build better hospitals, better schools, better transportation, better cities, better housing. Stop trying to protect the old, the entrenched, the irrelevant; commit the public sector fully to the future. Milton Friedman once said the great public sector mistake is to judge policies and programs by their intentions rather than their results. Instead of taking that as an insult, take it as a challenge — build new things and show the results!</p>
            <p>Show that new models of public sector healthcare can be inexpensive and effective — how about starting with the VA? When the next coronavirus comes along, blow us away! Even private universities like Harvard are lavished with public funding; why can’t 100,000 or 1 million students a year attend Harvard? Why shouldn’t regulators and taxpayers demand that Harvard build? Solve the climate crisis by building — energy experts say that all carbon-based electrical power generation on the planet could be replaced by a few thousand new zero-emission nuclear reactors, so let’s build those. Maybe we can start with 10 new reactors? Then 100? Then the rest?</p>
            <p>In fact, I think building is how we reboot the American dream. The things we build in huge quantities, like computers and TVs, drop rapidly in price. The things we don’t, like housing, schools, and hospitals, skyrocket in price. What’s the American dream? The opportunity to have a home of your own, and a family you can provide for. We need to break the rapidly escalating price curves for housing, education, and healthcare, to make sure that every American can realize the dream, and the only way to do that is to build.</p>
            <p>Building isn’t easy, or we’d already be doing all this. We need to demand more of our political leaders, of our CEOs, our entrepreneurs, our investors. We need to demand more of our culture, of our society. And we need to demand more from one another. We’re all necessary, and we can all contribute, to building.</p>
            <p>Every step of the way, to everyone around us, we should be asking the question, what are you building? What are you building directly, or helping other people to build, or teaching other people to build, or taking care of people who are building? If the work you’re doing isn’t either leading to something being built or taking care of people directly, we’ve failed you, and we need to get you into a position, an occupation, a career where you can contribute to building. There are always outstanding people in even the most broken systems — we need to get all the talent we can on the biggest problems we have, and on building the answers to those problems.</p>
            <p>I expect this essay to be the target of criticism. Here’s a modest proposal to my critics. Instead of attacking my ideas of what to build, conceive your own! What do you think we should build? There’s an excellent chance I’ll agree with you.</p>
            <p>Our nation and our civilization were built on production, on building. Our forefathers and foremothers built roads and trains, farms and factories, then the computer, the microchip, the smartphone, and uncounted thousands of other things that we now take for granted, that are all around us, that define our lives and provide for our well-being. There is only one way to honor their legacy and to create the future we want for our own children and grandchildren, and that’s to build.</p>
        </div>"
    },
    {
        title: 'The Lesson to Unlearn',
        author:  'Paul Graham',
        url: 'paulgraham.com',
        full_url: 'http://paulgraham.com/lesson.html', 
        reading_time: '18 min',
        featured: false, 
        # cover_img: 'pg.jpg'
        content: 
        "<div class='article-content'>
            <p>December 2019</p>
            <p>The most damaging thing you learned in school wasn't something you learned in any specific class. It was learning to get good grades.</p>
            <p>When I was in college, a particularly earnest philosophy grad student once told me that he never cared what grade he got in a class, only what he learned in it. This stuck in my mind because it was the only time I ever heard anyone say such a thing.</p>
            <p>For me, as for most students, the measurement of what I was learning completely dominated actual learning in college. I was fairly earnest; I was genuinely interested in most of the classes I took, and I worked hard. And yet I worked by far the hardest when I was studying for a test.</p>
            <p>In theory, tests are merely what their name implies: tests of what you've learned in the class. In theory you shouldn't have to prepare for a test in a class any more than you have to prepare for a blood test. In theory you learn from taking the class, from going to the lectures and doing the reading and/or assignments, and the test that comes afterward merely measures how well you learned.</p>
            <p>In practice, as almost everyone reading this will know, things are so different that hearing this explanation of how classes and tests are meant to work is like hearing the etymology of a word whose meaning has changed completely. In practice, the phrase \"studying for a test\" was almost redundant, because that was when one really studied. The difference between diligent and slack students was that the former studied hard for tests and the latter didn't. No one was pulling all-nighters two weeks into the semester.</p>
            <p>Even though I was a diligent student, almost all the work I did in school was aimed at getting a good grade on something.</p>
            <p>To many people, it would seem strange that the preceding sentence has a \"though\" in it. Aren't I merely stating a tautology? Isn't that what a diligent student is, a straight-A student? That's how deeply the conflation of learning with grades has infused our culture.</p>
            <p>Is it so bad if learning is conflated with grades? Yes, it is bad. And it wasn't till decades after college, when I was running Y Combinator, that I realized how bad it is.</p>
            <p>I knew of course when I was a student that studying for a test is far from identical with actual learning. At the very least, you don't retain knowledge you cram into your head the night before an exam. But the problem is worse than that. The real problem is that most tests don't come close to measuring what they're supposed to.</p>
            <p>If tests truly were tests of learning, things wouldn't be so bad. Getting good grades and learning would converge, just a little late. The problem is that nearly all tests given to students are terribly hackable. Most people who've gotten good grades know this, and know it so well they've ceased even to question it. You'll see when you realize how naive it sounds to act otherwise.</p>
            <p>Suppose you're taking a class on medieval history and the final exam is coming up. The final exam is supposed to be a test of your knowledge of medieval history, right? So if you have a couple days between now and the exam, surely the best way to spend the time, if you want to do well on the exam, is to read the best books you can find about medieval history. Then you'll know a lot about it, and do well on the exam.</p>
            <p>No, no, no, experienced students are saying to themselves. If you merely read good books on medieval history, most of the stuff you learned wouldn't be on the test. It's not good books you want to read, but the lecture notes and assigned reading in this class. And even most of that you can ignore, because you only have to worry about the sort of thing that could turn up as a test question. You're looking for sharply-defined chunks of information. If one of the assigned readings has an interesting digression on some subtle point, you can safely ignore that, because it's not the sort of thing that could be turned into a test question. But if the professor tells you that there were three underlying causes of the Schism of 1378, or three main consequences of the Black Death, you'd better know them. And whether they were in fact the causes or consequences is beside the point. For the purposes of this class they are.</p>
            <p>At a university there are often copies of old exams floating around, and these narrow still further what you have to learn. As well as learning what kind of questions this professor asks, you'll often get actual exam questions. Many professors re-use them. After teaching a class for 10 years, it would be hard not to, at least inadvertently.</p>
            <p>In some classes, your professor will have had some sort of political axe to grind, and if so you'll have to grind it too. The need for this varies. In classes in math or the hard sciences or engineering it's rarely necessary, but at the other end of the spectrum there are classes where you couldn't get a good grade without it.</p>
            <p>Getting a good grade in a class on x is so different from learning a lot about x that you have to choose one or the other, and you can't blame students if they choose grades. Everyone judges them by their grades graduate programs, employers, scholarships, even their own parents.</p>
            <p>I liked learning, and I really enjoyed some of the papers and programs I wrote in college. But did I ever, after turning in a paper in some class, sit down and write another just for fun? Of course not. I had things due in other classes. If it ever came to a choice of learning or grades, I chose grades. I hadn't come to college to do badly.</p>
            <p>Anyone who cares about getting good grades has to play this game, or they'll be surpassed by those who do. And at elite universities, that means nearly everyone, since someone who didn't care about getting good grades probably wouldn't be there in the first place. The result is that students compete to maximize the difference between learning and getting good grades.</p>
            <p>Why are tests so bad? More precisely, why are they so hackable? Any experienced programmer could answer that. How hackable is software whose author hasn't paid any attention to preventing it from being hacked? Usually it's as porous as a colander.</p>
            <p>Hackable is the default for any test imposed by an authority. The reason the tests you're given are so consistently bad so consistently far from measuring what they're supposed to measure is simply that the people creating them haven't made much effort to prevent them from being hacked.</p>
            <p>But you can't blame teachers if their tests are hackable. Their job is to teach, not to create unhackable tests. The real problem is grades, or more precisely, that grades have been overloaded. If grades were merely a way for teachers to tell students what they were doing right and wrong, like a coach giving advice to an athlete, students wouldn't be tempted to hack tests. But unfortunately after a certain age grades become more than advice. After a certain age, whenever you're being taught, you're usually also being judged.</p>
            <p>I've used college tests as an example, but those are actually the least hackable. All the tests most students take their whole lives are at least as bad, including, most spectacularly of all, the test that gets them into college. If getting into college were merely a matter of having the quality of one's mind measured by admissions officers the way scientists measure the mass of an object, we could tell teenage kids \"learn a lot\" and leave it at that. You can tell how bad college admissions are, as a test, from how unlike high school that sounds. In practice, the freakishly specific nature of the stuff ambitious kids have to do in high school is directly proportionate to the hackability of college admissions. The classes you don't care about that are mostly memorization, the random \"extracurricular activities\" you have to participate in to show you're \"well-rounded,\" the standardized tests as artificial as chess, the \"essay\" you have to write that's presumably meant to hit some very specific target, but you're not told what.</p>
            <p>As well as being bad in what it does to kids, this test is also bad in the sense of being very hackable. So hackable that whole industries have grown up to hack it. This is the explicit purpose of test-prep companies and admissions counsellors, but it's also a significant part of the function of private schools.</p>
            <p>Why is this particular test so hackable? I think because of what it's measuring. Although the popular story is that the way to get into a good college is to be really smart, admissions officers at elite colleges neither are, nor claim to be, looking only for that. What are they looking for? They're looking for people who are not simply smart, but admirable in some more general sense. And how is this more general admirableness measured? The admissions officers feel it. In other words, they accept who they like.</p>
            <p>So what college admissions is a test of is whether you suit the taste of some group of people. Well, of course a test like that is going to be hackable. And because it's both very hackable and there's (thought to be) a lot at stake, it's hacked like nothing else. That's why it distorts your life so much for so long.</p>
            <p>It's no wonder high school students often feel alienated. The shape of their lives is completely artificial.</p>
            <p>But wasting your time is not the worst thing the educational system does to you. The worst thing it does is to train you that the way to win is by hacking bad tests. This is a much subtler problem that I didn't recognize until I saw it happening to other people.</p>
            <p>When I started advising startup founders at Y Combinator, especially young ones, I was puzzled by the way they always seemed to make things overcomplicated. How, they would ask, do you raise money? What's the trick for making venture capitalists want to invest in you? The best way to make VCs want to invest in you, I would explain, is to actually be a good investment. Even if you could trick VCs into investing in a bad startup, you'd be tricking yourselves too. You're investing time in the same company you're asking them to invest money in. If it's not a good investment, why are you even doing it?</p>
            <p>Oh, they'd say, and then after a pause to digest this revelation, they'd ask: What makes a startup a good investment?</p>
            <p>So I would explain that what makes a startup promising, not just in the eyes of investors but in fact, is <a href=\"http://paulgraham.com/growth.html\"><u>growth</u></a>. Ideally in revenue, but failing that in usage. What they needed to do was get lots of users.</p>
            <p>How does one get lots of users? They had all kinds of ideas about that. They needed to do a big launch that would get them \"exposure.\" They needed influential people to talk about them. They even knew they needed to launch on a tuesday, because that's when one gets the most attention.</p>
            <p>No, I would explain, that is not how to get lots of users. The way you get lots of users is to make the product really great. Then people will not only use it but recommend it to their friends, so your growth will be exponential once you <a href=\"http://paulgraham.com/ds.html\"><u>get it started</u></a>.</p>
            <p>At this point I've told the founders something you'd think would be completely obvious: that they should make a good company by making a good product. And yet their reaction would be something like the reaction many physicists must have had when they first heard about the theory of relativity: a mixture of astonishment at its apparent genius, combined with a suspicion that anything so weird couldn't possibly be right. Ok, they would say, dutifully. And could you introduce us to such-and-such influential person? And remember, we want to launch on Tuesday.</p>
            <p>It would sometimes take founders years to grasp these simple lessons. And not because they were lazy or stupid. They just seemed blind to what was right in front of them.</p>
            <p>Why, I would ask myself, do they always make things so complicated? And then one day I realized this was not a rhetorical question.</p>
            <p>Why did founders tie themselves in knots doing the wrong things when the answer was right in front of them? Because that was what they'd been trained to do. Their education had taught them that the way to win was to hack the test. And without even telling them they were being trained to do this. The younger ones, the recent graduates, had never faced a non-artificial test. They thought this was just how the world worked: that the first thing you did, when facing any kind of challenge, was to figure out what the trick was for hacking the test. That's why the conversation would always start with how to raise money, because that read as the test. It came at the end of YC. It had numbers attached to it, and higher numbers seemed to be better. It must be the test.</p>
            <p>There are certainly big chunks of the world where the way to win is to hack the test. This phenomenon isn't limited to schools. And some people, either due to ideology or ignorance, claim that this is true of startups too. But it isn't. In fact, one of the most striking things about startups is the degree to which you win by simply doing good work. There are edge cases, as there are in anything, but in general you win by getting users, and what users care about is whether the product does what they want.</p>
            <p>Why did it take me so long to understand why founders made startups overcomplicated? Because I hadn't realized explicitly that schools train us to win by hacking bad tests. And not just them, but me! I'd been trained to hack bad tests too, and hadn't realized it till decades later.</p>
            <p>I had lived as if I realized it, but without knowing why. For example, I had avoided working for big companies. But if you'd asked why, I'd have said it was because they were bogus, or bureaucratic. Or just yuck. I never understood how much of my dislike of big companies was due to the fact that you win by hacking bad tests.</p>
            <p>Similarly, the fact that the tests were unhackable was a lot of what attracted me to startups. But again, I hadn't realized that explicitly.</p>
            <p>I had in effect achieved by successive approximations something that may have a closed-form solution. I had gradually undone my training in hacking bad tests without knowing I was doing it. Could someone coming out of school banish this demon just by knowing its name, and saying begone? It seems worth trying.</p>
            <p>Merely talking explicitly about this phenomenon is likely to make things better, because much of its power comes from the fact that we take it for granted. After you've noticed it, it seems the elephant in the room, but it's a pretty well camouflaged elephant. The phenomenon is so old, and so pervasive. And it's simply the result of neglect. No one meant things to be this way. This is just what happens when you combine learning with grades, competition, and the naive assumption of unhackability.</p>
            <p>It was mind-blowing to realize that two of the things I'd puzzled about the most the bogusness of high school, and the difficulty of getting founders to see the obvious both had the same cause. It's rare for such a big block to slide into place so late.</p>
            <p>Usually when that happens it has implications in a lot of different areas, and this case seems no exception. For example, it suggests both that education could be done better, and how you might fix it. But it also suggests a potential answer to the question all big companies seem to have: how can we be more like a startup? I'm not going to chase down all the implications now. What I want to focus on here is what it means for individuals.</p>
            <p>To start with, it means that most ambitious kids graduating from college have something they may want to unlearn. But it also changes how you look at the world. Instead of looking at all the different kinds of work people do and thinking of them vaguely as more or less appealing, you can now ask a very specific question that will sort them in an interesting way: to what extent do you win at this kind of work by hacking bad tests?</p>
            <p>It would help if there was a way to recognize bad tests quickly. Is there a pattern here? It turns out there is.</p>
            <p>Tests can be divided into two kinds: those that are imposed by authorities, and those that aren't. Tests that aren't imposed by authorities are inherently unhackable, in the sense that no one is claiming they're tests of anything more than they actually test. A football match, for example, is simply a test of who wins, not which team is better. You can tell that from the fact that commentators sometimes say afterward that the better team won. Whereas tests imposed by authorities are usually proxies for something else. A test in a class is supposed to measure not just how well you did on that particular test, but how much you learned in the class. While tests that aren't imposed by authorities are inherently unhackable, those imposed by authorities have to be made unhackable. Usually they aren't. So as a first approximation, bad tests are roughly equivalent to tests imposed by authorities.</p>
            <p>You might actually like to win by hacking bad tests. Presumably some people do. But I bet most people who find themselves doing this kind of work don't like it. They just take it for granted that this is how the world works, unless you want to drop out and be some kind of hippie artisan.</p>
            <p>I suspect many people implicitly assume that working in a field with bad tests is the price of making lots of money. But that, I can tell you, is false. It used to be true. In the mid-twentieth century, when the economy was <a href=\"http://paulgraham.com/re.html\"><u>composed of oligopolies</u></a>, the only way to the top was by playing their game. But it's not true now. There are now ways to get rich by doing good work, and that's part of the reason people are so much more excited about getting rich than they used to be. When I was a kid, you could either become an engineer and make cool things, or make lots of money by becoming an \"executive.\" Now you can make lots of money by making cool things.</p>
            <p>Hacking bad tests is becoming less important as the link between work and authority erodes. The erosion of that link is one of the most important trends happening now, and we see its effects in almost every kind of work people do. Startups are one of the most visible examples, but we see much the same thing in writing. Writers no longer have to submit to publishers and editors to reach readers; now they can go direct.</p>
            <p>The more I think about this question, the more optimistic I get. This seems one of those situations where we don't realize how much something was holding us back until it's eliminated. And I can foresee the whole bogus edifice crumbling. Imagine what happens as more and more people start to ask themselves if they want to win by hacking bad tests, and decide that they don't. The kinds of work where you win by hacking bad tests will be starved of talent, and the kinds where you win by doing good work will see an influx of the most ambitious people. And as hacking bad tests shrinks in importance, education will evolve to stop training us to do it. Imagine what the world could look like if that happened.</p>
            <p>This is not just a lesson for individuals to unlearn, but one for society to unlearn, and we'll be amazed at the energy that's liberated when we do.</p>
            <p></p>
            <p></p>
            <p></p>
            <p></p>
            <p></p>
            <p>Notes</p>
            <p>[1] If using tests only to measure learning sounds impossibly utopian, that is already the way things work at Lambda School. Lambda School doesn't have grades. You either graduate or you don't. The only purpose of tests is to decide at each stage of the curriculum whether you can continue to the next. So in effect the whole school is pass/fail.</p>
            <p>[2] If the final exam consisted of a long conversation with the professor, you could prepare for it by reading good books on medieval history. A lot of the hackability of tests in schools is due to the fact that the same test has to be given to large numbers of students.</p>
            <p>[3] Learning is the naive algorithm for getting good grades.</p>
            <p>[4] <a href=\"http://paulgraham.com/gba.html\"><u>Hacking</u></a> has multiple senses. There's a narrow sense in which it means to compromise something. That's the sense in which one hacks a bad test. But there's another, more general sense, meaning to find a surprising solution to a problem, often by thinking differently about it. Hacking in this sense is a wonderful thing. And indeed, some of the hacks people use on bad tests are impressively ingenious; the problem is not so much the hacking as that, because the tests are hackable, they don't test what they're meant to.</p>
            <p>[5] The people who pick startups at Y Combinator are similar to admissions officers, except that instead of being arbitrary, their acceptance criteria are trained by a very tight feedback loop. If you accept a bad startup or reject a good one, you will usually know it within a year or two at the latest, and often within a month.</p>
            <p>[6] I'm sure admissions officers are tired of reading applications from kids who seem to have no personality beyond being willing to seem however they're supposed to seem to get accepted. What they don't realize is that they are, in a sense, looking in a mirror. The lack of authenticity in the applicants is a reflection of the arbitrariness of the application process. A dictator might just as well complain about the lack of authenticity in the people around him.</p>
            <p>[7] By good work, I don't mean morally good, but good in the sense in which a good craftsman does good work.</p>
            <p>[8] There are borderline cases where it's hard to say which category a test falls in. For example, is raising venture capital like college admissions, or is it like selling to a customer?</p>
            <p>[9] Note that a good test is merely one that's unhackable. Good here doesn't mean morally good, but good in the sense of working well. The difference between fields with bad tests and good ones is not that the former are bad and the latter are good, but that the former are bogus and the latter aren't. But those two measures are not unrelated. As Tara Ploughman said, the path from good to evil goes through bogus.</p>
            <p>[10] People who think the recent increase in <a href=\"http://paulgraham.com/ineq.html\"><u>economic inequality</u></a> is due to changes in tax policy seem very naive to anyone with experience in startups. Different people are getting rich now than used to, and they're getting much richer than mere tax savings could make them.</p>
            <p>[11] Note to tiger parents: you may think you're training your kids to win, but if you're training them to win by hacking bad tests, you are, as parents so often do, training them to fight the last war.</p>
            <p></p>
            <p><b>Thanks</b> to Austen Allred, Trevor Blackwell, Patrick Collison, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this.</p>
            <p></p>
            </div>
        </div>"
    },
    {
        title: 'Psychology of Human Misjudgment',
        author:  'Neil Kakkar',
        url: 'neilkakkar.com',
        full_url: 'https://neilkakkar.com/Psychology-of-Human-Misjudgment.html', 
        reading_time: '15 min',
        featured: false, 
        # cover_img: 'https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fcharlie-munger.jpeg'
        content: 
            "<div>
            <p>Charlie Munger gave a <a rel=\"noopener\"
            href=\"https://fs.blog/2013/02/the-psychology-of-human-misjudgement/\">speech</a> to Harvard University in
            1995. It was about how he built his own curriculum for Psychology. He’s an ace investor - dealing with
            psychology everyday to make money in the market. However, this curriculum is not just about investing
            psychology. It’s about everything.</p>
            <h1>Key takeaway</h1>
            <p>There are 25 tendencies that humans display. These are useful for running ourselves in automatic mode. However,
            sometimes, they trip us up. It pays to know when one of your tendencies is acting against you - and when it’s
            acting for you.</p>
            <p>Keep your mind in order, and it will serve you well.</p>
            <p>Munger began with the theory of evolution.</p>
            <blockquote>
            <p>I was aware that man was a “social animal,” greatly and automatically influenced by behavior he observed in
            men around him. I also knew that man lived, like barnyard animals and monkeys, in limited size dominance
            hierarchies, wherein he tended to respect authority and to like and cooperate with his own hierarchy members
            while displaying considerable distrust and dislike for competing men not in his own hierarchy.</p>
            </blockquote>
            <p>However, this template wasn’t enough to explain the extreme irrationality surrounding him. Thus began his quest
            to cope with irrationality better. The search for his models to explain human behaviour gave birth to this essay
            - The Psychology of Human Misjudgment.</p>
            <blockquote>
            <p>“I saw this patterned irrationality, which was so extreme, and I had no theory to deal with it. So I created
            my own system of psychology.”</p>
            </blockquote>
            <p>Here are the tendencies.</p>
            <h1>Reward and Punishment super-response tendency</h1>
            <p>Incentives drive people. People work towards rewards, and away from punishments.</p>
            <p>FedEx faced a peculiar problem. They wanted to deliver packages on time. For this, they needed to load their
            airplanes before sunrise. The catch - they were never on time. The night shift workers were paid by the hour,
            and expected to work the entire 8 hour night shift. The idea was, if they spend enough time on the shift,
            they’ll get the job done. (As some managers still think - in places with fixed work hours)</p>
            <p>Finally, somebody got the idea that it was foolish to pay the night shift by the hour. The employer wanted rapid
            loading of a plane, not maximized billable hours of employee service. Maybe if they paid the employees per shift
            and let all night shift employees go home when all the planes were loaded, the system would work better. And, lo
            and behold, that solution worked.</p>
            <blockquote>
            <p>“If you would persuade, appeal to interest and not to reason.” - Ben Franklin</p>
            </blockquote>
            <p>Incentives are one of the better understood parts of Psychology. However, that doesn’t mean incentives explains
            everything. Focusing just on this because you understand it better can lead to ruin. This is <a rel=\"noopener\"
            href=\"https://en.wikipedia.org/wiki/Law_of_the_instrument\">the man with a hammer tendency</a> - To a man
            with just a hammer, everything looks like a nail.</p>
            <blockquote>
            <p>Skinner [a famed psychologist] lost most of his personal reputation (a) by overclaiming for incentive
            superpower to the point of thinking he could create a human utopia with it and (b) by displaying hardly any
            recognition of the power of the rest of psychology.</p>
            </blockquote>
            <h4>Incentive caused bias</h4>
            <p>Since incentives are so powerful, they end up biasing you and everyone around you.</p>
            <p>This means, you should distrust the advice of your professional advisor - specially when it’s good for the
            advisor. The general antidotes here are:</p>
            <blockquote>
            <p>(1) fear professional advice when it is good for the advisor; <br>
            (2) learn and use the basic elements of your advisor’s trade as you deal with your advisor; <br>
            (3) double check, disbelieve, or replace much of what you’re told, to the degree that seems appropriate
            after objective thought.</p>
            </blockquote>
            <p>The ubiquity of incentive-caused
            bias has vast consequences. For instance, a sales force living only on commissions will be much harder to keep
            moral than one under less pressure to earn their living.</p>
            <p>Things work the same way with punishments, although not as well as rewards.</p>
            <blockquote>
            <p>Around the time of Caesar, there was a European tribe that, when the assembly horn blew, always killed the
            last warrior to reach his assigned place, and no one enjoyed fighting this tribe.</p>
            </blockquote>
            <h1>Liking / Loving tendency</h1>
            <p>We tend to see people we like in a better light than they are.</p>
            <p>One consequence of Liking/ Loving Tendency is that it acts as a conditioning device that makes the liker or lover
            tend to -</p>
            <p>(1) ignore faults of and comply with wishes of, the object of his affection <br>
            (2) favor people, products, and actions merely associated with the object
            of his affection (<a
            href=\"https://neilkakkar.com/Psychology-of-Human-Misjudgment.html#influence-from-mere-association-tendency\">Influence
            from mere association tendency</a>) <br>
            (3) distort other facts to facilitate love.</p>
            <p>The phenomenon of liking and loving causing admiration also works in reverse. Admiration
            also causes or intensifies liking or love. With this “feedback mode” in place, the consequences are often
            extreme, sometimes even causing deliberate self-destruction to help what is loved.</p>
            <h1>Disliking / Hating tendency</h1>
            <p>We tend to see people we dislike in a worse light than they are.</p>
            <blockquote>
            <p>Distortion of that kind is often so extreme that miscognition is shockingly large. When the World Trade
            Center was destroyed, many Pakistanis immediately concluded that the Hindus did it, while many Muslims
            concluded that the Jews did it. Such factual distortions often make mediation between opponents locked in
            hatred either difficult or impossible.</p>
            </blockquote>
            <h1>Doubt Avoidance tendency</h1>
            <p>The brain of man is programmed to quickly remove doubt by reaching some decision.</p>
            <blockquote>
            <p>After all, the one thing that is surely counterproductive for a prey animal that is threatened by a predator
            is to take a long time in deciding what to do</p>
            </blockquote>
            <h1>Inconsistency Avoidance Tendency</h1>
            <p>Humans like to be consistent in everything they do, everything they say, and everything they are.</p>
            <blockquote>
            <p>Practically everyone has a great many bad habits he has long maintained despite their being known as bad</p>
            </blockquote>
            <blockquote>
            <p>“An ounce of prevention is worth a pound of cure.” -Ben Franklin</p>
            </blockquote>
            <p>Ben Franklin is indicating that Inconsistency-Avoidance Tendency makes it much easier to prevent a habit than to
            change it.</p>
            <p>A quickly reached conclusion, triggered by Doubt-Avoidance Tendency, when combined with a tendency to resist any
            change in that conclusion, will cause a lot of errors in cognition for modern man.</p>
            <p>This tendency, along with doubt avoidance work together against <a
            href=\"https://neilkakkar.com/A-framework-for-First-Principles-Thinking.html\">First Principles thinking.</a>
            </p>
            <blockquote>
            <p>“It was not the intrinsic difficulty of new ideas that prevented their acceptance. Instead, the new ideas
            were not accepted because they were inconsistent with old ideas in place” - Lord Keynes</p>
            </blockquote>
            <p>The antidote comes from Charles Darwin. He trained himself to intensively consider any evidence tending to
            disconfirm any hypothesis
            of his, more so if he thought his hypothesis was a particularly good one.</p>
            <blockquote>
            <p>“At his peak, Einstein was a great destroyer of his own ideas.”</p>
            </blockquote>
            <p>As a result, it is important not to put one’s brain in chains before one has come anywhere near his full
            potential as a rational person.</p>
            <blockquote>
            <p>“It’s very important to not put your brain in chains too young by what you shout out.”</p>
            </blockquote>
            <h1>Curiosity Tendency</h1>
            <p>Humans are curious. Those who can get into a culture which enhances curiosity will have several benefits.</p>
            <blockquote>
            <p>Curiosity, enhanced by the best of modern education, helps man to prevent or reduce bad consequences arising
            from other psychological tendencies. The curious are also provided with much fun and wisdom long after
            formal education has ended.</p>
            </blockquote>
            <h1>Kantian Fairness Tendency</h1>
            <p>Kant was famous for his <a rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Categorical_imperative\">categorical
            imperative</a>, a “golden rule” that required humans to follow behavior patterns that, if followed by all
            others, would make the surrounding human system work best for everybody.</p>
            <p>It’s like the “treat others like you wish to be treated” idiom.</p>
            <p>Most humans expect fairness and are fair in situations they expect fairness. This shows up in queueing up for
            something, first come first serve and letting lane switchers on a merging highway cut in front of you.</p>
            <h1>Envy and Jealousy Tendency</h1>
            <blockquote>
            <p>“A member of a species designed through evolutionary process to want often-scarce food is going to be driven
            strongly toward getting food when it first sees food. And this is going to occur often and tend to create
            some conflict when the food is seen in the possession of another member of the same
            species”</p>
            </blockquote>
            <h1>Reciprocation Tendency</h1>
            <p>The automatic tendency of humans to reciprocate both favors and disfavors is extreme, as it is in apes, monkeys,
            dogs, and many less cognitively gifted animals. The tendency facilitates group cooperation for the benefit of
            members.</p>
            <p>The standard antidote to one’s overactive hostility is to train oneself to defer reaction.</p>
            <blockquote>
            <p>“You can always tell the man off tomorrow if it is such a good idea.” - Tom Murphy</p>
            </blockquote>
            <p>Reciprocate-favor tendency operates at a subconscious level.</p>
            <p>When an automobile salesman graciously steers you into a comfortable place to sit and gives you a cup of coffee,
            you are very likely being tricked, by this small courtesy alone, into parting with an extra five hundred
            dollars.</p>
            <p>But here this might be balanced by your loss - the 500$ out of your pocket.</p>
            <p>But, if you’re a representative for a rich employer - you have no such downside. Now the minor favor you receive
            from the salesman
            is less opposed by the threat of extra cost to you because someone else is paying the extra cost. Under such
            circumstances, the salesman is often able to maximize his advantage, particularly when the government is the
            purchaser.</p>
            <p>The simplest antidote works best: Don’t let employees accept any favors from vendors.
            </p>
            <h1>Influence from Mere Association Tendency</h1>
            <p>A man buys a can of branded shoe polish, has a good experience with it when shining his shoes, and because of
            this “reward,” buys the same shoe polish when he needs another can.</p>
            <p>Advertisers know about the power of mere association. You won’t see Coke advertised alongside some account of the
            death of a child. Instead. Coke ads picture life as happier than reality.</p>
            <p>Some of the most important miscalculations come from what is accidentally associated with one’s past success, or
            one’s liking and loving, or one’s disliking and hating, which includes a natural hatred for bad news.</p>
            <p>Think of the following situations as antidotes -</p>
            <p>(1) Napoleon and Hitler when they invaded Russia after using their armies with much success elsewhere.</p>
            <p>(2) A man foolishly gambles in a casino and yet wins. This unlikely correlation causes him to try the casino
            again, or again and again, to his horrid detriment.</p>
            <p>The proper antidotes to being made such a patsy by past success are</p>
            <ul>
            <li>to carefully examine each past success, looking for accidental, non-causative factors associated with such
            success that will mislead</li>
            <li>to look for dangerous aspects of the new undertaking that were not present when past success occurred. <br>
            </li>
            </ul>
            <blockquote>
            <p>Hating and disliking also cause miscalculation triggered by mere association. In business, I commonly see
            people underappraise both the competency and morals of competitors they dislike. This is a dangerous
            practice, usually disguised because it occurs on a subconscious basis.</p>
            </blockquote>
            <p>Influence-from-Mere-Association Tendency often has a shocking effect that helps swamp
            the normal tendency to return favor for favor.</p>
            <p>Sometimes, when one receives a favor, his condition is unpleasant, due to poverty, sickness, subjugation, or
            something else. In this case, the favor may trigger an envy-driven dislike for the person who was in so
            favorable a state that he could easily be a favor giver.</p>
            <p>Influence from mere association shows up in judging people via stereotypes as well.</p>
            <blockquote>
            <p>Just as one must learn that trend does not always correctly predict destiny, one must learn that the average
            dimension in some group will not reliably guide him to the dimension of some specific item. Otherwise one
            will make many errors, like that of the fellow who drowned in a river that averaged out only eighteen inches
            deep.</p>
            </blockquote>
            <p>More on this quote and stereotypes: <a href=\"https://neilkakkar.com/averages-are-meaningless.html\">Averages are
            meaningless.</a></p>
            <h1>Pain Avoiding Psychological Denial</h1>
            <p>The reality is too painful to bear, so one distorts the facts until they become bearable. The tendency’s most
            extreme outcomes are usually mixed up with love, death, and chemical dependency.</p>
            <blockquote>
            <p>“It is not necessary to hope in order to persevere.” &shy;- William the Silent</p>
            </blockquote>
            <h1>Excessive Self Regard Tendency</h1>
            <p>Humans think of themselves as better than they are, think of their possessions as more valuable than they are,
            and think of their decisions as better than they are.</p>
            <p>There is a name in psychology for this overappraise-your-own-possessions phenomenon: the “<a rel=\"noopener\"
            href=\"https://en.wikipedia.org/wiki/Endowment_effect\">endowment effect</a>”. And all man’s decisions are
            suddenly regarded by him as better than would have been the case just before he made them.</p>
            <p>Mans excessive self regard tendency also makes him prefer people like him.</p>
            <p>This has perverse consequences.</p>
            <ul>
            <li>Degrading cults / orgs - A dysfunctional group in charge of hiring degrades the entire organization - as
            they hire more people like themselves.</li>
            <li>Irrational bets - I think I’m taller, faster, and stronger than you (even if I might not be)</li>
            </ul>
            <p>On a personal level, excessive self regard makes you blind to your shortcomings.
            Try to face the two simple facts:</p>
            <ul>
            <li>
            <p>Fixable but unfixed bad performance is bad character and tends to create more
            of itself causing more damage to the excuse giver with each tolerated instance.</p>
            </li>
            <li>
            <p>In demanding places, like athletic teams and General Electric, you are almost
            sure to be discarded in due course if you keep giving excuses instead of
            behaving as you should.</p>
            </li>
            </ul>
            <blockquote>
            <p>I once heard of a child-teaching method so effective that the child remembered the learning experience over
            fifty
            years later. The child later became Dean of the USC School of Music and then related to me what his father
            said when he saw his child taking candy from the stock of his employer with the excuse that he intended to
            replace it later. The father said, “Son, it would be better for you to simply take all you want and call
            yourself a thief every time you do it.”</p>
            </blockquote>
            <h1>Overoptimism Tendency</h1>
            <p>We are over-optimistic.</p>
            <p>One standard antidote to foolish optimism is trained, habitual use of the <a
            href=\"https://neilkakkar.com/Bayes-Theorem-Framework-for-Critical-Thinking.html\">simple probability math of
            Fermat and Pascal</a>. The mental rules of thumb that evolution gives you to deal with risk are not
            adequate. They resemble the dysfunctional golf grip you would have if you relied on a grip driven by evolution
            instead of golf lessons.</p>
            <h1>Deprival Superreaction Tendency</h1>
            <p>The quantity of man’s pleasure from a ten dollar gain does not match the quantity of his displeasure from a
            ten-dollar loss.
            This is also known as <a rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Loss_aversion\">loss aversion</a>.
            </p>
            <p>In displaying Deprival-Superreaction Tendency, man incurs disadvantage by misframing his problems. He will often
            compare what is near instead of what really matters. For instance, a man with $10 million in his brokerage
            account will often be extremely irritated by the accidental loss of $100 out of the $300 in his wallet.</p>
            <h1>Social-Proof Tendency</h1>
            <p>Mans evolution left him with Social-Proof Tendency, an automatic tendency to think and act as he sees others
            around him thinking and acting.
            What simpler way could there be to find out how to walk to a big football game in a strange city than by
            following the flow of the crowd?</p>
            <p>Triggering of social proof occurs in the presence of puzzlement or stress, and particularly when both exist.</p>
            <p>Because both bad and good behavior are made contagious by Social-Proof Tendency, it is important that human
            societies</p>
            <p>(1) stop any bad behavior before it spreads <br>
            (2) foster and display all good behavior.</p>
            <p>In social proof, it is not only action by others that misleads but also their inaction. In the presence of doubt,
            inaction by others becomes social proof that inaction is the right course.</p>
            <blockquote>
            <p>If only one lesson is to be chosen from a package of lessons involving Social-Proof tendency, and used in
            self improvement, my favorite would be: Learn how to ignore the examples from others when they are wrong.
            </p>
            </blockquote>
            <h1>Contrast-Misreaction Tendency</h1>
            <p>The nervous system of man does not naturally measure in absolute scientific units. So, it must rely on something
            simpler. The answer is contrast, the difference between things.</p>
            <p>The eyes register the contrast in what is seen. They’ll see what is faster, not the actual speed.</p>
            <p>As perception goes, so goes cognition.</p>
            <blockquote>
            <p>Large scale damages often ruin lives, as when a wonderful woman having terrible parents marries a man who
            would be judged satisfactory only in comparison to her parents. Or as when a man takes wife number two who
            would be appraised as alright only in comparison to wife number one.</p>
            </blockquote>
            <blockquote>
            <p>When a man’s steps are consecutively taken toward disaster, with each step being very small, the brain’s
            Contrast-Misreaction Tendency will often
            let the man go too far toward disaster to be able to avoid it. This happens because each step presents so
            small a contrast from his present position.</p>
            </blockquote>
            <h1>Availability-Misweighing Tendency</h1>
            <blockquote>
            <p>“When I’m not near the girl I love, I love the girl I’m near.”</p>
            </blockquote>
            <p>Mans imperfect, limited-capacity brain drifts into working with what’s easily available to it. And the brain
            can’t use what it can’t remember or what it is blocked from recognizing because it is heavily influenced by one
            or more psychological tendencies. And so the mind overweighs what is easily available.</p>
            <p>This tendency also shows up when thinking about difficult problems. Instead of measuring the factors that
            actually represent the problem, man tends to focus on measuring the factors that are easily available. These
            factors are usually numeric in nature. For example, solving for growth of a country via measuring only GDP.</p>
            <p>A simple, incomplete explanation to a complex phenomena gains widespread attention because it’s simple, easily
            available. This is called an <a rel=\"noopener\"
            href=\"https://en.wikipedia.org/wiki/Availability_cascade\">Availability Cascade</a>, a potent result of <a
            rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Availability_heuristic\">availability bias</a></p>
            <p>The main antidote to mis-cues from Availability- Misweighing Tendency involve procedures, like use of checklists.
            </p>
            <p>Another antidote is to behave like Darwin did when he emphasized disconfirming evidence.</p>
            <p>Another option is <a rel=\"noopener\" href=\"https://redteams.net/rules\">red teaming</a>.</p>
            <p>The great algorithm to remember in dealing with this tendency is simple: An idea or a fact is not worth more
            merely because it is easily available to you.</p>
            <h1>Use it or lose it Tendency</h1>
            <p>If you stop using your skills, you’re going to lose them.</p>
            <p>People tend to forget that they learnt these skills and stop using them, which makes them worse off.</p>
            <p>It is thus, essential to assemble your skills into a checklist that you routinely use.</p>
            <blockquote>
            <p>If a skill is raised to fluency, instead of merely being crammed in briefly to enable one to pass some test,
            then the skill (1) will be lost more slowly and (2) will come back faster when refreshed with new learning.
            These are not minor advantages, and <strong>a wise man engaged in learning some important skill will not
            stop until he is really fluent in it.</strong></p>
            </blockquote>
            <h1>Authority-Misinfluence Tendency</h1>
            <p>People’s brain turns to mush when hearing orders from authority.</p>
            <p>Thus, be careful whom you appoint to power because a dominant authority figure will often be hard to remove.</p>
            <h1>Twaddle Tendency</h1>
            <p>Some people like to work, others like to twaddle. Twaddling means talking about something you don’t know, or just
            making up stuff as you go, or talking about meaningless stuff.</p>
            <blockquote>
            <p>The principal job of an academic administration is to keep the people who don’t matter from interfering with
            the work of the people that do.</p>
            </blockquote>
            <p>As a direct result, choose people you want to talk to carefully.</p>
            <h1>Reason Respecting Tendency</h1>
            <p>People respect reasons for things. There is a natural joy that comes from being able to explain why.</p>
            <p>Few practices, therefore, are wiser than not only thinking through reasons before giving orders but also
            communicating these reasons to the recipient of the order.</p>
            <blockquote>
            <p>Carl Braun [a CEO] well knew that ideas got through best when reasons for the ideas were meticulously laid
            out. - You had to tell Who was to do What, Where, When, and Why.</p>
            </blockquote>
            <p>An unfortunate byproduct of Reason-Respecting Tendency is respecting bullshit reasoning. For example, at the
            photocopy machine, people would let others cut in line when they said “Excuse me, can I cut in because I have to
            make some copies?”</p>
            <h1>Lollapalooza Tendency</h1>
            <blockquote>
            <p>The Tendency to Get Extreme Consequences from Confluences of Psychological Tendencies Acting in Favor of a
            Particular Outcome.</p>
            </blockquote>
            <p>When some of the above tendencies work together, you get extreme consequences.</p>
            </div>"
    },
    {
        title: 'Sessions, Cookies, and Authentication',
        url: 'theodinproject.com',
        full_url: 'https://www.theodinproject.com/courses/ruby-on-rails/lessons/sessions-cookies-and-authentication', 
        reading_time: '15 min',
        featured: false, 
        # cover_img: 'https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fwww.theodinproject.com%2Fassets%2Fog-logo-003bdac3098010b4f1143000a941b80c67eebd74fbc6f36b2e7ef92dd03e8020.png',
        content: 
            "<div class='article-content'>
            <h3 id=\"introduction\">Introduction</h3>
            <p>“Sessions” are the idea that your user’s state is somehow preserved when he/she clicks from one page to the next. Remember, HTTP is stateless, so it’s up to either the browser or your application to “remember” what needs to be remembered.</p>
            <p>In this lesson you’ll learn about sessions, browser cookies, and how authentication is built in Rails. We’ll cover both home-grown authentication and the most commonly used authentication gem, Devise.</p>
            <h3 id=\"learning-outcomes\">Learning Outcomes</h3>
            <p>Look through these now and then use them to test yourself after doing the assignment:</p>
            <ul><li>What is a session?</li>
            <li>How is the <code>session</code> “hash” different from the <code>cookies</code> “hash”?</li>
            <li>What is the <code>flash</code> “hash” used for?</li>
            <li>When would you need to use <code>flash.now</code> instead of <code>flash</code>?</li>
            <li>What are controller filters and why are they useful?</li>
            <li>How do you run a controller filter for just a specific few actions?</li>
            <li>What’s the difference between authentication and authorization?</li>
            <li>Why is <code>#has_secure_password</code> a handy method?</li>
            <li>What is the basic overview of how to authenticate a user with that method?</li>
            <li>What additional steps (on a high level) are needed to actually “remember” a user after they’ve closed the browser?</li>
            <li>What is the Devise gem and why is it useful?</li>
            </ul><h3 id=\"cookies-sessions-and-flashes\">Cookies, Sessions, and Flashes</h3>
            <p>Cookies, Sessions and Flashes are three special objects that Rails gives you which each behave a lot like hashes. They are used to persist data between requests, whether until just the next request, until the browser is closed, or until a specified expiration has been reached. In addition to different temporal concerns, they each solve slightly different use cases, covered below.</p>
            <h3 id=\"cookies\">Cookies</h3>
            <p>Cookies are key-value data pairs that are stored in the user’s browser until they reach their specified expiration date. They can be used for pretty much anything, most commonly to “bookmark” the user’s place in a web page if she gets disconnected or to store simple site display preferences. You could also store shopping cart information or even passwords but that would be a bad idea – you shouldn’t store anything in regular browser cookies that needs to either be secure or persisted across browser sessions. It’s too easy for users to clear their cache and/or steal/manipulate unsecured cookies.</p>
            <p>To work with cookies, Rails gives you access to a special hash called <code>cookies</code>, where each key-value pair is stored as a separate cookie on the user’s browser. If you were to save <code>cookies[:hair-color] = \"blonde\"</code>, you’d be able to pull up your browser’s developer tools and see a cookie on the user’s browser that has a key of <code>hair-color</code> and a value of <code>blonde</code>. Delete it using <code>cookies.delete(:hair-color)</code>.</p>
            <p>With each new request to your server, the browser will send along all the cookies and you can access them in your controllers and views like a normal hash. You can also set their expiration dates, for example using syntax like <code>cookies[:name] = { value: \"cookies YUM\", expires: Time.now + 3600}</code>.</p>
            <h3 id=\"sessions\">Sessions</h3>
            <p>Think about how websites keep track of how a user is logged in when the page reloads. HTTP requests are stateless so how can you tell that a given request actually came from that particular user who is logged in? This is why cookies are important – they allow you to keep track of your user from one request to another until the cookie expires.</p>
            <p>A special case is when you want to keep track of data in the user’s “session”, which represents all the stuff your user does while you’ve chosen to “remember” her, typically until the browser window is closed. In that case, every page she visits until the browser is closed will be part of the same session.</p>
            <p>To identify a user’s session information, Rails stores a special secure and tamper-proof cookie on the user’s browser that contains their entire session hash (look for it in your developer tools, usually under the “Resources” section) and it expires when the browser is closed. Whenever the user makes a request to your application, that request will also automatically include that session cookie (along with the other cookies) and you can use it to keep track of her logged-in state. This may all seem abstract now, but you’ll get a chance to see it in action shortly.</p>
            <p>Rails gives you access to the <code>session</code> hash in an almost identical way to the above-mentioned <code>cookies</code> hash. Use the <code>session</code> variable in your views or controllers like so:</p>
            <pre><code class=\"language-ruby\">  # app/controllers/users_controller.rb
            ...
            # Set a session value
            session[:current_user_id] = user.id

            # Access a session value
            some_other_variable_value = session[:other_variable_key]

            # Reset a session key
            session[:key_to_be_reset] = nil

            # Reset the entire session
            reset_session
            ...
            </code>
            </pre>
            <p>Why would you need both cookies and sessions? They are similar but not the same. <code>session</code> is an entire hash that gets put in the secure session cookie that expires when the user closes the browser. If you look in your developer tools, the “expiration” of that cookie is “session”. Each value in the <code>cookies</code> hash gets stored as an individual cookie.</p>
            <p>So cookies and sessions are sort of like temporary free database tables for you to use that are unique to a given user and will last until you either manually delete them, they have reached their expiration date, or the session is ended (depending on what you specified).</p>
            <h4 id=\"a-few-additional-notes-on-sessions-and-cookies\">A Few Additional Notes on Sessions and Cookies**</h4>
            <ul><li><code>session</code> and <code>cookies</code> aren’t really hashes, Rails just pretends they are so it’s easy for you to work with them. You can still consider them as hashes just because they act very similarly to hashes.</li>
            <li>You are size-limited in terms of how much you can store inside a session hash or browser cookie (~4kb). It is sufficient for any “normal” usage, but don’t go pretending either of these are actually substitutes for a database.</li>
            </ul><h3 id=\"flashes\">Flashes</h3>
            <p>You’ve already seen and used the <code>flash</code> hash by now, but we’ll cover it again from the perspective of understanding sessions. <code>flash</code> is a special hash (okay, a method that acts like a hash) that persists only from one request to the next. You can think of it as a <code>session</code> hash that self destructs after it’s opened. It’s commonly used to send messages from the controller to the view so the user can see success and failure messages after submitting forms.</p>
            <p>If you want to pop up “Thanks for signing up!” on the user’s browser after running the <code>#create</code> action (which usually uses <code>redirect_to</code> to send the user to a totally new page when successful), how do you send that success message? You can’t use an instance variable because the redirect caused the browser to issue a brand new HTTP request and so all instance variables were lost.</p>
            <p>The flash is there to save the day! Just store <code>flash[:success]</code> (or whatever you’d like it called) and it will be available to your view on the next new request. As soon as the view accesses the hash, Rails erases the data so you don’t have it show up every time the user hits a new page. So clean, so convenient.</p>
            <p>What about cases where the user can’t sign up because of failed validations? In this case, the typical <code>#create</code> action would just <code>render</code> the <code>#new</code> action using the existing instance variables. Since it’s not a totally new request, you’ll want to have your error message available immediately. That’s why there’s the handy <code>flash.now</code> hash, e.g. <code>flash.now[:error] = \"Fix your submission!\"</code>. Just like the regular flash, this one self destructs automatically after opening.</p>
            <p>You still have to write view code to display the flash messages. It’s common to write a short view helper that will pin any available flash message(s) to the top of the browser. You might also add a class to the message which will allow you to write some custom CSS, for instance turning <code>:success</code> messages green and <code>:error</code> messages red.</p>
            <pre><code class=\"language-html\">  # app/views/layouts/application.html.erb
            ...
            &lt;% flash.each do |name, message| %&gt;
                &lt;div class=\"&lt;%= name %&gt;\"&gt;&lt;%= message %&gt;&lt;/div&gt;
            &lt;% end %&gt;
            </code>
            </pre>
            <h3 id=\"controller-filters\">Controller Filters</h3>
            <p>Before we talk about authentication, we need to cover controller filters. The idea of these filters is to run some code in your controller at very specific times, for instance before any other code has been run. That’s important because, if a user is requesting to run an action they haven’t been authorized for, you need to nip that request in the bud and send back the appropriate error/redirect before they’re able to do anything else. You’re basically “filtering out” unauthorized requests.</p>
            <p>We do this through the use of a “before filter”, which takes the name of the method we want to run:</p>
            <pre><code class=\"language-ruby\">  # app/controllers/users_controller
            before_action :require_login
            ...
            private

            def require_login
                # do stuff to check if user is logged in
            end
            </code>
            </pre>
            <p>The <code>before_action</code> method takes the symbol of the method to run before anything else gets run in the controller. If it returns <code>false</code> or <code>nil</code>, the request will not succeed.</p>
            <p>You can specify to only apply the filter for specific actions by specifying the <code>only</code> option, e.g. <code>before_action :require_login, only: [:edit, :update]</code>. The opposite applies by using the <code>:except</code> option… it will run for all actions except those specified.</p>
            <p>You’ll want to hide your filter methods behind the <code>private</code> designation so they can only be used by that controller.</p>
            <p>Finally, filters are inherited so if you’d like a filter to apply to absolutely every controller action, put it in your <code>app/controllers/application_controller.rb</code> file.</p>
            <h3 id=\"authentication\">Authentication</h3>
            <p>The whole point of authentication is to make sure that the user is who they say they are. The standard way of managing this is through logging in your user via a sign in form. Once the user is logged in, you keep track of that user using the session until the user logs out.</p>
            <p>A related concept is authorization. Yes, you may be signed in, but are you actually authorized to access what you’re trying to access? The typical example is the difference between a regular user and an admin user. They both authenticate with the system but only the admin is authorized to make changes to certain things.</p>
            <p>Authentication and authorization go hand in hand – you first authenticate someone so you know who they are and can check if they’re authorized to view a page or perform an action. When you build your app, you’ll have a system of authentication to get the user signed in and to verify the user is who he says he is. You authorize the user to do certain things (like delete stuff) based on which methods are protected by controller filters that require signin or elevated permissions (e.g. admin status).</p>
            <h3 id=\"basic-and-digest-authentication\">Basic and Digest Authentication</h3>
            <p>If you’re looking for a very casual and insecure way of authenticating people, HTTP Basic authentication can be used. We won’t cover the details here, but it basically involves submitting a username and password to a simple form and sending it (unencrypted) across the network. You use the <code>#http_basic_authenticate_with</code> method to do so (see the reading for examples) and to restrict access to certain controllers without it.</p>
            <p>For a slightly more secure (over HTTP) authentication system, use HTTP Digest Authentication. We’ll again not cover it here. It relies on a <code>#before_action</code> running a method which calls upon <code>#authenticate_or_request_with_http_digest</code>, which takes a block that should return the “correct” password that should have been provided.</p>
            <p>The problem with both of these is that they hard code user names and passwords in your controller (or somewhere), so it’s really just a band-aid solution.</p>
            <h3 id=\"rolling-your-own-auth\">Rolling Your Own Auth</h3>
            <p>If you want user logins, you’ll need to go through a few extra steps. We won’t cover them explicitly here because you’ll get a chance to do them in the project. A few principles are useful to know going in, though. The following stuff may seem a bit abrupt and devoid of examples, but it’s really just a preview of what you’ll be doing shortly in the project.</p>
            <p>First, we don’t store passwords in plain text in the database. That’s just asking for trouble (how many news stories have you seen about major sites getting hacked and passwords being exposed in plain text?). Instead, you’ll store an encrypted “password digest” version of the password.</p>
            <p>When a user submits their password via the login form, instead of comparing it directly with a plaintext version of that password, you actually convert the submitted password into digest form. You’ll then compare that new digest with the digest you’d previously stored from the user. If they match, you’ve got yourself a logged in user.</p>
            <p>This is much better because digests are one-way encryption. You can easily create a digest from a password string, but it’s extremely difficult to decrypt the digest and retrieve the original password. The most effective way to crack a bunch of digests is just to make a giant list of possible passwords, turn them into digests, and see if those digests match the one you’re trying to crack (i.e. guess-and-check on a massive scale).</p>
            <p>Rails doesn’t make you do everything yourself. It has a method called <code>#has_secure_password</code> which you just drop into your User model and it will add a lot of the functionality you’re looking for. To work with that handy method, you basically set up your User model to handle accepting <code>password</code> and <code>password_confirmation</code> attributes but you won’t actually persist those to the database. <code>has_secure_password</code> intercepts those values and converts them into the password digest for you.</p>
            <p>To initialize a new user session (when your user signs in), you’ll need to create a new controller (usually <code>sessions_controller.rb</code>) and the corresponding routes for <code>:new</code>, <code>:create</code> and <code>:destroy</code>. If the user passes the correct credentials (which we can check using the <code>#authenticate</code> method), you’ll use the <code>session</code> variable to store their ID, which you can use to validate that they are who they say they are. This is a simple way of authenticating the user that uses Rails’ existing session infrastructure, but only lasts as long as the session does.</p>
            <p>If your user wants to be “remembered” (you’ve probably seen the “remember me” checkbox plenty of times on login forms), you need a way to remember them for longer than just the length of the browser session. To do this, you’ll need to create another column in your Users table for an encrypted <code>remember_token</code> (or whatever you’d like to call it). You’ll use that to store a random string for that user that will be used in the future to identify him/her.</p>
            <p>You will drop the unencrypted token as a permanent cookie (using <code>cookies.permanent[:remember_token]</code>) into the user’s browser. That cookie will be submitted with each new request, so you can check with the encrypted version in the database to see who that user is whenever they make a request. This is basically a more explicit and permanent version of what Rails is doing with sessions. It’s best practice to reset the token on each new signin if the user signs out.</p>
            <p>It’s usually good to make some helper methods for yourself to cover common behavior like signing in a user, checking if a user is signed in, and comparing the currently signed in user with another user (useful if the current user is looking at another user’s page and shouldn’t see links to “edit” it). These things will make your life much easier since you can reuse them in your controller filters or your views or even your tests.</p>
            <p>A generic step-by-step overview:</p>
            <ol><li>Add a column to your Users table to contain the user’s <code>password_digest</code>.</li>
            <li>When the user signs up, turn the password they submitted into digest form and then store THAT in the new database column by adding the <code>has_secure_password</code> method to your User model.</li>
            <li>Don’t forget any necessary validations for password and password confirmation length.</li>
            <li>Build a sessions controller (and corresponding routes) and use the <code>#authenticate</code> method to sign in the user when the user has submitted the proper credentials using the signin form.</li>
            <li>Allow the user to be remembered by creating a <code>remember_token</code> column in the Users table and saving that token as a permanent cookie in the user’s browser. Reset on each new signin.</li>
            <li>On each page load that requires authentication (and using a <code>#before_action</code> in the appropriate controller(s)), first check the user’s cookie <code>remember_token</code> against the database to see if he’s already signed in. If not, redirect to the signin page.</li>
            <li>Make helper methods as necessary to let you do things like easily determine if a user is signed in or compare another user to the currently signed in user.</li>
            <li>Profit.</li>
            </ol><h3 id=\"devise\">Devise</h3>
            <p>Devise is a gem which has been built to handle all this stuff for you. It may be tempting to immediately dive into it, but that’s not a good idea for beginners. It’s first of all quite important to understand the basic steps of authentication. Devise can also get fairly complex if you start running into problems or nonstandard use cases. It’s more useful for intermediate+ users of Rails than beginners.</p>
            <p>That said, you’ll end up using it on most of your projects once you’ve mastered rolling your own authentication. It’s ultimately better than rolling your own auth because they’ve covered a lot of the edge cases and security loopholes that you might not think about. Devise lets you interface with more advanced authentication systems for talking to APIs like OAuth. So it’s quite useful down the road.</p>
            <p>In a short word, Devise prepackages for you a bunch of signin and signup forms and methods to help implement them. It’s made up of 10 modules (and you can choose which ones you want to use). You install the <code>devise</code> gem and run the installer to drop their files into your application. You’ll also need to run a database migration to add their additional fields to your Users table.</p>
            <p>Configuration will be dependent on your use case. Do you want to make the user confirm their signup using an email (the <code>Confirmable</code> module)? Do you want to allow the user to reset his password (the <code>Recoverable</code> module)?</p>
            <p>It’s beyond the scope of this lesson to teach Devise but you’ll certainly be using it by the end of the course. The trick is to read the documentation. They’ve got a fairly impressive set of docs available <a href=\"https://github.com/plataformatec/devise\">on Github</a>. The point of showing it here is for you to see it, read it, and keep it in the back of your head until you actually use it.</p>
            <h3 id=\"assignment\">Assignment</h3>
            <div class=\"lesson-content__panel\">
            <ol><li>Read <a href=\"https://www.justinweiss.com/articles/how-rails-sessions-work/\">this article about how Rails sessions work</a>.</li>
            <li>Watch <a href=\"https://www.youtube.com/watch?v=mqUbnZIY3OQ\">this video to dive deep into sessions</a>.</li>
            <li>Read <a href=\"http://guides.rubyonrails.org/action_controller_overview.html#session\">Rails Guides on Controllers</a> chapters 5-6. Don’t worry too much about the details of <code>session_store</code> configurations in 5.1 right now.</li>
            <li>Read <a href=\"http://guides.rubyonrails.org/action_controller_overview.html#filters\">Rails Guides on Controllers</a> chapter 8 to understand controller filters.</li>
            <li>Read <a href=\"http://guides.rubyonrails.org/action_controller_overview.html#http-authentications\">Rails guides on Controllers</a> chapter 11 to understand more about authentication.</li>
            <li>Glance over the <a href=\"https://github.com/plataformatec/devise\">Devise Documentation</a>. Read about how to install it in your Rails App and what the different modules do. You don’t need to use Devise just yet, so this is more of a reconnaissance mission for later.</li>
            </ol></div>
            <h3 id=\"conclusion\">Conclusion</h3>
            <p>Authentication can appear to be a fairly complicated topic – there are a lot of moving parts. At it’s core, though, you’re just checking whether the person making a request is actually a signed in user who has the permissions to do so, all by using browser cookies in some form or another.</p>
            <p>This lesson should have given you some appreciation for how complicated login systems can potentially get but it should also have removed the veil from the websites you’ve visited countless times. Auth isn’t rocket science and you’ll shortly be building it into your own applications.</p>
            <h3 id=\"additional-resources\">Additional Resources</h3>
            <p>This section contains helpful links to other content. It isn’t required, so consider it supplemental for if you need to dive deeper into something.</p>
            </div>"
    },
    {
        title: 'Data Structures & Algorithms I Used Working at Tech Companies',
        author: "Gergely Orosz",
        url: 'blog.pragmaticengineer.com',
        full_url: 'https://blog.pragmaticengineer.com/data-structures-and-algorithms-i-actually-used-day-to-day/', 
        reading_time: '14 min',
        featured: false, 
        # cover_img: 'https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fblog.pragmaticengineer.com%2Fcontent%2Fimages%2F2020%2F07%2Falgorithms.jpg',
        content: 
            "<div lang=\"en\"><p>Do you actually use data structures and algorithms on your day to day job? I've noticed a growing trend of people assuming algorithms are pointless questions that are asked by tech companies purely as an arbitrary measure. I hear more people complain about how all of this is a purely academic exercise. This notion was definitely popularized after Max Howell, the author of Homebrew, posted his Google interview experience:</p>
            <blockquote>
            <p dir=\"ltr\" lang=\"en\">Google: 90% of our engineers use the software you wrote (Homebrew), but you can’t invert a binary tree on a whiteboard so fuck off.</p>
            — Max Howell (@mxcl) <a href=\"https://twitter.com/mxcl/status/608682016205344768?ref_src=twsrc%5Etfw\"></a></blockquote>
            <p>While I've also never needed to use binary tree inversion, but I have come across everyday use cases of data structures and algorithms when <a href=\"https://blog.pragmaticengineer.com/surprising-things-about-working-at-tech-unicorns/\">working at Skype/Microsoft, Skyscanner and Uber</a>. This included writing code and making decisions based on these concepts. But for the most part, I used this knowledge to understand how and why some things were built. Knowing these concepts made it easy to understand design and implementations referencing these.</p>
            <p><strong>This article is a set of real-world examples where data structures like trees, graphs, and various algorithms were used in production</strong>. I hope to illustrate that a generic data structures and algorithms knowledge is not \"just for the interview\" - but something that you'd likely find yourself reaching for when working at fast-growing, innovative tech companies. If you want to buy one book that teaches you everything you need to know for the majority of interviews, <strong><a href=\"https://geni.us/5v5U\">Grokking Algorithms</a></strong> is hands down the best guide.</p>
            <p>I've used a very small subset of algorithms, but almost all data structures. It should be of no surprise that I am no fan of algorithm-heavy and non-practical interview questions with exotic data types like Red-Black trees or AVL trees. Never asked these, and never will. You can read about what I think about these interviews at the <a href=\"https://blog.pragmaticengineer.com/data-structures-and-algorithms-i-actually-used-day-to-day/#interviews-and-algorithms-and-data-structures\">end of this article</a>. Still, there is lots of value in being aware of what options for basic data types they can choose to tackle certain problems. With this, let's jump into examples.</p>
            <h2>Trees and tree traversing: Skype, Uber and UI frameworks</h2>
            <p>When we built Skype of Xbox One, we worked on a barebones Xbox OS, that was missing key libraries. We were building one of the first full-fledged applications on the platform. We needed a navigation solution that we could hook up both to touch gestures and to voice commands.</p>
            <p>We built a generic navigation framework on top of WinJS. To do so, we needed to maintain a DOM-like <strong>graph</strong> to keep track of the actionable elements. To find these elements, we did DOM traversal - basically, a <strong>tree</strong> traversal - across the existing DOM. This is a classic case of <strong>BFS</strong> or <strong>DFS</strong> (breadth-first search or depth-first search).</p>
            <p>If you're doing web development, you already work with a tree structure: the DOM. All DOM nodes can have children and the browser renders nodes on-screen after traversing the DOM tree. If you are searching for a specific element, you can use built-in DOM methods to find it - such as getElementById - or you could implement a BFS or DFS search to go through all the nodes, similar to how it's done <a href=\"https://www.softnami.com/posts_pr/traversing_the_html_dom_using_depth_and_breath_first_search.html\">in this example</a>.</p>
            <p>Many frameworks that render UI elements use tree structures behind the scenes. React maintains a virtual DOM, and uses smart <a href=\"https://reactjs.org/docs/reconciliation.html\">reconciliation</a> - a \"diffing\" algorithm - to deliver great performance, by only re-rendering parts of the screen that's changed. Ryan Bas visualizes this process <a href=\"https://medium.com/@ryanbas21/react-reconciliation-7075e3f07437\">in his writeup on React reconciliation</a>.</p>
            <p>Uber's mobile architecture, <a href=\"https://github.com/uber/RIBs\">RIBs</a> uses trees as well - similar to most UI frameworks that render elements in a hierarchy. RIBs maintains a RIB tree <a href=\"https://github.com/uber/RIBs/wiki#state-management\">for state management</a>, attaching and detaching the RIBs that need to be rendered. When working with RIBs, we would sometimes sketch where the new RIBs would fit in the hierarchy, and discuss whether the RIBs in question should have views - making it a part of the view hierarchy - or just manage state.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"State transitions in an example RIBs use case. See RIBs documentation and code here.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fblog.pragmaticengineer.com%2Fcontent%2Fimages%2F2020%2F07%2FScreenshot-2020-07-17-at-8.23.22.png\">
            <figcaption>State transitions in an example RIBs use case. See RIBs documentation and code here.</figcaption>
            </figure>
            </div><p>If you ever find yourself needing to build visualization of hirearchical elements, a common approach is to use a tree-like structure, traverse the tree and render the elements you visit. I've come across many internal tools that use this approach. An example is the RIBs visualization tool built by Uber's Mobile Platform team, that you can see in a demo <a href=\"https://www.youtube.com/watch?v=FfwZSk6VRVY&amp;feature=youtu.be&amp;t=1635\">in this video</a>.</p>
            <h2>Weighed graphs and shortest paths: Skyscanner</h2>
            <p>Skyscanner finds the best deals on airline tickets. It does this by scanning all routes worldwide, then putting them together. While the nature of the problem is more on crawling, and less on caching - as airlines calculate the layover options - the multi-city planning option becomes the <strong>shortest path problem</strong>.</p>
            <p>Multi-city was one of the features that took Skyscanner quite a bit of time to build - in all fairness, the difficulty was more on the product side, than anything. The best multi-city deals are calculated by using shortest path algorithms like Dijkstra or A*. Flight routes are represented as a directed graph, with each edge having a weight of the cost of the ticket. Calculating the cheapest price option between two cities was done via an implementation of a modified <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\"><strong>A* search algorithm</strong></a> per route. If you're interested in flights and shortest paths, the article on implementing the shortest flight search path using <strong>BFS</strong> <a href=\"https://medium.com/free-code-camp/exploring-the-applications-and-limits-of-breadth-first-search-to-the-shortest-paths-in-a-weighted-1e7b28b3307\">by Sachin Malhotra</a> is a good read.</p>
            <p>With Skyscanner, the actual algorithm was far less important, though. Caching, crawling, and handling the varying website load were much more difficult things to crack. Still, a variation of the shortest paths problem comes up with many several travel companies that optimize for price based on combinations. Unsurprisingly, this topic was also a source of hallway discussions here.</p>
            <h2>Sorting: Skype (kind of)</h2>
            <p>Sorting is an algorithm family I rarely had an excuse to implement or needed to use in-depth. It's interesting to understand the different types of ways to sort, from bubble sort, insertion sort, merge sort, selection sort and - the most complex one - quicksort. Still, I found that there is rarely a reason to implement any of this, especially when you don't need to write sort functions as part of a library.</p>
            <p>At Skype, I got to exercise a bit on this knowledge, though. One of the other engineers decided to implement an insertion sort for listing contacts. In 2013, when Skype connected to the network, contacts would arrive in bursts, and it would take some time for all the contacts to arrive. So this engineer thought it's more performant to build the contact list organized by name, using insertion sort. We had a back-and-forth on this, over why not just use the default sort algorithm. In the end, it was more work to properly test the implementation, and to benchmark it. I personally didn't see much point in doing so: but we were in the stage of the project that we had the time.</p>
            <p>There are definitely some real-world use cases where efficient sorting matters, and having control over what type of sorting you use, based on the data, can make a difference. Insertion sort can be useful when streaming realtime data in large chunks and building realtime visualization for these data sources. Merge sort can work well with divide-and-conquer approaches if it comes to large amounts of data stored on different nodes. I've not worked with these, so I'll still mark sorting algorithms as something with little day-to-day use, beyond the appreciation of the different approaches.</p>
            <h2>Hashtables and hashing: everywhere</h2>
            <p>The most frequent data structure I've used regularly was <strong>hashtables</strong> and the <strong>hashing function</strong>. It's such a handy tool from counting, to detecting duplications, to caching, all the way to <a href=\"https://blog.pragmaticengineer.com/distributed-architecture-concepts-i-have-learned-while-building-payments-systems/#sharding-and-quorum\">distributed systems use cases like sharding</a>. After arrays, it's easily the most common data structure I've used on countless occasions. Almost all languages come with this data structure, and it's simple to implement if you'd need it.</p>
            <h2>Stacks and queues: every now and then</h2>
            <p>The <strong>stack</strong> data structure will be very familiar to anyone who has debugged a language that has a stack trace. As a data structure, I've had a few problems to use it for, but debugging and performance profiling makes me intricately familiar with it. It's also an obvious choice when traversing a tree depth-first.</p>
            <p>I rarely had to choose <strong>queues</strong> as data structures for my code, but I came across it plenty of times in codebases, code popping, or pushing values in it. A common use case is implementing BFS traversing of trees, where the queue data structure lends itself. You can also use queues for a variety of other use cases. I once read code scheduling jobs that made good use of <a href=\"https://en.wikipedia.org/wiki/Priority_queue\">priority queues</a>, running the shortest jobs first, using the <a href=\"https://docs.python.org/2/library/heapq.html\">Python heap queue algorithm</a>.</p>
            <h2>Crypto: Uber</h2>
            <p>User-entered sensitive information coming from mobile or web clients needs to be encrypted before sending through the network, only to be decrypted on a specific service. To do so, a crypto approach needs to be implemented on the client and the backend.</p>
            <p>Understanding crypto is interesting. You don't come up with a new algorithm - this can be one of the worst ideas to do. Instead, you take an existing, well-documented standard and the use the framework built-in primitives. The standard of choice is usually a part of <a href=\"https://csrc.nist.gov/csrc/media/publications/fips/197/final/documents/fips-197.pdf\">the AES standard</a>. If it's secure enough to <a href=\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#Security\">encrypt classified US information</a>, and there is no known, working attack on the protocol, AES192 or AES256 is usually a secure enough choice for most use cases.</p>
            <p>When I joined Uber, mobile and web crypto were already implemented on top of these primitives, giving me the excuse to look up details of the Advanced Encryption Standard (<strong>AES</strong>), Hashed Message Authentication Codes (<strong>HMAC</strong>), the <strong>RSA</strong> public-key encryption and others. Understanding how a series of encryption steps are provably secure was another interesting area. Between encrypt-and-MAC, MAC-then-encrypt, and encrypt-then-MAC, only <a href=\"http://www.daemonology.net/blog/2009-06-24-encrypt-then-mac.html\">one of them is provably secure</a> - even though this doesn't mean the others are not secure.</p>
            <p>Implementing crypto primitives is rarely ever something you will ever need to do - unless building a brand new core framework. However, deciding on which primitives to use, how to combine primitives is a decision you might face - or have to understand why a certain approach was taken.</p>
            <h2>Decision trees: Uber</h2>
            <p>On one of the projects, we had to implement complex business logic in the mobile application. Based on half a dozen rules, we had to display one of several different screens. The rules were unusually complex due to a series of compliance checks and user choices that we needed to take into account.</p>
            <p>The engineer building this feature first tried to code the rules with a series of if-else statements, which got too complex. In the end, they decided to go with a decision tree, as it was easy to validate with product and compliance, reasonable to implement and easy to change, if needed. We needed to build an implementation for edges to execute rules, but this was about it. While we could have saved implementing this tree with a different approach, the team found this solution to be easier to maintain, going forward. Here's how the decision tree looked like - the edges being results of rules executed (either binary outcome or value ranges) and the leaf nodes marking what screen to transition to.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"The structure of a decision tree we built to decide what screen to show, following a complex set of rules.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fblog.pragmaticengineer.com%2Fcontent%2Fimages%2F2020%2F07%2FBlank-Diagram.png\">
            <figcaption>The structure of a decision tree we built to decide what screen to show, following a complex set of rules.</figcaption>
            </figure>
            </div><p>Uber's mobile build system, called SubmitQueue, also utilizes decision trees built on the fly. The Developer Experience team had to solve the difficult problem of hundreds of mobile merges happening each day, with each build needing around 30 minutes to run - compiling, the running unit, integration, and UI tests. Parallelizing the builds was not a good enough solution, as two builds would often have overlapping changes, and a merge conflict would occur. In practice, this meant sometimes engineers would have to wait 2-3 hours, rebasing and staring the merge process again, and hoping there would be no conflict.</p>
            <p>The Developer Experience team took an innovative approach by predicting merge conflicts and queued builds accordingly, using speculation graphs. Speculation graphs are very similar to binary decision trees:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"SubmitQueue uses a speculation tree - a binary decision tree - annotated with prediction probabilities for each edge. This approach determines which set of builds to to run in parallel, to improve turnaround time and throughput while keeping master green. Read the full paper here.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fblog.pragmaticengineer.com%2Fcontent%2Fimages%2F2020%2F07%2FScreenshot-2020-07-17-at-12.46.34.png\">
            <figcaption>SubmitQueue uses a speculation tree - a binary decision tree - annotated with prediction probabilities for each edge. This approach determines which set of builds to to run in parallel, to improve turnaround time and throughput while keeping master green. Read the full paper here.</figcaption>
            </figure>
            </div><p>There are a <a href=\"https://eng.uber.com/research/keeping-master-green-at-scale/\">lot more details in this whitepaper</a> written by engineers on the Developer Experience team who built the solution - and this paper is a great read. Adrian Colyer also has a nice, <a href=\"https://blog.acolyer.org/2019/04/18/keeping-master-green-at-scale/\">visual analysis on the approach</a>. The result was a much faster build system that optimized build time, and made the lives of hundreds of mobile engineers far more pleasant.</p>
            <h2>Hexagonal Grids, Hierarchical Indexes: Uber</h2>
            <p>This last project is one I've come about purely based on stumbling across it. I learned about a new data structure: hexagonal grids with hierarchical indexes.</p>
            <p>One of the most difficult and interesting problems to solve at Uber is how to optimize the pricing of trips, and the dispatching of partners. Prices can be dynamic, and drivers are constantly on the move. <a href=\"https://eng.uber.com/h3/\">H3</a> is a grid system engineers built to both visualize and analyze data across cities, at an increasingly granular level. The data - and visualization - structure for this is a hexagonal grid with hierarchical indexes, and a couple of internal visualization tools are built on top of it.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"Subdividing regions with hexagons. Read a thorough article here.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fblog.pragmaticengineer.com%2Fcontent%2Fimages%2F2020%2F07%2FScreenshot-2020-07-08-at-17.24.25.png\">
            <figcaption>Subdividing regions with hexagons. Read a thorough article here.</figcaption>
            </figure>
            </div><p>The data structure has specific indexing, traversal, hierarchical grid, region, and unidirectional edge functions, <a href=\"https://h3geo.org/docs/api/indexing\">detailed in the API reference</a>. For a more detailed deep-dive, see the <a href=\"https://eng.uber.com/h3/\">article on the H3 library</a>, the <a href=\"https://github.com/uber/h3\">source code</a>, or the <a href=\"https://www.youtube.com/watch?time_continue=768&amp;v=ay2uwtRO3QE&amp;feature=emb_title\">presentation</a> on how and why this tool was built.</p>
            <p>What I really liked about this experience is learning that creating your own specialized data structures can make sense in a niche area. There aren't many use cases where hexagonal grids with hierarchical indexes would make sense beyond combining mapping with various data levels within each cell. Still, if you're familiar with some data structures, understanding this new data structure is much easier - as it would be for you to design yet another data structure for a similarly specialized need.</p>
            <h2>Interviews and algorithms and data structures</h2>
            <p>Those were the highlights of the actual data structures and algorithms I've come across professionally between multiple companies and many years. So let's go back to the original tweet that complained about asking things like inverting a binary tree on a whiteboard. I'm on Max's side on this one.</p>
            <p><strong>Knowing how popular algorithms or exotic data structures work are not something you should need to know to work at a tech company.</strong> You should know what an algorithm is, and should be able to come up with simple ones on your own, like a greedy one. You should also know about basic data structures that are pretty common, like hashtables, queues, or stacks. But specific algorithms like Dijkstra, A* and more advanced ones are not things you'd need to memorize: you'll have a reference for this. The most I did with algorithms beyond sorting was look them up and understand them myself. Same with exotic data structures like Red-Black trees or AVL trees. I've never had to use these data structures. Even if I did, I would look them up again. I have never asked questions that needed this kind of knowledge to solve them, and never will.</p>
            <p>I am all for asking practical coding exercises, where there are many good solutions, from brute force or greedy approaches to potentially more sophisticated ones. For example, asking to implement a justify text function <a href=\"https://leetcode.com/problems/text-justification/\">like this question</a> is a fair one: it's something I did when building a text rendering component on Windows Phone. You can solve this problem just by using an array and a few if/else statements, without any fancy data structures.</p>
            <p><strong>The reality is that many teams and companies are going overboard with algorithmic challenges.</strong> I can see the appeal of algorithmic questions: they give you signal in 45 minutes or less, and questions can be easily swapped around; thus, there is little harm if the question leaks. They are also easy to scale when recruiting, as you can have a question pool of 100+ questions, and any interviewer can evaluate any of them. Especially in Silicon Valley, it is more and more common to hear questions geared for dynamic programming or exotic data structures. These questions will help hire strong engineers - but also result in turning away people who would have excelled at a job that doesn't need advanced algorithms knowledge.</p>
            <p>To anyone reading whose company has a bar to hire people who know some of the advanced algorithms by heart: think again if this is what you need. I've hired fantastic teams at Skyscanner London and Uber Amsterdam without any tricky algorithm questions, covering no more than data structures and problem solving. You shouldn't need to know algorithms by heart. What you do need is awareness of the most common data structures and the ability to come up with simple algorithms to solve the problem at hand, as a toolset.</p>
            <h2>Data structures and algorithms are a toolset</h2>
            <p>If you work in a fast-moving, innovative tech company, you will almost certainly come across all sorts of data structures and different algorithm implementations in the codebase. When you build new and innovative solutions, you will often find yourself reaching to the right data structure. This is when you'll want to be aware of the options to choose from and their tradeoffs.</p>
            <p><strong>Data structures and algorithms are a tool that you should use with confidence when building software</strong>. Know these tools, and you'll be familiar with navigating codebases that use them. You'll also be far more confident in how to implement solutions to hard problems. You'll know the theoretical limits, the optimizations you can make, and you'll come up with solutions that are as good as they get - all tradeoffs considered.</p>
            <p>To get started, I recommend the following resources:</p>
            <ul><li>Read up on the hashtable, linked list, tree, graphs, heap, queue, and stack data structures. Play around with how to can use them in your language. GeeksforGeeks has a <a href=\"https://www.geeksforgeeks.org/overview-of-data-structures-set-1-linear-data-structures/?ref=lbp\">good overview on these</a>. For coding practice, I'd recommend the <a href=\"https://www.hackerrank.com/domains/data-structures\">HackerRank Data Structures collection</a>.</li>
            <li><strong><a href=\"https://geni.us/5v5U\">Grokking Algorithms</a></strong> by <a href=\"http://adit.io/\">Aditya Bhargava</a> is hands down the best guide on algorithms from beginners to experienced engineers. A very approachable, and visual guide, covering all that most people need to know on this topic. <strong>I am convinced that you don't need to know more about algorithms than this book covers. &nbsp;</strong>Buy it <a href=\"https://www.manning.com/books/grokking-algorithms?a_bid=0e014108&amp;a_aid=gorosz\">on the publisher's website</a> or <a href=\"https://geni.us/5v5U\">on Amazon</a>.</li>
            <li><a href=\"https://www.amazon.com/gp/product/1848000693/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=gregdoesit-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1848000693&amp;linkId=f2b71eb63361e28c617672a4ae87e97e\">The Algorithm Design &nbsp;Manual</a> and <a href=\"https://www.amazon.com/gp/product/032157351X/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=gregdoesit-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=032157351X&amp;linkId=626cdc6e69e57d907cf344635bade01c\">Algorithms: Fourth Edition</a> are both books I picked up back in the day to refresh some of the my university algorithm class studies. I gave up midway, and found them pretty dry, and not applicable to my day-to-day work.</li>
            </ul><p>Read comments on this article <a href=\"https://news.ycombinator.com/item?id=23841491\">on Hacker News</a>, <a href=\"https://www.reddit.com/r/programming/comments/hralcf/data_structures_algorithms_i_actually_used/\">on Reddit</a> and <a href=\"https://lobste.rs/s/n8tyip/data_structures_algorithms_i_actually\">on Lobsters</a>.</p>
            </div>"
    },
    {
        title: 'Common Plots of Economic History',
        author: "Morgan Housel",
        url: 'collaborativefund.com',
        full_url: 'https://www.collaborativefund.com/blog/common-plots-of-economic-history/', 
        reading_time: '20 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com/684x440/filters:no_upscale()/http%3A%2F%2Fwww.collaborativefund.com%2Fuploads%2Fviktor-forgacs-eNzH7lx4eS4-unsplash.jpg",
        content: 
            "<div lang=\"en\"><p>Complicated stuff can stem from a handful of common roots. Understanding those common roots can be more important than trying to wrap your head around complexity.</p>
            <p>To show you what I mean, let me tell you a story about stories.</p>
            <hr><p>Tens of millions of fictional stories have been published, written by authors of every age, culture, and life experience that exists. Most never knew each other or read each other’s work. Their stories range from prehistoric battles to science fiction. Stories are perhaps the most diverse things our imaginations have ever created.</p>
            <p>But all of these stories – every story, <a href=\"https://amzn.to/2PINhmU\">according to</a> author Christopher Booker – take shape in remarkably similar ways.</p>
            <p>“There are a small number of plots which are so fundamental to the way we tell stories that it is virtually impossible for any storyteller ever entirely to break away from them,” Booker writes.</p>
            <p>He argues only seven plots exist in the history of fictional storytelling: 1. Overcoming Monsters, 2. Rags to Riches, 3. The Quest, 4. Voyage and Return, 5. Rebirth, 6. Comedy, and 7. Tragedy.</p>
            <p>It’s not that all stories are the same. It’s that the plots that make good stories are less diverse than the characters and scenes that populate good stories. And the plots that catch readers’ attention are psychological common denominators among all of us, so the seven plots show up in stories told by cultures that have little in common.</p>
            <p>This is more than a casual observation. It’s a window into how people think. Booker writes:</p>
            <blockquote>
            <p>Whereas we are prepared to devote untold physical and mental resources to reaching out into the furthest recesses of the galaxy – in an attempt, as we like to think, to plumb every last secret of the universe – one of the greatest and most important mysteries is lying so close beneath our noses that we scarcely even recognise it to be a mystery at all … once we become acquainted with this symbolic language, there is literally no story in the world which cannot then be seen in a new light: because we have come to the heart of what stories are about and why we tell them.</p>
            </blockquote>
            <p>This point applies to many fields.</p>
            <p>Try to learn about people using a bottom-up approach of analyzing every character and scene from every culture and you will drown in complexity and anecdotes. There are too many stories, and even within a single culture the specific details of what people believe can be contradictory and filled with nuance. A top-down approach – looking for a few universal plot patterns – can be more useful, because it reveals things fundamental to how all people think, which are likely to be repeated in the future and relevant to your own situation.</p>
            <p>Booker’s insight is that you will better understand the characters and scenes from individual stories if you first appreciate the universal plots that show up in all stories.</p>
            <p>Which is an idea relevant to anyone interested in how the economy works.</p>
            <p>Economic history is complicated because it’s more than economics. It’s part politics, psychology, sociology, criminology, biology, military, technology, art, engineering, education, finance, etc.</p>
            <p>But within that tangle of complexity – different people from different cultures and different eras – is a lot of commonality.</p>
            <p>People tend to want the same economic things – security, power, admiration, fulfillment.</p>
            <p>They tend to use the same tactics to get those things – work, risk, incentives, persuasion, theft, control.</p>
            <p>And they tend to fall for the same flaws in pursuit of those things – overconfidence, pessimism, no room for error, underestimating how fast things can change, etc.</p>
            <p>Economic history may be complicated. But the common denominators of human behavior means there are, if you look, only a handful of broad story plots that pop up again and again, throughout history and around the globe, connecting the economic experiences of people who otherwise seem to have little in common.</p>
            <p>To better understand the characters and scenes from individual economic stories, we must first appreciate the universal plots that show up in all stories.</p>
            <p>Here are five.</p>
            <hr><p><strong>Plot 1:</strong> <strong>A good idea taken to the furthest extreme becomes indistinguishable from a terrible idea</strong>.</p>
            <p>A rule of thumb holds that no technological breakthrough is so important that it can’t be overdosed on. And a common plot in economic history is that the most important technological breakthroughs deliver such a high that they will, in fact, be overdosed on. Money and brains are attracted to good ideas more than they are capable of knowing the limits of good ideas.</p>
            <p>Take railroads.</p>
            <p>A one-word telegraph was sent at noon on May 10th, 1869, across the United States. “DONE” it said. The transcontinental railroad was finished.</p>
            <p>It’s hard to overstate how big a deal this was. Technology sat at such a low baseline in the 1860s that the railroad age transformed the nation in ways that are hard to imagine today. Historian Stephen Ambrose writes:</p>
            <blockquote>
            <p>This was the first time that news of an epochal event had been greeted with such celebration by so many people at the same time: Across the nation, bells pealed. Even the venerable Liberty Bell in Philadelphia was rung. Then came the boom of cannons, 220 of them in San Francisco, a hundred in Washington, D. C., countless fired off elsewhere. It was said that more cannons were fired in celebration than ever took part in the Battle of Gettysburg.</p>
            </blockquote>
            <p>To understand how important railroads were in the 19th century you have to consider life before their arrival. The biggest change the average American saw from the rail age was not their ability to travel; it was their diet. Historian Robert Gordon:</p>
            <blockquote>
            <p>In most parts of the United States people were virtually without fresh fruit and green vegetables from late autumn to late spring. The result was that innumerable Americans were in sluggish health during the months of late winter and early spring, when their diet was short of vitamins.</p>
            </blockquote>
            <p>Railroads changed this. “A major change between the 1890s and 1920s was the increasing availability of fresh vegetables in the winter as a result of refrigerated railroad cars,” writes Gordon.</p>
            <p>Rails transformed every inch of the economy, from military tactics to where people could live and work. It was for physical goods what the internet is for information. Few inventions were as powerful in their ability to deliver life-changing value to the masses as railroads were in the 19th century.</p>
            <p>Interesting, then, how many railroad investors lost their shirts during this period. It’s only a small exaggeration to say the number of railroad investors who prospered during this time rounds to zero.</p>
            <p>Excitement over rail’s potential sparked overbuilding and cutthroat competition that pulled the country into economic chaos at least three times. Railroads created booms and busts that make the 2000s housing bubble look tame. More than half of all rail companies were bankrupt by 1894. At times the overbuilding turned into madness, with identical tracks built side by side. Historian Robert Gordon again:</p>
            <blockquote>
            <p>Nearly twenty new miles of track a day, on average, were built for thirty years. In contrast to the single transcontinental line from Omaha to Sacramento linked together in the historic 1869 ceremony, by 1893 there were seven transcontinental lines, of which three were relatively close together, traversing Kansas and Nebraska, and two were quite close together in their paths through the sparsely populated Dakotas and Montana.</p>
            </blockquote>
            <p>The book <em>Lords of Creation</em> describes the economic battles:</p>
            <blockquote>
            <p>During the 1880s competition among the railroads got completely out of hand; it was easy for daring and unscrupulous plungers to build new lines simply as a form of economic blackmail—in order to be bought off by their competitors in self-defense; at one time there were five lines bidding against each other for the traffic between New York and Chicago, two more were under construction, and the passenger fare for the through trip had been beaten down to the ruinous figure of one dollar.</p>
            </blockquote>
            <p>The history of oil, cars, banks, housing, and technology are identical.</p>
            <p>Same for certain regulations, deregulations, social programs, management principles and business strategies.</p>
            <p>Good ideas get pushed to bad levels. The characters and scenes change throughout history, but the plot is timeless.</p>
            <p>This is not a flaw of capitalism. Being overconfident in your abilities and overoptimistic about your opportunities can be a useful intellectual shield, keeping you motivated in a world where most things are so hard that a coldly rational person might refuse to take any risk. And all economic opportunities will be exploited beyond their capacity, <a href=\"https://www.collaborativefund.com/uploads/Collaborative%20Fund%20--%20The%20Reasonable%20Formation%20of%20Unreasonable%20Things2.pdf\">because if markets never crashed</a> they wouldn’t be risky. And if they weren’t risky they would get really expensive. And when they get really expensive they crash. That’s why this plot persists.</p>
            <p>Few economic ideas are inherently good. At best, things are good at one level and ruinous at another.</p>
            <hr><p><strong>Plot 2: A competitive advantage that once looked invincible is squandered.</strong></p>
            <p>The only thing harder than gaining a competitive edge is not losing an advantage when you have one.</p>
            <p>If you were a movie scriptwriter and had to dream up a fake company with the strongest competitive advantage you can imagine you would probably come up with something that looks like what Sears was in real life in the 1970s.</p>
            <p>Sears was the largest retailer in the world, housed in the tallest building in the world, employing one of the largest workforces.</p>
            <p>“No one has to tell you you’ve come to the right place. The look of merchandising authority is complete and unmistakable,” <em>The New York Times</em> wrote of Sears in 1983.</p>
            <p>Sears was so good at retailing that in the 1970s and ‘80s it ventured into other areas, like finance. It owned Allstate Insurance, Discover credit card, the Dean Witter brokerage for your stocks and Coldwell Banker brokerage for your house.</p>
            <p>Sears was, in almost every way, the Amazon of its day: so dominant at retailing efficiency that it could spread its magic into unrelated industries, where it’d terrify rivals. <em>The Times</em> wrote in 1974:</p>
            <blockquote>
            <p>“Donald T. Regan, chairman of Merrill Lynch … indicated yesterday that the firm sees itself eventually as a Sears, Roebuck of the investment business. “We must get as efficient as possible to keep costs to the consumer down,” he said. “That’s what made Sears a success, and that’s a rule we must keep in mind.</p>
            </blockquote>
            <p>And then everything fell to pieces.</p>
            <p>Growing income inequality pushed consumers to either bargain or luxury goods, leaving Sears in the shrinking middle. Competition from Wal-Mart and Target – younger and hungrier – took off.</p>
            <p>By the late 2000s Sears was a shell of its former self. “YES, WE ARE OPEN” a sign outside my local Sears read – a reminder to customers who had all but written it off.</p>
            <p>The story of how Sears lost its competitive advantage is fascinating. But it is not unique. It is in many ways the default outcome of once-dominant companies.</p>
            <p>Going public is a sign that a company has found enough competitive advantage to scale into a large corporation. But almost 40% of all public companies <a href=\"https://www.collaborativefund.com/blog/risk/\">lost all of their value</a> from 1980 to 2014. The list of top-10 Fortune 500 companies that went bankrupt includes General Motors, Chrysler, Kodak – and Sears. Those indistinguishable from their former selves is longer, and includes General Electric, Time Warner, AIG and Motorola. Countries follow similar fates. At various points in the past the world’s scientific and economic progress has been dominated by China, Amsterdam, and the Middle East.</p>
            <p>Whenever a once-powerful thing loses an advantage it once it had it is tempting to ridicule the mistakes of its leaders. But it’s easy to overlook how many forces pull you away from a competitive advantage once you have one, specifically <em>because</em> you have one. Success has its own gravity. “The higher the monkey climbs a tree, the more you can see his ass,” T. Boone Pickens used to say.</p>
            <p>Five things contribute to the gravitational pull of competitive advantages.</p>
            <p>One is that being right instills confidence that you can’t be wrong, which is a devastating characteristic in a world where outlier success has a target on its back with competitors chasing in tow.</p>
            <p>Another is that success tends to lead to growth – usually by design – but a big organization is a different animal than a small one, and strategies that led to success at one size can be impossible at another. There is a long history of star fund managers from one decade underperforming in the next. Some of this is the unraveling of luck. But success also attracts capital, and a big fund can’t do some things a small fund can. The career version of this is the Peter Principle: talented people will keep getting promoted until they’re in over their head.</p>
            <p>A third is the irony that people often work hard to gain a competitive advantage for the express purpose of not having to work so hard at some point in the future. Hard work is in pursuit of a goal, and once that goal is met the relaxation that feels so justified removes paranoia that lets competitors and a changing world to creep in unnoticed.</p>
            <p>A fourth is a skill that’s valuable in one era may not extend to the next. You can work as hard and be as paranoid as you’ve always been, but if the world no longer values your skill, it’s a loss.</p>
            <p>And one-trick pony-ism is common, because a top reason competitive advantages were once valuable is an unshakable devotion to one big idea while rejecting all others.</p>
            <p>The last is that some success is owed to being in the right place at the right time. The reversion to reality that unmasks good luck is often only obvious with hindsight, and is both humbling and tempting to refuse to believe.</p>
            <p>The idea that advantage has a shelf life is a fundamental part of growth. It doesn’t have to be a tragedy – not all competitive advantages end like Sears. Great Britain lost the economic and military supremacy it held in the 19th century, but remained a pretty nice place to live in the 20th.</p>
            <hr><p><strong>Plot 3: Future progress is underrated because past progress is misunderstood.</strong></p>
            <p>There are times like the late 1990s when optimism is so great and so broad that we become blind to future risks. Here’s President Clinton <a href=\"https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-7\">in the 2000</a> State of the Union address – a year before September 11th and a new recession:</p>
            <blockquote>
            <p>We are fortunate to be alive at this moment in history. Never before has our Nation enjoyed, at once, so much prosperity and social progress with so little internal crisis and so few external threats. Never before have we had such a blessed opportunity and, therefore, such a profound obligation to build the more perfect Union of our Founders’ dreams.</p>
            <p>My fellow Americans, the state of our Union is the strongest it has ever been.</p>
            </blockquote>
            <p>Despite centuries of progress and growth, these moods are an outlier. Pessimism over what lies ahead is far more common.</p>
            <p>Gallup has been asking Americans for more than four decades, “Are you satisfied with the way things are going in the U.S. right now?”</p>
            <p>The average percent of Americans answering “no” since 1969 is 63%.</p>
            <p>What’s interesting is that Gallup asks a follow-up question: “Are you satisfied with the way things are going in your own life right now?”</p>
            <p>There, the average “no” response is just 15.8%.</p>
            <p>People tend to feel good about themselves but pessimistic about others. This helps explain why the congressional reelection rate is so high when the approval rate is so low: most people approve of their own congress member but despise everyone else’s. It creates a situation where most people you know are probably reasonably happy, but pessimism over the direction of the country and its future remains stubbornly high. And when people are pessimistic about others, a natural path is to discount what those other people are capable of in the future.</p>
            <p>This was true in the early 1900s, when people <a href=\"https://www.collaborativefund.com/blog/the-wonders-of-the-future/\">asked if the age of invention</a> – led by people like Thomas Edison – was over, a lucky blip that couldn’t repeat.</p>
            <p>It was true in 1950, when we wondered if the advent of the nuclear bomb would cast humanity back into the stone age.</p>
            <p>It’s been true in the last decade when, as data scientist Jeff Hammerbacher put it, “The best minds of my generation are thinking about how to make people click ads. That sucks.”</p>
            <p>The pessimists may be right one day. Squandering a competitive advantage is a common plot after all.</p>
            <p>But underestimating future growth – particularly of people in general, not necessarily individual companies or countries – is also a common plot, for a few reasons.</p>
            <p>One is that viewing past progress as a one-time miracle misses how much of progress is incremental. If you think the computer, the airplane, or the internet were just “invented,” of course you’ll be pessimistic about our ability to do anything as meaningful in the future. They look like one-in-a-billion discoveries that can’t be repeated. But breakthroughs never occur in isolation. They’re the product of countless little discoveries – often meaningless on their own – that someone eventually ties together. Thomas Edison told this to a pessimistic newspaper reporter in 1908:</p>
            <blockquote>
            <p>You can never tell what apparently small discovery will lead to. Somebody discovers something and immediately a host of experimenters and inventors are playing all the variations upon it. Take Faraday’s experiments with copper disks. Looked like a scientific plaything, didn’t it? Well, it eventually gave us the trolly car. Or take Crooke’s tubes; looked like an academic discovery, but we got the X-ray from it. A whole host of experimenters are at work today; what great things their discoveries will lead to, no one can foretell.</p>
            </blockquote>
            <p>More recently, Safi Bahcall wrote in his book <em>Loonshots</em> about the origins of Polaroid film:</p>
            <blockquote>
            <p>Sick dogs that were fed quinine to treat parasites showed an unusual type of crystal in their urine. Those microscopic crystals, called herapathite, turned out to be the highest-quality polarizers ever discovered.</p>
            </blockquote>
            <p>When it’s never obvious what little discoveries of the day will eventually merge into big ones, it’s easy to assume that there will be no big ones. Which is almost always wrong.</p>
            <p>Another reason is the assumption that the existence of big current problems will prohibit future progress. What this misses is that most progress – particularly technological breakthroughs – feed off big current problems. The push to solve problems rises when there’s a panicked necessity like a war or recession that requires solutions to problems immediately. Comfortable lives incentivize more utopian visions of the future than they do late nights in the laboratory. The back-to-back Great Depression, World War II, and Cold War are three of the worst things that happened in the 20th century. They also <a href=\"https://www.collaborativefund.com/blog/careful-what-you-wish-for/\">sparked the greatest scientific discoveries</a> of the 20th century.</p>
            <p>Last, in real time it almost always looks like progress over the previous decade or so has stalled, seeming to verify that we’ve reached the limit of our innovation. The 1960s brought the computer. The 1970s, the microprocessor. The 1980s, the PC. 1990s, the internet. The 2000s, mobile. The 2010s … I don’t know, viral GIFs? This same plot can be repeated at almost any point in history with different characters. The reason the near past always looks less innovative than the further past is because it <a href=\"https://www.collaborativefund.com/blog/when-you-change-the-world-and-no-one-notices/\">often takes a decade or more</a> for breakthroughs to be noticed. The best work of the last decade won’t be recognized for years to come.</p>
            <hr><p><strong>Plot 4: Surprises are constant, and not necessarily because we’re bad at predicting, but because everything important in the economy is driven by power laws where a tiny portion of things are responsible for the majority of outcomes, and it’s impossible for any one forecaster to track every moving part.</strong></p>
            <p>Fifteen billion people were born in the 19th and 20th centuries. But try to imagine how different the world would be today if just seven of them never existed:</p>
            <ul><li>
            <p>Adolf Hitler</p>
            </li>
            <li>
            <p>Joseph Stalin</p>
            </li>
            <li>
            <p>Mao Zedong</p>
            </li>
            <li>
            <p>Thomas Edison</p>
            </li>
            <li>
            <p>Bill Gates</p>
            </li>
            <li>
            <p>Martin Luther King</p>
            </li>
            </ul><p>I’m not sure that’s the most meaningful list. But almost everything about the world today – from borders to technology to social norms – would be different if these seven people hadn’t left their mark. Another way to put this is that 0.00000000004% of people were responsible for perhaps the majority of the world’s direction over the last century.</p>
            <p>The same goes for projects, innovations, and events. Imagine the last century without:</p>
            <ul><li>
            <p>The Manhattan Project</p>
            </li>
            <li>
            <p>Vaccines</p>
            </li>
            <li>
            <p>Antibiotics</p>
            </li>
            <li>
            <p>September 11th</p>
            </li>
            <li>
            <p>The fall of the Soviet Union</p>
            </li>
            </ul><p>How many projects and events occurred in the 20th century? Billions, trillions – who knows. But those six alone impacted the world orders upon orders of magnitude more than others.</p>
            <p>The thing that makes tail events easy to underappreciate is how easy it is to underestimate how things compound. How, for example, 9/11 prompted the Fed to cut interest rates, which helped drive the housing bubble, which led to the financial crisis, which led to a poor jobs market, which led tens of millions to seek a college education, which led to $1.6 trillion in student loans with a 10.8% default rate. It’s not intuitive to link 19 hijackers to the current weight of student loans, but such is the power of compounding in a tail-driven world. And you can, I believe, <a href=\"https://www.google.com/search?q=three/+big/+things&amp;oq=three/+big/+things&amp;aqs=chrome..69i57j0l4j69i60j69i61j69i60.1858j0j7&amp;sourceid=chrome&amp;ie=UTF-8\">tie the majority of what’s happening</a> in the world today to just a handful of past events.</p>
            <p>The most common plot of economic history is the role of surprises. The reason surprises occur is not because our models are wrong or our intelligence is low. It’s because the odds that Adolf Hitler’s parents argued on the evening nine months he was born were the same as them conceiving a child. Technology is hard to predict because Bill Gates may have died from polio if Jonas Salk got cranky and gave up on his quest to find a vaccine. The reason we couldn’t predict the student loan growth is because an airport security guard may have confiscated a hijacker’s knife on 9/11. That’s all there is to it.</p>
            <p>In those alternative worlds another monster may have risen in Hitler’s absence, another tech genius may have become the Bill Gates of their day, and without 9/11 we may still have tripped into a financial crisis. Imagining an alternative world without the tail events we’ve had isn’t to say the world could have been better or worse – just that it would have been very different. And when a little difference in one era compounds into a massive change in another, surprises become a recurring plot in economic history.</p>
            <hr><p><strong>Plot 5:</strong> <strong>The ability to believe things that aren’t true or haven’t happened yet is the foundation of all economic growth and decline.</strong></p>
            <p>Stories of things happening are not as common as things happening because of stories.</p>
            <p>In his book <em>Fantasyland</em>, Kurt Andersen argues that a founding virtue of America is its willingness, if not desire, to believe things that aren’t true.</p>
            <p>It started with the whole idea of the New World, when 16th century Europeans were told of a magical land across the Atlantic filled with abundance, only to find a malarial swamp when they arrived.</p>
            <p>It continued with the Salem Witch trials, onto modern things like P.T. Barnum and Hollywood.</p>
            <p>“From the start, our ultra-individualism was attached to epic dreams and epic fantasies—every citizen was free to believe absolutely anything, or to pretend to be absolutely anybody,” Andersen writes.</p>
            <p>Every society tells fictional stories and is willing to believe them to some degree. But Americans’ propensity to believe in things that aren’t yet verified or are easily debunked with the known science is in many ways not only unparalleled, but foundational to the whole idea of this country.</p>
            <p>And a willingness to believe in things that haven’t happened isn’t always a bad thing. It’s the root of most growth. Most economic growth comes from optimism, and optimism requires a degree of believing in things you can not or have not yet verified. The ability to believe in things that aren’t yet real to such a degree that you spend your life chasing them is a characteristic we might take for granted. But it’s responsible for most of the things we get to enjoy in real life.</p>
            <p>And some of the stuff we’re embarrassed about, too.</p>
            <p>A quirk breakthrough technology is that it’s a big deal because no one had thought of it before, and no one thinking of it before means it’s likely either illogical or appears to violate previously accepted rules. So pursuing a breakthrough – as an entrepreneur, investor, or consumer – requires taking a leap of faith in your own abilities, or those of someone else, often a stranger.</p>
            <p>So no one should be surprised when some of those leaps of faith end up wrong. Well-meaning leaps of faith fail because the odds of success were low to begin with. Others fail because we took a leap of faith on someone whose claims couldn’t be verified and <a href=\"https://www.collaborativefund.com/blog/theranos-lessons/\">it turns out they were</a> misleading all along. Whatever the reason, a willingness to believe in things that aren’t true accounts for the majority of why businesses fail, investors underperform, careers implode, and countries decline. We’ve had good growth, good productivity, great innovation. With that comes a mountain of stories of people who were wrong in their views. Those two things don’t repel each other; they work hand in hand.</p>
            <p>Stock markets owe most of their value to future earnings, which are merely a story.</p>
            <p>WeWork’s $47 billion was also built on a story.</p>
            <p>Steve Jobs told a story. So did Elizabeth Holmes.</p>
            <p>Capitalism – <a href=\"https://www.collaborativefund.com/blog/the-greatest-story-ever-told/\">that’s a whole story in itself</a>.</p>
            <p>The counterintuition that the same storytelling traits that enable growth also ensure occasional decline is one of the most fascinating and recurring plots of economic history. The stories we tell and believe about the economy are more powerful and important than any tangible thing we have in the economy, and if you remove the role of luck, most of the difference in outcomes between people and countries is owed to differences in the ability to tell and believe stories. German author Elias Canette wrote:</p>
            <blockquote>
            <p>The largest crowds are drawn by the storytellers. It is around them that the people throng most densely and stay longest… their words come from further off and hang longer in the air than those of ordinary people.</p>
            </blockquote>
            <p>Which brings us back to where this story began, with <em>The Seven Plots</em>. It argues all stories, however diverse, are just vehicles for one of a few basic human urges to travel upon. The better you understand a few common story plots that control our economy, the more sense you can make of the diverse cast of characters.</p>
            </div>"
    },
    {
        title: 'If Sapiens were a blog post',
        author: "Neil Kakkar",
        url: 'neilkakkar.com',
        full_url: 'https://neilkakkar.com/sapiens.html', 
        reading_time: '32 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens.jpg",
        content: 
            "<div>
            <map name=\"image-map\"><area shape=\"poly\" coords=\"0,77,115,77,80,150,0,150\" href=\"#development-of-brains\" title=\"Development of Brains\" alt=\"Taming fire\"><area shape=\"poly\" coords=\"115,77,230,77,195,150,80,150\" href=\"#cognitive-revolution\" title=\"Cognitive Revolution\" alt=\"cognitive revolution\"><area shape=\"poly\" coords=\"230,77,345,77,310,150,195,150\" href=\"#agricultural-revolution\" title=\"Agricultural Revolution\" alt=\"agriculture revolution\"><area shape=\"poly\" coords=\"345,77,460,77,415,150,310,150\" href=\"#unification-of-humankind\" title=\"Unification of Humanity\" alt=\"unificiation\"><area shape=\"poly\" coords=\"460,77,575,77,520,150,415,150\" href=\"#scientific-revolution\" title=\"Scientific Revolution\" alt=\"scientific revolution\"><area shape=\"poly\" coords=\"575,77,690,77,635,150,540,150\" href=\"#industrial-revolution\" title=\"Industrial Revolution\" alt=\"industrial revolution\"><area shape=\"poly\" coords=\"690,77,805,77,750,150,655,150\" href=\"#the-end-of-homo-sapiens\" title=\"The Present\" alt=\"present\"></map><figure><a rel=\"noopener\" href=\"https://amzn.to/2WWNsjq\">
            <div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens.jpg\">
            </figure>
            </div></a>
            </figure><p>I spent over 25 hours building a cut-down version of Sapiens. The goal? Future-me should be happy to read this once future-me forgets how we evolved. It’s massive for a blog post, just under 30 minutes, but that’s the best I could do, condensing 9 hours worth of material.</p>
            <p>I’ve tried to keep editing to a minimum: It’s the original text, edited to ensure it still flows like the book.</p>
            <p>You can get the book <a rel=\"noopener\" href=\"https://amzn.to/2WWNsjq\">here</a><sup><a href=\"https://neilkakkar.com/sapiens.html#fn:2\">1</a></sup></p>
            <p>The best way of navigating is clicking on the images. These are best experienced on a tablet or a laptop. I’ve also included the table of contents, which work well on every screen size.</p>
            <nav>
            </nav><h2>Development of brains</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-1.jpg\">
            </figure>
            </div></figure><p>What caused our brains to develop? We’re not sure.</p>
            <p>It doesn’t seem likely. A larger brain needs more energy and thus reduces the chance you’ll survive. Getting more energy meant hunting more.</p>
            <p>One contributing factor was the domestication of fire. Fire paved the way for cooking.</p>
            <p>Whereas chimpanzees spend 5 hours a day chewing raw food, a single hour suffices for people eating cooked food. The advent of cooking enabled humans to eat more kinds of food, to devote less time to eating, and to make do with smaller teeth and shorter intestines. Some scholars believe there is a direct link between the advent of cooking, the shortening of the human intestinal track, and the growth of the human brain. Since long intestines and large brains are both massive energy consumers, it’s hard to have both. By shortening the intestines and decreasing their energy consumption, cooking inadvertently opened the way to the jumbo brains.</p>
            <p>And, we weren’t alone. Competing with us were the Neanderthals, among other species. They were stronger, they had bigger brains, and they could survive the cold. How come, then, did we “win”?</p>
            <p>We aren’t sure. The most likely answer is the very thing that makes the debate possible: Homo sapiens conquered the world thanks above all to its unique language.</p>
            <h2>Cognitive Revolution</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-2.jpg\">
            </figure>
            </div></figure><p>The appearance of new ways of thinking and communicating, between 70,000 and 30,000 years ago, constitutes the Cognitive Revolution. What caused it? We’re not sure. The most commonly believed theory argues that accidental genetic mutations changed the inner wiring of the brains of Sapiens, enabling them to think in unprecedented ways and to communicate using a new type of language. We might call it the Tree of Knowledge mutation.</p>
            <p>What’s special about our language?</p>
            <p>Our language is amazingly supple. We can connect a limited number of sounds and signs to produce an infinite number of sentences, each with a distinct meaning. We can thereby ingest, store and communicate a prodigious amount of information about the surrounding world. A monkey can yell to its comrades, ‘Careful! A lion!’ But a modern human can tell her friends that this morning, near the bend in the river, she saw a lion tracking a herd of bison. She can then describe the exact location, including the different paths leading to the area. With this information, the members of her band can put their heads together and discuss whether they ought to approach the river in order to chase away the lion and hunt the bison.</p>
            <p>Yet the truly unique feature of our language is not its ability to transmit information about men and lions. Rather, it’s the ability to transmit information about things that do not exist at all. As far as we know, only Sapiens can talk about entire kinds of entities that they have never seen, touched or smelled.</p>
            <p>Using language, lots of humans could work together, form a tribe, help each other, and hunt together.</p>
            <p>However, communicating with lots of people brings with it new co-ordination problems. The critical threshold for a group interacting together is 150 people.</p>
            <p>How did Homo sapiens manage to cross this critical threshold, eventually founding cities comprising tens of thousands of inhabitants and empires ruling hundreds of millions? The secret was probably the appearance of fiction. Large numbers of strangers can cooperate successfully by believing in common myths. This is an imagined reality.</p>
            <p>Unlike lying, an imagined reality is something that everyone believes in, and as long as this communal belief persists, the imagined reality exerts force in the world.</p>
            <p>In other words, while the behaviour patterns of archaic humans remained fixed for tens of thousands of years, Sapiens could transform their social structures, the nature of their interpersonal relations, their economic activities and a host of other behaviours within a decade or two.</p>
            <p>For example, consider trade. Trade cannot exist without trust, and it is very difficult to trust strangers. The global trade network of today is based on our trust in such fictional entities as the dollar, the Federal Reserve Bank, and the totemic trademarks of corporations. When two strangers in a tribal society want to trade, they will often establish trust by appealing to a common god, mythical ancestor or totem animal.</p>
            <p>Our Tree of Knowledge mutation was significant. However, it’s not just our biology that brought us where we are today.</p>
            <ol><li>
            <p>Biology sets the basic parameters for the behaviour and capacities of Homo sapiens. The whole of history takes place within the bounds of this biological arena.</p>
            </li>
            <li>
            <p>This arena is extraordinarily large, allowing Sapiens to play an astounding variety of games. Thanks to their ability to invent fiction, Sapiens create more and more complex games, which each generation develops and elaborates even further</p>
            </li>
            <li>
            <p>Consequently, in order to understand how Sapiens behave, we must describe the historical evolution of their actions.</p>
            </li>
            </ol><h2>Agricultural Revolution</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-3.jpg\">
            </figure>
            </div></figure><p>The currency of evolution is neither hunger nor pain, but rather copies of DNA helixes. Just as the economic success of a company is measured only by the number of dollars in its bank account, not by the happiness of its employees, so the evolutionary success of a species is measured by the number of copies of its DNA. If no more DNA copies remain, the species is extinct, just as a company without money is bankrupt. If a species boasts many DNA copies, it is a success, and the species flourishes. From such a perspective, 1,000 copies are always better than a hundred copies. This is the essence of the Agricultural Revolution: the ability to keep more people alive under worse conditions.</p>
            <p>Rather than heralding a new era of easy living, the Agricultural Revolution left farmers with lives generally more difficult and less satisfying than those of foragers. Hunter-gatherers spent their time in more stimulating and varied ways, and were less in danger of starvation and disease. The Agricultural Revolution certainly enlarged the sum total of food at the disposal of humankind, but the extra food did not translate into a better diet or more leisure. Rather, it translated into population explosions and pampered elites. The average farmer worked harder than the average forager, and got a worse diet in return. The Agricultural Revolution was history’s biggest fraud.</p>
            <p>Compared to foraging, working a bit harder on the farm meant more food. Settling down on the farm also meant greater freedom to reproduce - you’re not travelling all the time, so you can afford to have more children. This in turn, meant a higher population. And a higher population meant higher food requirements. Thus, the farmers were forced to work even harder.</p>
            <p>Why didn’t humans abandon farming when the plan backfired? Partly because it took generations for the small changes to accumulate and transform society and, by then, nobody remembered that they had ever lived differently. And partly because population growth burned humanity’s boats. If the adoption of ploughing increased a village’s population from a hundred to 110, which ten people would have volunteered to starve so that the others could go back to the good old times? There was no going back. The trap snapped shut.</p>
            <blockquote>
            <p>One of history’s few iron laws is that luxuries tend to become necessities and spawn new obligations</p>
            </blockquote>
            <p>This discrepancy between evolutionary success and individual suffering is perhaps the most important lesson we can draw from the Agricultural Revolution. When we study the narrative of plants such as wheat and maize, maybe the purely evolutionary perspective makes sense. Yet in the case of animals such as cattle, sheep and Sapiens, each with a complex world of sensations and emotions, we have to consider how evolutionary success translates into individual experience.</p>
            <h3>Imagined Realities - Solving the co-ordination problem</h3>
            <p>According to the science of biology, people were not ‘created’. They have evolved. And they certainly did not evolve to be ‘equal’. The idea of equality is inextricably intertwined with the idea of creation.</p>
            <p>A natural order is a stable order. There is no chance that gravity will cease to function tomorrow, even if people stop believing in it. In contrast, an imagined order is always in danger of collapse, because it depends upon myths, and myths vanish once people stop believing in them. In order to safeguard an imagined order, continuous and strenuous efforts are imperative. Some of these efforts take the shape of violence and coercion.</p>
            <p>To say that a social order is maintained by military force immediately raises the question: what maintains the military order? It is impossible to organise an army solely by coercion. At least some of the commanders and soldiers must truly believe in something, be it God, honour, motherland, manhood or money.</p>
            <p>To maintain an imagined order, we need people who believe in it - the military, the elites, and the peasants.</p>
            <p>Three main factors prevent people from realising that the order organising their lives exists only in their imagination:</p>
            <h4><strong>The imagined order is embedded in the material world.</strong></h4>
            <p>For example, houses have small rooms for every person living inside. This architecture promotes Individualism.</p>
            <h4><strong>The imagined order shapes our desires.</strong></h4>
            <p>Most people do not wish to accept that the order governing their lives is imaginary, but in fact every person is born into a pre-existing imagined order, and his or her desires are shaped from birth by its dominant myths. Our personal desires thereby become the imagined order’s most important defences.</p>
            <p>The elite of ancient Egypt spent their fortunes building pyramids and having their corpses mummified, but none of them thought of going shopping in Babylon or taking a skiing holiday in Phoenicia. People today spend a great deal of money on holidays abroad because they are true believers in the myths of romantic consumerism. Romanticism tells us that in order to make the most of our human potential we must have as many different experiences as we can.</p>
            <p>A wealthy man in ancient Egypt would never have dreamed of solving a relationship crisis by taking his wife on holiday to Babylon. Instead, he might have built for her the sumptuous tomb she had always wanted.</p>
            <p>Few question the myths that cause us to desire the pyramid in the first place.</p>
            <h4><strong>The imagined order is inter-subjective.</strong></h4>
            <p>Even if by some superhuman effort I succeed in freeing my personal desires from the grip of the imagined order, I am just one person. In order to change the imagined order I must convince millions of strangers to cooperate with me. For the imagined order is not a subjective order existing in my own imagination – it is rather an inter-subjective order, existing in the shared imagination of thousands and millions of people.</p>
            <p>The inter-subjective is something that exists within the communication network linking the subjective consciousness of many individuals. If a single individual changes his or her beliefs, or even dies, it is of little importance. However, if most individuals in the network die or change their beliefs, the inter-subjective phenomenon will mutate or disappear</p>
            <p>A change of such magnitude can be accomplished only with the help of a complex organisation, such as a political party, an ideological movement, or a religious cult. However, in order to establish such complex organisations, it’s necessary to convince many strangers to cooperate with one another. And this will happen only if these strangers believe in some shared myths. It follows that in order to change an existing imagined order, we must first believe in an alternative imagined order.</p>
            <h3>Preserving the imagined reality</h3>
            <p>After the imagined order, comes a means of facilitating and propagating the order.</p>
            <p>Between the years 3500 BC and 3000 BC, some unknown Sumerian geniuses invented a system for storing and processing information outside their brains, one that was custom-built to handle large amounts of mathematical data. The Sumerians thereby released their social order from the limitations of the human brain, opening the way for the appearance of cities, kingdoms and empires. The data-processing system invented by the Sumerians is called ‘writing’.</p>
            <p>Animals that engage strangers in ritualised aggression do so largely by instinct – puppies throughout the world have the rules for rough-and-tumble play hard-wired into their genes. But human teenagers have no genes for football. They can nevertheless play the game with complete strangers because they have all learned an identical set of ideas about football. These ideas are entirely imaginary, but if everyone shares them, we can all play the game.</p>
            <p>With greater numbers and growing imagined orders, we needed a system to organise the rules: No one person could remember all of them.</p>
            <p>Just imprinting a document in clay is not enough to guarantee efficient, accurate and convenient data processing. That requires methods of organisation like catalogues, methods of reproduction like photocopy machines, methods of rapid and accurate retrieval like computer algorithms, and pedantic librarians who know how to use these tools. Inventing such methods proved to be far more difficult than inventing writing.</p>
            <h3>More individual suffering</h3>
            <p>Understanding human history in the millennia following the Agricultural Revolution boils down to a single question: how did humans organise themselves in mass-cooperation networks, when they lacked the biological instincts necessary to sustain such networks? The short answer is that humans created imagined orders and devised scripts. These two inventions filled the gaps left by our biological inheritance.</p>
            <p>However, for many, this was a dubious blessing.</p>
            <p>With an order came social and political hierarchies.</p>
            <p>Throughout history, and in almost all societies, concepts of pollution and purity have played a leading role in enforcing social and political divisions and have been exploited by numerous ruling classes to maintain their privileges.</p>
            <p>If you want to keep any human group isolated – women, Jews, Roma, gays, blacks – the best way to do it is convince everyone that these people are a source of pollution.</p>
            <p>This led to a vicious circle of exploitation of people “lower” on the social ladder.</p>
            <p>These Social structures form depending on what hierarchy is visible or convenient.</p>
            <p>One example is the dark-skinned slaves.</p>
            <p>European conquerors of America chose to import slaves from Africa rather than from Europe or East Asia due to three circumstantial factors.</p>
            <p>Firstly, Africa was closer, so it was cheaper to import slaves from Senegal than from Vietnam.</p>
            <p>Secondly, in Africa there already existed a well-developed slave trade (exporting slaves mainly to the Middle East), whereas in Europe slavery was very rare. It was obviously far easier to buy slaves in an existing market than to create a new one from scratch</p>
            <p>Thirdly, and most importantly, American plantations in places such as Virginia, Haiti and Brazil were plagued by malaria and yellow fever, which had originated in Africa. Africans had acquired over the generations a partial genetic immunity to these diseases, whereas Europeans were totally defenseless and died in droves. It was consequently wiser for a plantation owner to invest his money in an African slave than in a European slave or indentured labourer</p>
            <p>Paradoxically, genetic superiority (in terms of immunity) translated into social inferiority.</p>
            <p>Different societies adopt different kinds of imagined hierarchies. Race is very important to modern Americans but was relatively insignificant to medieval Muslims. Caste was a matter of life and death in medieval India, whereas in modern Europe it is practically non-existent. One hierarchy, however, has been of supreme importance in all known human societies: the hierarchy of gender. People everywhere have divided themselves into men and women. And almost everywhere men have got the better deal, at least since the Agricultural Revolution.</p>
            <p>How can we distinguish what is biologically determined from what people merely try to justify through biological myths? A good rule of thumb is ‘Biology enables, Culture forbids.’ Biology is willing to tolerate a very wide spectrum of possibilities. It’s culture that obliges people to realise some possibilities while forbidding others. Biology enables women to have children – some cultures oblige women to realise this possibility. Biology enables men to enjoy sex with one another – some cultures forbid them to realise this possibility.</p>
            <p>Thus, the Agricultural Revolution led to much individual suffering. It led to more humans with a limited diet, higher risk of starvation, imagined realities to co-ordinate so many people, scripts to preserve that order, and social hierarchies to maintain that order. At the same time, it was an evolutionary success - much more people were alive at any given time.</p>
            <h2>Unification of Humankind</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-4.jpg\">
            </figure>
            </div></figure><p>All imagined orders created untill 1000 BC had a clear boundary. In united Egypt, everyone outside the country were barbarians. Our culture vs theirs. As Seth Godin puts it today, “People like us, do things like this.” - a microculture.</p>
            <p>The first millennium BC witnessed the appearance of three potentially universal orders, whose devotees could for the first time imagine the entire world and the entire human race as a single unit governed by a single set of laws. Everyone was ‘us’, at least potentially. There was no longer ‘them’.</p>
            <p>The first universal order to appear was economic: the monetary order.</p>
            <p>The second universal order was political: the imperial order.</p>
            <p>The third universal order was religious: the order of universal religions such as Buddhism, Christianity and Islam.</p>
            <h3>Money</h3>
            <p>The rise of cities and kingdoms and the improvement in transport infrastructure brought about new opportunities for specialisation. Densely populated cities provided full-time employment not just for professional shoemakers and doctors, but also for carpenters, priests, soldiers and lawyers.</p>
            <p>Everyone always wants money. This is perhaps its most basic quality. Everyone always wants money because everyone else also always wants money, which means you can exchange money for whatever you want or need.</p>
            <h4>How Money Works</h4>
            <p>Money is a psychological construct. It works by converting matter into mind. But why does it succeed? Why should anyone be willing to exchange a fertile rice paddy for a handful of useless cowry shells? Why are you willing to flip hamburgers, sell health insurance or babysit three obnoxious brats when all you get for your exertions is a few pieces of coloured paper?</p>
            <p>Its a system of mutual trust, and not just any system of mutual trust: money is the most universal and most efficient system of mutual trust ever devised.</p>
            <p>The first iteration of money were set weights made of pure silver. Everyone had to keep scales handy to measure them. Counterfeiting was easy. You couldn’t be sure it’s really pure silver.</p>
            <p>Coins helped solve these problems. The mark imprinted on them testifies to their exact value, so the shoemaker doesn’t have to keep a scale on his cash register. More importantly, the mark on the coin is the signature of some political authority that guarantees the coin’s value.</p>
            <p>With notes, we’ve extended the same basic concept. The Treasury guarantees the value of the notes, the number signifies its value.</p>
            <h4>Proliferation of gold</h4>
            <p>After silver, came gold. The Europeans believed in gold: They considered it to be very valuable. They wanted to trade gold with the world, and turns out, almost everyone else did, too.</p>
            <p>What would have happened to the global economy if the Chinese had refused to accept payment in gold and silver? Yet why should Chinese, Indians, Muslims and Spaniards – who belonged to very different cultures that failed to agree about much of anything – nevertheless share the belief in gold?</p>
            <p>Once trade is established, thanks to laws of supply and demand, the value of gold in India and the Mediterranean would be quite similar. The mere fact that Mediterranean people believed in gold would cause Indians to start believing in it as well.</p>
            <blockquote>
            <p>When everything is convertible, and when trust depends on anonymous coins and cowry shells, it corrodes local traditions, intimate relations and human values, replacing them with the cold laws of supply and demand.</p>
            </blockquote>
            <h3>Empires</h3>
            <p>It is difficult to rule an empire in which every little district has its own set of laws, its own form of writing, its own language and its own money. Standardisation was a boon to emperors. Thus, empires spread a common culture.</p>
            <p>A second and equally important reason was to gain legitimacy within the districts. This led to the Imperial Cycle.</p>
            <h4>The Imperial Cycle</h4>
            <ul><li>A small group establishes a big empire.</li>
            <li>An imperial culture is forged.</li>
            <li>The imperial culture is adopted by the subjects people.</li>
            <li>The subject people demand equal status in the name of common imperial values.</li>
            <li>The empires founders lose their dominance.</li>
            <li>The imperial culture continues to flourish and develop.</li>
            </ul><p>The global empire being forged before our eyes is not governed by any particular state or ethnic group. Much like the Late Roman Empire, it is ruled by a multi-ethnic elite, and is held together by a common culture and common interests. Throughout the world, more and more entrepreneurs, engineers, experts, scholars, lawyers and managers are called to join the empire. They must ponder whether to answer the imperial call or to remain loyal to their state and their people. More and more choose the empire.</p>
            <p>This multi ethnic group of people come together to create a global empire – not via war, we are easily more sophisticated than that, but via economic and political powers.</p>
            <p>As the cycle goes, people start freely adopting the ideals of this global empire, then they demand to be equals. Then the rulers die off and the people continue the empire.</p>
            <h3>Religion</h3>
            <p>Today religion is often considered a source of discrimination, disagreement and disunion. Yet, in fact, religion has been the third great unifier of humankind, alongside money and empires.</p>
            <p>Since all social orders and hierarchies are imagined, they are all fragile, and the larger the society, the more fragile it is. The crucial historical role of religion has been to give superhuman legitimacy to these fragile structures. Religions assert that our laws are not the result of human caprice, but are ordained by an absolute and supreme authority. This helps place at least some fundamental laws beyond challenge, thereby ensuring social stability.</p>
            <p>Religion can thus be defined as a system of human norms and values that is founded on a belief in a superhuman order. This involves two distinct criteria:</p>
            <ol><li>
            <p>Religions hold that there is a superhuman order, which is not the product of human whims or agreements. Professional football is not a religion, because despite its many laws, rites and often bizarre rituals, everyone knows that human beings invented football themselves, and FIFA may at any moment enlarge the size of the goal or cancel the offside rule.</p>
            </li>
            <li>
            <p>Based on this superhuman order, religion establishes norms and values that it considers binding. Many Westerners today believe in ghosts, fairies and reincarnation, but these beliefs are not a source of moral and behavioural standards. As such, they do not constitute a religion.</p>
            </li>
            </ol><p>In order to unite under its aegis a large expanse of territory inhabited by disparate groups of human beings, a religion must possess two further qualities. First, it must espouse a universal superhuman order that is true always and everywhere. Second, it must insist on spreading this belief to everyone. So, it must be universal and missionary.</p>
            <p>It’s interesting to watch the evolution of religion.</p>
            <details><summary>Expand: Evolution of Religion</summary><p>Earlier, Animism was the dominant belief system. Human norms and values had to take into consideration the outlook and interests of a multitude of other beings, such as animals, plants, fairies and ghosts.</p>
            <p>Local beliefs led to rules. Animals and plants were equal. Don’t cut down this fig tree lest the fig-tree spirit becomes angry. If the spirit stays happy, we’ll get lots of figs.</p>
            <p>But with agriculture, humans started rearing plants and animals. They couldn’t harm us anymore. We obtained control. They switched from mysteries to possessions.</p>
            <p>Much of ancient mythology is a legal contract in which humans promise everlasting devotion to the gods in exchange for mastery over plants and animals.</p>
            <p>The Agricultural Revolution initially had a far smaller impact on the status of other members of the animist system, such as rocks, springs, ghosts and demons. However, these too gradually lost status in favour of the new gods.</p>
            <p>As long as people lived their entire lives within limited territories of a few hundred square kilometers, most of their needs could be met by local spirits. But once kingdoms and trade networks expanded, people needed to contact entities whose power and authority encompassed a whole kingdom or an entire trade basin.</p>
            <p>The attempt to answer these needs led to the appearance of polytheistic religions (from the Greek: poly = many, theos = god).</p>
            <p>A terrible flood might wipe out billions of ants, grasshoppers, turtles, antelopes, giraffes and elephants, just because a few stupid Sapiens made the gods angry. Polytheism thereby exalted not only the status of the gods, but also that of humankind.</p>
            <p>The fundamental insight of polytheism, which distinguishes it from monotheism, is that the supreme power governing the world is devoid of interests and biases, and therefore it is unconcerned with the mundane desires, cares and worries of humans.</p>
            <p>Since polytheists believe, on the one hand, in one supreme and completely disinterested power, and on the other hand in many partial and biased powers, there is no difficulty for the devotees of one god to accept the existence and efficacy of other gods. Polytheism is inherently open-minded, and rarely persecutes ‘heretics’ and ‘infidels’.</p>
            <h4>Battle between good and evil for monotheism</h4>
            <p>Religion comes hand in hand with some fundamental concerns.</p>
            <p>Why is there evil in the world? Why is there suffering? Why do bad things happen to good people?</p>
            <p>Monotheists have to practice intellectual gymnastics to explain how an all-knowing, all-powerful and perfectly good God allows so much suffering in the world.</p>
            <p>Dualism (one good and one evil god) has its own drawbacks. While solving the Problem of Evil, it is unnerved by the Problem of Order. If the world was created by a single God, it’s clear why it is such an orderly place, where everything obeys the same laws. But if Good and Evil battle for control of the world, who enforces the laws governing this cosmic war?</p>
            <p>So, monotheism explains order, but is mystified by evil. Dualism explains evil, but is puzzled by order. There is one logical way of solving the riddle: to argue that there is a single omnipotent God who created the entire universe – and He’s evil. But nobody in history has had the stomach for such a belief.</p>
            <h4>Towards principles other than god</h4>
            <p>Gautama found that there was a way to exit this vicious circle of craving, disappointment and wanting more. If, when the mind experiences something pleasant or unpleasant, it simply understands things as they are, then there is no suffering. If you experience sadness without craving that the sadness go away, you continue to feel sadness but you do not suffer from it. There can actually be richness in the sadness. If you experience joy without craving that the joy linger and intensify, you continue to feel joy without losing your peace of mind.</p>
            <blockquote>
            <p>A person who does not crave cannot suffer.</p>
            </blockquote>
            <p>He encapsulated his teachings in a single law: suffering arises from craving; the only way to be fully liberated from suffering is to be fully liberated from craving; and the only way to be liberated from craving is to train the mind to experience reality as it is. This law, known as dharma or dhamma, is seen by Buddhists as a universal law of nature.</p>
            </details><p>  <br>
            The modern age has witnessed the rise of a number of new natural-law religions, such as liberalism, Communism, capitalism, nationalism and Nazism. These creeds do not like to be called religions, and refer to themselves as ideologies.</p>
            <p>Indeed, they are religions that worship humanity.</p>
            <h2>Scientific Revolution</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_6\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-5.jpg\">
            </figure>
            </div></figure><p>The typical premodern ruler gave money to priests, philosophers and poets in the hope that they would legitimise his rule and maintain the social order. He did not expect them to discover new medications, invent new weapons or stimulate economic growth.</p>
            <p>During the last five centuries, humans increasingly came to believe that they could increase their capabilities by investing in scientific research. This wasn’t just blind faith - it was proven empirically.</p>
            <p>This belief came in part because modern science differs from all previous traditions of knowledge in three critical ways:</p>
            <ol><li>
            <p><strong>The willingness to admit ignorance.</strong> Modern science is based on the Latin injunction ignoramus – ‘we do not know’. It assumes that we don’t know everything. Even more critically, it accepts that the things that we think we know could be proven wrong as we gain more knowledge. No concept, idea or theory is sacred and beyond challenge.</p>
            </li>
            <li>
            <p><strong>The centrality of observation and mathematics.</strong> Having admitted ignorance, modern science aims to obtain new knowledge. It does so by gathering observations and then using mathematical tools to connect these observations into comprehensive theories.</p>
            </li>
            <li>
            <p><strong>The acquisition of new powers.</strong> Modern science is not content with creating theories. It uses these theories in order to acquire new powers, and in particular to develop new technologies.</p>
            </li>
            </ol><p>It was inconceivable that the Bible, the Qur’an or the Vedas were missing out on a crucial secret of the universe – a secret that might yet be discovered by flesh-and-blood creatures.</p>
            <p>One of the things that has made it possible for modern social orders to hold together (after the move away from religions holding together social order) is the spread of an almost religious belief in technology and in the methods of scientific research, which have replaced to some extent the belief in absolute truths.</p>
            <p>Modern science has no dogma. Yet it has a common core of research methods, which are all based on collecting empirical observations – those we can observe with at least one of our senses – and putting them together with the help of mathematical tools.</p>
            <p>For men of science, death is not an inevitable destiny, but merely a technical problem. People die not because the gods decreed it, but due to various technical failures – a heart attack, cancer, an infection. And every technical problem has a technical solution.</p>
            <p>Further, most scientific studies are funded because somebody believes they can help attain some political, economic or religious goal.</p>
            <p>Thus, scientific research can flourish only in alliance with some religion or ideology. The ideology justifies the costs of the research. In exchange, the ideology influences the scientific agenda and determines what to do with the discoveries.</p>
            <p>Two forces in particular deserve our attention: imperialism and capitalism. The feedback loop between science, empire and capital has been history’s chief engine for the past 500 years.</p>
            <h3>Imperialism and science</h3>
            <p>The plant-seeking botanist and the colony-seeking naval officer shared a similar mindset. Both scientist and conqueror began by admitting ignorance – they both said, ‘I don’t know what’s out there.’ They both felt compelled to go out and make new discoveries. And they both hoped the new knowledge thus acquired would make them masters of the world.</p>
            <p>During the fifteenth and sixteenth centuries, Europeans began to draw world maps with lots of empty spaces – one indication of the development of the scientific mindset, as well as of the European imperial drive. The empty maps were a psychological and ideological breakthrough, a clear admission that Europeans were ignorant of large parts of the world.</p>
            <p>The discovery of America was the foundational event of the Scientific Revolution. It not only taught Europeans to favour present observations over past traditions, but the desire to conquer America also obliged Europeans to search for new knowledge at breakneck speed. If they really wanted to control the vast new territories, they had to gather enormous amounts of new data about the geography, climate, flora, fauna, languages, cultures and history of the new continent. Christian Scriptures, old geography books and ancient oral traditions were of little help.</p>
            <h3>Capitalism and science</h3>
            <p>Scientific research is usually funded by either governments or private businesses. When capitalist governments and businesses consider investing in a particular scientific project, the first questions are usually, ‘Will this project enable us to increase production and profits? Will it produce economic growth?’ A project that can’t clear these hurdles has little chance of finding a sponsor. No history of modern science can leave capitalism out of the picture.</p>
            <p>However, the feedback loop between capitalism and science runs much deeper. Capitalism wouldn’t be possible without science, either. It’s based on a belief in the future - the future will be better than the present, thanks to science.</p>
            <blockquote>
            <p>A society of wolves would be extremely foolish to believe that the supply of sheep would keep on growing indefinitely.</p>
            </blockquote>
            <p>There’s much more to say about Capitalism. So much more, that I’ve separated it into a post of its own. Read here: <a href=\"https://neilkakkar.com/capitalism.html\">A Short History of Capitalism</a></p>
            <h2>Industrial Revolution</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_7\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-6.jpg\">
            </figure>
            </div></figure><p>The modern economy grows thanks to our trust in the future and to the willingness of capitalists to reinvest their profits in production. Yet that does not suffice. Economic growth also requires energy and raw materials, and these are finite. When and if they run out, the entire system will collapse.</p>
            <p>For millennia prior to the Industrial Revolution, humans already knew how to make use of a large variety of energy sources. They burned wood in order to smelt iron, heat houses and bake cakes. Sailing ships harnessed wind power to move around, and watermills captured the flow of rivers to grind grain. Yet all these had clear limits and problems. Trees were not available everywhere, the wind didn’t always blow when you needed it, and water power was only useful if you lived near a river.</p>
            <p>An even bigger problem was that people didn’t know how to convert one type of energy into another.</p>
            <p>Since human and animal bodies were the only energy conversion device available, muscle power was the key to almost all human activities. Everyone was fuelled by solar energy – captured and packaged in wheat, rice and potatoes.</p>
            <p>The invention of the steam engine brought about the Industrial Revolution.</p>
            <p>You burn some kind of fuel, such as coal, and use the resulting heat to boil water, producing steam. As the steam expands it pushes a piston. The piston moves, and anything that is connected to the piston moves with it. You have converted heat into movement.</p>
            <p>In the decades that followed, British entrepreneurs improved the efficiency of the steam engine, brought it out of the mineshafts, and connected it to looms and gins. This revolutionised textile production, making it possible to produce ever-larger quantities of cheap textiles.</p>
            <p>People became obsessed with the idea that machines and engines could be used to convert one type of energy into another. Any type of energy, anywhere in the world, might be harnessed to whatever need we had, if we could just invent the right machine.</p>
            <p>Another crucial discovery was the internal combustion engine, which took little more than a generation to revolutionise human transportation and turn petroleum into liquid political power. Petroleum had been known for thousands of years, and was used to waterproof roofs and lubricate axles. Yet until just a century ago nobody thought it was useful for much more than that. The idea of spilling blood for the sake of oil would have seemed ludicrous. You might fight a war over land, gold, pepper or slaves, but not oil.</p>
            <p>At heart, the Industrial Revolution has been a revolution in energy conversion. It has demonstrated again and again that there is no limit to the amount of energy at our disposal. Or, that the only limit is set by our ignorance. Every few decades we discover a new energy source, so that the sum total of energy at our disposal just keeps growing.</p>
            <p>The result of the industrial revolution was an explosion in human productivity. The explosion was felt first and foremost in agriculture. Usually, when we think of the Industrial Revolution, we think of an urban landscape of smoking chimneys, or the plight of exploited coal miners sweating in the bowels of the earth. Yet the Industrial Revolution was above all else the Second Agricultural Revolution.</p>
            <p>Without the industrialisation of agriculture the urban Industrial Revolution could never have taken place – there would not have been enough hands and brains to staff factories and offices.</p>
            <h3>Aftereffects of Industry</h3>
            <p>The Industrial Revolution turned the timetable and the assembly line into a template for almost all human activities. Shortly after factories imposed their time frames on human behaviour, schools too adopted precise timetables, followed by hospitals, government offices and grocery stores. Even in places devoid of assembly lines and machines, the timetable became king. If the shift at the factory ends at 5 p.m., the local pub had better be open for business by 5:02.</p>
            <p>This modest beginning spawned a global network of timetables, synchronised down to the tiniest fractions of a second. When the broadcast media – first radio, then television – made their debut, they entered a world of timetables and became its main enforcers and evangelists.</p>
            <p>Along with adapting to industrial time, this revolution brought about urbanisation, the disappearance of the peasantry, the rise of the industrial proletariat<sup><a href=\"https://neilkakkar.com/sapiens.html#fn:1\">2</a></sup>, the empowerment of the common person, democratisation, youth culture and the disintegration of patriarchy.</p>
            <p>Most of the traditional functions of families and communities were handed over to states and markets.</p>
            <h4>Collapse of family</h4>
            <p>Village life involved many transactions but few payments. There were some markets, of course, but their roles were limited. You could buy rare spices, cloth and tools, and hire the services of lawyers and doctors. Yet less than 10 per cent of commonly used products and services were bought in the market. Most human needs were taken care of by the family and the community.</p>
            <p>Traditional agricultural economies had few surpluses with which to feed crowds of government officials, policemen, social workers, teachers and doctors. Consequently, most rulers did not develop mass welfare systems, health-care systems or educational systems.</p>
            <p>This was left for the family and communities.</p>
            <p>There wasn’t much a person renouncing his family could do. In order to survive, such a person had to find an alternative family or community. Boys and girls who ran away from home could expect, at best, to become servants in some new family. At worst, there was the army or the brothel.</p>
            <p>All this changed dramatically over the last two centuries. The Industrial Revolution gave the market immense new powers, provided the state with new means of communication and transportation, and placed at the government’s disposal an army of clerks, teachers, policemen and social workers. At first the market and the state discovered their path blocked by traditional families and communities who had little love for outside intervention. Parents and community elders were reluctant to let the younger generation be indoctrinated by nationalist education systems, conscripted into armies or turned into a rootless urban proletariat.</p>
            <p>The state and the market approached people with an offer that could not be refused. ‘Become individuals,’ they said. ‘Marry whomever you desire, without asking permission from your parents. Take up whatever job suits you, even if community elders frown. Live wherever you wish, even if you cannot make it every week to the family dinner. You are no longer dependent on your family or your community. We, the state and the market, will take care of you instead. We will provide food, shelter, education, health, welfare and employment. We will provide pensions, insurance and protection.’</p>
            <h4>Movement from war to peace</h4>
            <p>With industry and trade, countries moved from a plundering focused mindset to a protection focused mindset.</p>
            <p>Four factors led to peace.</p>
            <ol><li>Threat of nuclear holocaust.</li>
            <li>Declining profits of war with no new territories.</li>
            <li>Flourishing trade which increased cost of war.</li>
            <li>International connections between countries.</li>
            </ol><p>There is a positive feedback loop between all these four factors. The threat of nuclear holocaust fosters pacifism; when pacifism spreads, war recedes and trade flourishes; and trade increases both the profits of peace and the costs of war.</p>
            <p>Over time, this feedback loop creates another obstacle to war, which may ultimately prove the most important of all. The tightening web of international connections erodes the independence of most countries, lessening the chance that any one of them might single-handedly let slip the dogs of war. Most countries no longer engage in full-scale war for the simple reason that they are no longer independent.</p>
            <h2>The End of Homo Sapiens</h2>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_8\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fsapiens-timeline-7.jpg\">
            </figure>
            </div></figure><p>The beauty of Darwin’s theory of Natural Selection is that it does not need to assume an intelligent designer to explain how giraffes ended up with long necks, or why lean and fast chicken survive.</p>
            <p>Biologists the world over are locked in battle with the intelligent-design movement, which opposes the teaching of Darwinian evolution in schools and claims that biological complexity proves there must be a creator who thought out all biological details in advance. The biologists are right about the past, but the proponents of intelligent design might, ironically, be right about the future.</p>
            <p>The first light for intelligent design appeared about 10,000 years ago, during the Agricultural Revolution. Sapiens who dreamed of fat, slow-moving chickens discovered that if they mated the fattest hen with the slowest cock, some of their offspring would be both fat and slow. If you mated those offspring with each other, you could produce a line of fat, slow birds. It was a race of chickens unknown to nature, produced by the intelligent design not of a god but of a human.</p>
            <blockquote>
            <p>History teaches us that what seems to be just around the corner may never materialise due to unforeseen barriers, and that other unimagined scenarios will in fact come to pass.</p>
            </blockquote>
            <p>What we should take seriously is the idea that the next stage of history will include not only technological and organisational transformations, but also fundamental transformations in human consciousness and identity.</p>
            <p>We might stop being homo sapiens.</p>
            <figure><div class=\"RIL_IMG\" id=\"RIL_IMG_9\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fneilkakkar.com%2Fassets%2Fimages%2Fdivider.jpg\">
            </figure>
            </div></figure></div>"
    },
    {
        title: "The Secrets of the World's Greatest Art Thief",
        author: "Michael Finkel",
        url: 'gq.com',
        full_url: 'https://www.gq.com/story/secrets-of-the-worlds-greatest-art-thief', 
        reading_time: '39 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c59e511c15c09749bbf0c%2F16%3A9%2Fw_2560%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-01.jpg",
        content: 
            "<div lang=\"en\"><div class=\"RIL_IMG\" id=\"RIL_IMG_1\"></div><div>Stéphane Breitwieser robbed nearly 200 museums, amassed a collection of treasures worth more than $1.4 billion, and became perhaps the most prolific art thief in history. And as he reveals to GQ’s Michael Finkel, how Breitwieser managed to do all this is every bit as surprising as why.</div><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"© RMN / Rèunion des Musèes Nationaux / Sleeping Shepherd by François Boucher\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c59e511c15c09749bbf0c%2F16%3A9%2Fw_2560%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-01.jpg\">
            <figcaption>© RMN / Rèunion des Musèes Nationaux / Sleeping Shepherd by François Boucher</figcaption>
            </figure>
            </div><div><div><div><div><div><p><strong>“Don't worry about parking the car,”</strong> says the art thief. “Anywhere near the museum is fine.” When it comes to stealing from museums, Stéphane Breitwieser is virtually peerless. He is one of the most prolific and successful art thieves who have ever lived. Done right, his technique—daytime, no violence, performed like a magic trick, sometimes with guards in the room—never involves a dash to a getaway car. And done wrong, a parking spot is the least of his worries.</p><p>Just make sure to get there at lunchtime, Breitwieser stresses, when the visitors thin and the security staff rotates shorthanded to eat. Dress sharply, shoes to shirt, topped by a jacket that's tailored a little too roomy, with a Swiss Army knife stashed in a pocket.</p><p>Be friendly at the front desk. Buy your ticket, say hello. Once inside, Breitwieser adds, it's essential to focus. Note the flow of visitor traffic and memorize the exits. Count the guards. Are they sitting or patrolling? Check for security cameras and see if each has a wire—sometimes they're fake.</p><div></div><p>When it comes to museum flooring, creaky old wood is ideal, so even with his back turned, Breitwieser can hear footsteps two rooms away. Carpeting is the worst. Here, at the Rubens House, in Antwerp, Belgium, it's somewhere in between: marble. For this theft, Breitwieser has arrived with his girlfriend and frequent travel companion, Anne-Catherine Kleinklaus, who positions herself near the only doorway to a ground-floor exhibition room and coughs softly when anyone approaches.</p><div aria-hidden=\"true\" role=\"presentation\"><div></div><div></div></div><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"The Rubens House in Antwerp. The site of one of Breitwieser\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c66fff04fb6ea3ae06bc0d8%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-The-Rubens-House-gq.jpg\">
            <figcaption>The Rubens House in Antwerp. The site of one of Breitwieser's more memorable heists.
            Mark Renders/Getty Images</figcaption>
            </figure>
            </div></picture></span></div><p>The museum is the former home of Peter Paul Rubens, the great Flemish painter of the 1600s. Breitwieser isn't interested in stealing a Rubens; his paintings tend to be extremely large or too overtly religious for Breitwieser's taste. What sets Breitwieser apart from nearly every other art thief—it's the trait, he believes, that has facilitated his prowess—is that he will steal only pieces that stir him emotionally. And he insists that he never sells any. Stealing art for money, he says, is stupid. Money can be made with far less risk. But stealing for love, Breitwieser knows, is ecstatic.</p><div></div><p>And this piece, right in front of him, is a marvel. He had discovered it during a visit to the museum two weeks previous. He wasn't able to take it then, but its image blazed in his mind every time he sought sleep. This is why he's returned; this has happened before. There will be no good rest until the object is his.</p></div></div><div><div><p>It's an ivory sculpture of Adam and Eve, carved in 1627 by Georg Petel, a friend of Reubens's, who, according to Breitwieser, gifted him the piece for his 50th birthday. The carving is a masterpiece, just ten inches tall but dazzlingly detailed, the first humans gazing at each other as they move to embrace, Eve's hair scrolling down her back, the serpent coiled around the tree trunk behind them, and the unbitten apple, cheekily, in Adam's hand, indicating his complicity in the fall of man, contrary to the book of Genesis. “It's the most beautiful object I have ever seen,” says Breitwieser.</p><hr><div><figure><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"Georg Petel’s ivory sculpture of Adam and Eve, stolen from—and later returned to—the museum at the home of Peter Paul Rubens in Antwerp.
            Picasa\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c60357f3ce73fedc6c687%2Fmaster%2Fw_1600%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-04.jpg\">
            <figcaption>Georg Petel’s ivory sculpture of Adam and Eve, stolen from—and later returned to—the museum at the home of Peter Paul Rubens in Antwerp.
            Picasa</figcaption>
            </figure>
            </div></picture></span></div></figure></div><p><strong>The ivory sculpture</strong> is sealed beneath a plexiglass dome fastened to a thick base, resting on an antique dresser. Breitwieser's first objective is to remove the two screws that connect the dome and the base. There's no camera here, and only one guard is in motion, poking her head in every few minutes.</p><p>The tourists, as usual, are the problem—too many of them, lingering. The room is filled with items Rubens had amassed during his lifetime, including marble busts of Roman philosophers, a terra-cotta sculpture of Hercules, and a scattering of 17th-century oil paintings.</p><p>Patience is needed, but a moment soon comes when it's just Kleinklaus and Breitwieser alone, and in an instant he unfolds the screwdriver from the Swiss Army knife and sets upon the plexiglass dome. Breitwieser is shorter than average and tousle-haired, with piercing blue eyes that, for all his stealth, are often animate with expression. He is lithe and coordinated, and uses athleticism and theater in his work. Maybe five seconds pass before Kleinklaus coughs and he vaults away from the carving, reverting to casual-art-gazing mode.</p><p>It's a start. He has turned the first screw twice around. Each job is different; improvisation is crucial—rigid plans do not work during daytime thefts, when there are variables too numerous to preordain. During his previous trip to the museum, he had studied how the Adam and Eve was protected and had also spotted a convenient door, reserved for guards, that opened into the central courtyard and did not appear to have an alarm.</p><p>Over the course of ten minutes, progressing fitfully, Breitwieser removes the first screw and pockets it. He does not wear gloves, trading fingerprints for dexterity. The second screw takes equally as long.</p><p>Now he's set. The security guard has already appeared three times, and at each check-in Breitwieser and Kleinklaus had stationed themselves in different spots. Still, the time elapsed in this room has reached his acceptable limit. There's a group of visitors present, all using audio guides and studying a painting, and Breitwieser judges them appropriately distracted.</p><div></div><p>He nods to his girlfriend, who slips out of the room, then lifts the plexiglass dome and sets it carefully aside. He grasps the ivory and pushes it into the waistband of his pants, at the small of his back, adjusting his roomy jacket so the carving is covered. There's a bit of a lump, but you'd have to be exceptionally observant to notice.</p><p>Then he strides off, moving with calculation but no obvious haste. He knows that the theft will swiftly be spotted. He'd left the plexiglass bell to the side—no need to waste precious seconds replacing it—and the guard will surely initiate an emergency response. Though not, he's betting, quickly enough.</p><p>From the room with the ivory, the museum layout encourages visitors to ascend to the second floor, but Breitwieser pushes through the door he'd seen on his earlier trip, crosses the courtyard toward the main entrance, and walks past the front desk onto the streets of Antwerp. Kleinklaus rejoins him before they reach the car, a little Opel Tigra, and Breitwieser sets the ivory in the trunk and they drive slowly away, pausing at traffic lights on the route out of town.</p><hr><div><figure><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_6\">
            <figure>
            <img alt=\"Stéphane Breitwieser robbed nearly 200 museums to amass his secret art collection.
            CHRISTOPHE KARABA/EPA/REX/Shutterstock\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c5e6d1407441537b1aa55%2Fmaster%2Fw_1600%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-02.jpg\">
            <figcaption>Stéphane Breitwieser robbed nearly 200 museums to amass his secret art collection.
            CHRISTOPHE KARABA/EPA/REX/Shutterstock</figcaption>
            </figure>
            </div></picture></span></div></figure></div><p><strong>Crossing international</strong> borders is stressful but low-risk. They travel from Belgium to Luxembourg to Germany to their home in France without incident, just another young, stylish couple out for a jaunt. It's the first weekend of February 1997, and both are only 25 years old, though Breitwieser's already been stealing art for a while.</p></div></div><div><div><p>The road trip ends at a modest steep-roofed house built amid the sprawl of Mulhouse, an industrial city in eastern France. The ivory might be worth a million dollars, but Breitwieser is broke. He does not have a steady job—when he is employed, it's often as a waiter. His girlfriend works in a hospital as a nurse's aide, and the couple live in his mother's house. Their private space is on the top floor, an attic bedroom and small living area that Breitwieser always keeps locked.</p><p>They open the door now, cradling the ivory, and a wave of swirling colors seems to break over their heads as they step inside their fantasy world. The walls are lined with Renaissance paintings—portraits, landscapes, still lifes, allegories. There's a bustling peasant scene by Dutch master Adriaen van Ostade, an idyllic pastoral by French luminary François Boucher, an open-winged bat by German genius Albrecht Dürer. A resplendent 16th-century wedding portrait, the bride's dress threaded with pearls, by Lucas Cranach the Younger, may be worth more than all the houses on Breitwieser's block put together, times two.</p><p>In the center of the bedroom sits a grandiose canopied four-poster bed, draped with gold velour and red satin, surrounded by furniture stacked with riches. Silver goblets, silver platters, silver vases, silver bowls. A gold snuffbox once owned by Napoleon. A prayer book, lavishly illuminated, from the 1400s. Ornate battle weapons and rare musical instruments. Bronze miniatures and gilded teacups. Masterworks in enamel and marble and copper and brass. The hideaway shimmers with stolen treasure. “My Ali Baba's cave,” Breitwieser calls it.</p><p>Entering this place, every time, dizzies him with joy. He describes it as a sort of aesthetic rapture. Breitwieser sprawls on the bed, examining his new showpiece. The Adam and Eve ivory, after a four-century journey to arrive in his lair, appears more stunning than ever. It goes on the corner table, the first thing he sees when he opens his eyes.</p><div></div><p>During the week, while his girlfriend is working, he visits his local libraries. He learns everything he can about the ivory, the artist, his masters, his students. He takes detailed notes. He does this with nearly all his pieces—he gets attached to them. Back home, he meticulously cleans the carving, with soapy water and lemon, his thumb passing over the sculpture's every nubbin and ridge.</p><p>But this is not enough. His love for the ivory doesn't fade, that's not fair to say—he just has room in his heart for a little more love. So he consults his art magazines and auction catalogs. The Zurich art fair is about to begin. He plots a route into Switzerland, avoiding tolls to save money, and early the next Saturday morning they're back on the road.</p><hr><div><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_7\">
            <figure>
            <img alt=\"Sibylle of Cleves by Lucas Cranach the Younger, thought to be worth roughly $4.8 million, was perhaps the most valuable piece in Breitwieser’s collection.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c64db68709f45de84a84b%2Fmaster%2Fw_1600%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-05.jpg\">
            <figcaption>Sibylle of Cleves by Lucas Cranach the Younger, thought to be worth roughly $4.8 million, was perhaps the most valuable piece in Breitwieser’s collection.</figcaption>
            </figure>
            </div></picture></span></div><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_8\">
            <figure>
            <img alt=\"A still life of flowers by Jan van Kessel the Elder that was stolen from a village museum in Belgium, a country Breitwieser says attracted him &quot;like a lover.&quot;\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c64dd7f3ce73fedc6c689%2Fmaster%2Fw_1600%252Cc_limit%2FWorld_s-Greatest-Art-Thief-GQ030119-ArtThief-06.jpg\">
            <figcaption>A still life of flowers by Jan van Kessel the Elder that was stolen from a village museum in Belgium, a country Breitwieser says attracted him \"like a lover.\"</figcaption>
            </figure>
            </div></picture></span></div></div><p><strong>All his life,</strong> inanimate objects have had the power to seduce him. “I get smitten,” Breitwieser says. Before artwork, it was stamps and coins and old postcards, which he'd purchased with pocket money. Later it was medieval pottery fragments he'd find near archaeological sites, free for the taking.</p><p>When he covets an object, says Breitwieser, he feels the emotional wallop of a <em>coup de coeur</em>—literally, a blow to the heart. There are just things that make him swoon. “Looking at something beautiful,” he explains, “I can't help but weep. There are people who do not understand this, but I can cry for objects.”</p><p>His interactions with the world of the living were far less fulfilling. He never really understood his peers, or almost anyone else for that matter. Popular pastimes, like sports and video games, baffled him. He's never had any interest in drinking or drugs. He could happily spend all day alone at a museum—his parents often dropped him off—or touring archaeological sites, of which there are dozens in the area where he grew up, but around others he was sometimes hotheaded and temperamental.</p></div></div><div><div><p>Breitwieser was born in 1971 in the Alsace region of northeastern France, where his family has deep roots. He speaks French and German and a little English. His father was a sales executive in Switzerland, just over the border, and his mother was a nurse. He's an only child. The family, for most of his youth, was well-off, living in a grand house filled with elegant furniture—Louis XV armchairs, from the 1700s; Empire dressers, from the 1800s. His parents had hoped he'd become a lawyer, but he dropped out of university after a couple of years.</p><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_9\">
            <figure>
            <img alt=\"The Alsace region of France, where Breitwieser grew up, sits in the northeastern corner of the country,  along the borders of Germany and Switzerland.
            Christophe Dumoulin/Getty Images\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c670101a979c74995cdeb89%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-alsace-france-gq.jpg\">
            <figcaption>The Alsace region of France, where Breitwieser grew up, sits in the northeastern corner of the country,  along the borders of Germany and Switzerland.
            Christophe Dumoulin/Getty Images</figcaption>
            </figure>
            </div></picture></span></div><p>His first museum heist came shortly after a family crisis. When he was 22 years old, still living at home, his parents' marriage ended explosively. His father left and took his possessions with him, and Breitwieser and his mother tumbled down the social ladder, re-settling in a smaller place, the antiques replaced by Ikea.</p><p>Cushioning the trauma was a woman Breitwieser met through an acquaintance, a fellow archeology buff. Anne-Catherine Kleinklaus was the same age as Breitwieser, and similarly introverted, with a kindred sense of curiosity and adventure. She had a sly smile and an irresistible pixie cut. They shared a passion for museums, thrilled to be immersed in beauty. Breitwieser finally experienced a <em>coup de coeur</em> for an actual person. “I loved her right away,” he says. Soon after Breitwieser's father departed, Kleinklaus moved in.</p><div></div><p>A few months later, the couple were visiting a museum in the French village of Thann when Breitwieser spotted an antique pistol. His first thought, he recalls, was that he should already own something like this. Breitwieser's father had collected old weapons but had taken them when he'd left the family, not bothering to leave a single piece for his son. The firearm, exhibited in a glass case on the museum's second floor, was hand-carved around 1730. It was far nicer than anything his father had owned.</p><p>He felt an urge to possess it. The museum was small, no security guard or alarm system, just a volunteer at the entrance booth. The display case itself, Breitwieser noted, was partially open. He was wearing a backpack and could easily hide the pistol in there.</p><p>One must resist temptation, he knew. It even says so in the Bible, not that he was particularly religious. What our heart really wants, we must often deny. Maybe this is why so many people seem conflicted and miserable—we are taught to be at constant war with ourselves. As if that were a virtue.</p></div></div><div><div><p>What would happen, he wondered, if he did not resist temptation? If, instead, he fed temptation and freed himself from society's repressive restraints? He had no desire to physically harm anyone or so much as cause fright. He contemplated the flintlock pistol and whispered a few of these thoughts to his girlfriend.</p><p>Anne-Catherine Kleinklaus has never spoken to the media about her relationship with Breitwieser and any possible role in the crimes, and neither has Breitwieser's mother, Mireille Stengel. Though there exist supporting documents and reported accounts, much of this story is based primarily on interviews with Breitwieser. While he was in the museum, in front of the pistol, Kleinklaus's response, the way Breitwieser remembers it, made him believe that they were destined to be together.</p><p>“Go ahead,” she said. “Take it.” So he did.</p><hr><p><strong>From that moment on,</strong> he catered to his impulses in an unimaginable way. His only goal was to obey temptation. By the time he pilfers the Adam and Eve ivory, three years after stealing the pistol, he's amassed some 100 objects, all on display in his hideout. He is ecstatic beyond measure, cosseted like a king. He feels as though he and his girlfriend have discovered the meaning of life.</p><p>A curious thing about temptation, at least in Breitwieser's case, is that it never seems to abate. If anything, the more he feeds it, the hungrier it gets. The weekend after the ivory theft in Belgium, Breitwieser and Kleinklaus drive through the snow-streaked Alps to the Zurich art fair. Behind a dealer's back, quick as a cat, he steals a spectacular goblet, filigreed with silver and gold, from the 16th century.</p><div></div><p>Then they head to Holland for another fair, and at one booth, while the vendor is eating lunch and not keeping careful watch, Breitwieser takes a brilliant rendering of a lake bobbing with swans, dated 1620. At another booth, again with the dealer present, he removes a 17th-century seascape painted on copper.</p><p>A few weeks later, it's back to Belgium, to a village museum with a single security guard, where he takes a valuable still life, butterflies flitting around a bouquet of tulips, by Flemish master Jan van Kessel the Elder. This is followed by a trip to a Paris auction, where, at the pre-sale show, he steals a painting from the school of Pieter Brueghel the Elder and Pieter Brueghel the Younger, two polestars of Renaissance art.</p><p>Once again he returns to Belgium—a country whose museums, says Breitwieser, “attract me like a lover”—and filches a vivid tableau of a rural market, then over to Holland to snatch a droll 17th-century watercolor of house cats chasing hedgehogs, followed by a journey to the northern French city of Lille for another Renaissance oil work, and finally, for good measure, one more raid in Belgium.</p><p>All of this in a matter of months. These paintings alone represent a haul worth millions of dollars. And it's not just paintings—he also steals a gold-plated hourglass, a stained-glass windowpane, an iron alms box, a copper collection plate, a brass hunting bugle, a cavalry saber, a couple of daggers, a gilded ostrich egg, a wooden altarpiece, and a half-dozen pocket watches. Everything is crammed into the hideout, filling the walls top to bottom, overflowing the end tables, displayed in his closet's shoe rack, leaning on chairs, stuffed under the bed.</p><p>The collection is not random. Virtually everything he steals was made before the Industrial Revolution, in an age when items were all still formed by hand; no machines stamped out parts. Everything finely crafted in this way, Breitwieser believes, from medical instruments to kitchenware, is its own little work of art, the hand of the master visible in each chisel mark and burr. This, to Breitwieser, was the height of human civilization.</p><p>Today the world is wed to mass production and efficiency, much to our benefit. But a side effect is that beauty for beauty's sake seems increasingly quaint, and museums themselves, small ones especially, can have the whiff of the dying. Stocking pieces in his room, Breitwieser feels, is rescuing them, like pets from a shelter, giving them the love and attention they deserve.</p></div></div><div><div><hr><p><strong>The more he steals,</strong> the better he gets. He learns, with precision, the limits of a security camera's vision. He hones his timing and perfects his composure. “You have to control your gestures, your words, your reflexes,” Breitwieser says. “You need a predatory instinct.” He pounces the instant he senses everyone's attention is diverted. “The pleasure of having,” says Breitwieser, “is stronger than the fear of stealing.”</p><p>He tries to take only smaller pieces—with paintings, no more than about a foot by a foot—and if time allows, he prefers to remove the frame and hide it nearby, often in a bathroom, so the artwork disappears more completely beneath his jacket. He purchases new frames for most of the works. Sometimes he steals weapons, but he wouldn't think of brandishing one. To walk into a museum with a gun, he says, is disgusting.</p><div></div><p>The set of thefts he describes as the most exquisite of his career are a study in simplicity and sangfroid. They take place in Belgium, his beloved target, at the vast Art &amp; History Museum in Brussels, which Breitwieser estimates employs 150 guards. There he and Kleinklaus spot a partly empty display case, with a laminated card inside that reads \"Objects removed for study.\" Nothing in the case interests them, but Breitwieser has an idea and steals the card.</p><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_10\">
            <figure>
            <img alt=\"The Historical Museum of Mulhouse.
            Rieger Bertrand/Getty Images\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c6f242154c83560c6536794%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-mulhouse-museum-gq.jpg\">
            <figcaption>The Historical Museum of Mulhouse.
            Rieger Bertrand/Getty Images</figcaption>
            </figure>
            </div></picture></span></div><p>Breitwieser understands how security guards think. At age 19, he was employed for a month as a guard at the Historical Museum of Mulhouse, near his home. Most guards, he realized, hardly notice the art on the walls—they look only at people. Breitwieser's brashest thefts, like the Adam and Eve ivory, are spotted in minutes, but when he's furtive, hours often pass, and sometimes days, before anyone realizes what's happened.</p><p>In the Brussels Art &amp; History Museum, he carries the \"Objects removed\" sign to a gallery with a display case of silver pieces from the 16th century. To break into this case, Breitwieser uses a screwdriver and levers the sliding door off its tracks. Other times, he carries a box cutter and slices open a silicone joint. For museums with antique display cabinets, he brings a ring of a dozen old skeleton keys he's amassed—often one of his keys is able to tumble the lock. Also handy is a telescoping antenna, to nudge a ceiling-mounted security camera in a different direction.</p><p>He selects three silver items, a drinking stein and two figurines; then he sets the \"Objects removed\" card in the case and re-attaches the sliding door, and they leave the museum. They're already at the car before he realizes he's forgotten the lid to the stein.</p></div></div><div><div><p>Breitwieser detests missing parts or any sign of restoration. The items in his collection must be original and complete. Kleinklaus knows this, says Breitwieser, and she abruptly removes one of her earrings and heads back to the museum, her boyfriend in tow. She marches up to a security guard and says she's lost an earring and has a feeling she knows where it is. The couple are permitted back inside. They return to the case and he takes the stein's lid and, why not, two additional goblets from another case.</p><p>Two weeks later, they're back. Kleinklaus has changed her hairstyle, and Breitwieser has grown out his beard and added a pair of glasses and a baseball cap. At the display case, the \"Objects removed\" card still there, he grabs four more items, including a two-foot-tall chalice so breathtakingly gorgeous that Breitwieser suspends his size-limitation preference and, with nowhere else to put it, stuffs the item up the left sleeve of his jacket, forcing him to walk unnaturally, his arm swinging stiffly like a soldier's.</p><p>On their way to the exit, they're stopped by a guard. They feign calm, but Breitwieser has a terrible feeling that the end has come. The guard wants to see their entrance tickets. Breitwieser, unable to move his left arm, awkwardly reaches across his body with his right to fish the tickets from his left pocket. He wonders if the guard senses something amiss.</p><p>A guilty person would cower and try to leave, so Breitwieser boldly tells the guard that he's heading to the museum café for lunch. The guard's suspicion is defused, and the couple actually eat at the museum, Breitwieser's arm held rigid the entire time.</p><div></div><p>They rent a cheap hotel room and wait two days and return yet again, newly disguised, and he steals four more pieces. That's a total of 13, and such is their level of euphoria that on the drive home they can't contain themselves and stop at an antiques gallery displaying an immense ancient urn, made of silver and gold, in the front window.</p><p>Breitwieser enters, and the dealer calls from atop a staircase that he'll be right down, but by the time he descends no one is there. Nor is the urn. They return to France plunder-drunk and giddy, and for fun, Breitwieser recalls, Kleinklaus phones the gallery and asks how much the urn in the window costs. About $100,000, she's told. “Madame,” says the dealer, “you really must see it.” He hasn't yet noticed it's gone.</p><hr><p><strong>Of course the police are after them.</strong> Investigations are opened after many of their thefts—witnesses questioned, sketches made. Yet no one's ever quite sure what they saw. Breitwieser is videoed in action in a museum in France, but the images are grainy. The best the French authorities are able to deduce is that several times a year, in seemingly random places, a man and a woman steal art together; they envision the criminals as a retired couple, nowhere close to their actual age.</p><p>The couple themselves keep tabs on their peril by reading newspaper coverage of their crimes. Some articles mention that law enforcement is sure that a large network of international traffickers are systematically stealing. The authorities, much to Breitwieser's satisfaction, seem to have no clue as to whom they are chasing—the sheer scale of the thefts is so far beyond that of nearly every other case as to be practically inconceivable.</p><p>In the annals of art crime, it's hard to find someone who has stolen from ten different places. By the time the calendar flips to 2000, by Breitwieser's calculations, he's nearing 200 separate thefts and 300 stolen objects. For six years, he's averaged one theft every two weeks. One year, he is responsible for half of all paintings stolen from French museums.</p><p>By some combination of skill and luck, Breitwieser and Kleinklaus are doing everything right to avoid capture. They constantly shift the countries they target, alternating between rural and urban locations, large museums and small, while further mixing things up by stealing from churches, auction houses, and art fairs. They don't kick down doors or cover their faces with masks—actions that would trigger a much greater police response. Crime works best, Breitwieser believes, when no one realizes it's being committed.</p></div></div><div><div><p>Several times, he steals while they're on a guided tour, then casually continues the tour while holding the item. At an art fair in Holland, Breitwieser hears a shout of “Thief!” and sees security guards tackle a man. It's another burglar. Breitwieser takes advantage of the commotion and slips a painting under his coat.</p><hr><p><strong>There are, inevitably, several close calls.</strong> Once, Breitwieser accidentally shatters a glass display case. Another time, he returns to his car while holding sections of a 16th-century wooden altarpiece only to encounter a police officer in the process of giving him a parking ticket. While hiding the artwork beneath his jacket, he manages to persuade the officer to withdraw the ticket. Soon after a theft in France, roadblocks are set up on some of the routes leading from the museum, but Breitwieser and Kleinklaus manage to avoid being stopped.</p><div></div><p>Then they visit an art gallery in Lucerne, Switzerland. It's a hot day, and Breitwieser is not wearing a jacket that he can use to hide a stolen object—and even worse, they are the gallery's only visitors. The place is also directly across the street from a police station. Kleinklaus, according to Breitwieser, issues a warning. “Don't do anything,” she says. “I don't feel it, I'm telling you.”</p><p>But Breitwieser has spotted a 17th-century still life by Dutch painter Willem van Aelst that is simply too tempting. And it seems so easy to take. He puts the painting under his arm and walks out as casually as if he's carrying a baguette. A gallery employee instantly spots the theft, accosts the couple outside the gallery, and escorts them across the street to the police. Breitwieser and Kleinklaus remain in custody overnight but manage to convince the authorities that this is the first time they'd ever stolen and that they are terribly, deeply sorry. They are released with hardly any punishment.</p><p>Rattled, the couple make a vow never to steal in Switzerland again and decide to take a break from thieving entirely. The respite lasts all of three weeks before Breitwieser, at an auction in Paris, steals a scene of a grape harvest by Flemish painter David Vinckboons. After that, he returns to stealing as frequently as before.</p><p>An art thief Breitwieser admires, he says, is Thomas Crown, from the two <em>Thomas Crown Affair</em> movies. But that's fiction. Breitwieser is furious at nearly all actual art thieves, especially people like those who broke into Boston's Isabella Stewart Gardner Museum in 1990. The two thieves took 13 works worth a total of $500 million, but they used knives to slice some of the paintings from their frames. Breitwieser would never consider cutting out a painting—that, he says, is vandalism. He wouldn't even roll up a canvas, an action that risks cracking the paint. “You roll up an old painting,” he says, “and you kill it.”</p><p>About 50,000 artworks are stolen each year around the world, and according to the director of the London-based Art Loss Register, the most comprehensive database of stolen art, more than 99 percent of art thieves are motivated by profit rather than aesthetics. This is why art crimes are typically solved on the back end, when the thieves try to sell the work. But with Breitwieser, law enforcement's chief strategy—poring over art-market data, waiting for the stolen items to reappear—is dead on arrival.</p><p>Still, a multi-million-dollar collection of stolen art concealed in an attic bedroom in a middle-class suburb seems too extraordinary to remain secret forever. If just one friend found out, it's inevitable others would learn and the game would be finished.</p><p>Breitwieser and Kleinklaus, though, have no friends. “I've always been a loner,” he says. “I don't want any friends.” Kleinklaus, he claims, feels the same. They occasionally spend time with acquaintances but never invite anyone over. If repairs are needed in his room, he does them himself. Nobody is allowed to enter, ever, except him and his girlfriend. “We lived in a closed universe,” Breitwieser says.</p></div></div><div><div><hr><p><strong>They're both nearing 30 years old</strong> when their universe starts to crumble. A notion had been building in Kleinklaus ever since the night they spent in police custody in Switzerland—that perhaps there's something more fulfilling than life as an outlaw and rooms filled with riches. She'd like to start a family. But not, she realizes, with the man she's been dating for almost a decade. There is no option for a child in their conscribed existence. They could be arrested at any minute; they can't even entertain visitors. She begins to feel suffocated.</p><div></div><p>Breitwieser, meanwhile, says he feels “invincible.” Tension between the two intensifies, ugly fights erupt, and Breitwieser starts stealing alone. Any restraining influences Kleinklaus once provided are shed. From a village church not far from their house, he unbolts an enormous wooden carving of the Madonna and Child, weighing 150 pounds, and hauls it away, one strained step at a time, without the slightest attempt at stealth. If anyone had entered the church during the theft, he'd have been caught.</p><p>Later, in February 2001, at a hilltop castle, he removes a monumental 17th-century tapestry, larger than ten feet by ten feet, assuming ridiculous risk to steal it. There's no room in their lair for a trophy this size—it's left rolled up on a dresser—but Breitwieser tells his girlfriend they'll display it as soon as they are free of his mother and residing in a place of their own. By this point, Kleinklaus knows it's a fantasy. Living amid a mountain of stolen art, no matter where, can never offer true freedom at all.</p><p>After the police had taken their fingerprints in Switzerland, Breitwieser says, Kleinklaus fears that the prints are now filed in every nation's database. Even if she leaves him, she'll be hunted forever. What will they ever do with all this stuff? What's the endgame? She wants him to quit, but he doesn't even agree to abate. The best deal she can wrangle is a sworn promise that from now on, when stealing, he'll always wear surgical gloves, which she'll bring home from her job at the hospital. There is no endgame, Breitwieser says. He plans to keep going and going.</p><hr><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_11\">
            <figure>
            <img alt=\"Albrecht Dürer\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c5c631f68709f45de84a849%2Fmaster%2Fw_1600%252Cc_limit%2FWorld_s%25252520Greatest%25252520Art%25252520Thief-GQ030119-ArtThief-03.jpg\">
            <figcaption>Albrecht Dürer's gouache of a bat, which dates to 1522, was a prominent component of Breitwieser's illicit collection.
            Copyright: www.bridgemanimages.com</figcaption>
            </figure>
            </div></picture></span></div><p><strong>He returns from another thieving trip</strong> with a little curled bugle, dated from the 1580s, once used by hunters on horseback to communicate. It was a stylish theft, Breitwieser balancing atop a radiator to cut open a display case high on the wall, then delicately snipping the nylon cords holding the bugle in place. Kleinklaus is unimpressed. They already have one like it.</p></div></div><div><div><p>“Did you wear gloves?” she asks, suspicious.</p><p>“I'm really sorry,” he says.</p><p>The one thing she'd been promised. Then she learns that he'd stolen the bugle in Switzerland, the one country where they'd vowed never to steal from again. He had even gone to a museum near Lucerne—the same city in which they'd been caught. They argue bitterly, and in the morning Breitwieser says he'll go back to Switzerland and erase the prints.</p><p>Breitwieser says that this idea doesn't work for Kleinklaus; she wants to go to the museum and clean the prints herself. It's too risky for him. Breitwieser says that at least he should drive, and she consents.</p><div></div><p>They're frosty to each other on the trip, but as they pull into the Richard Wagner Museum, housed in a country manor where the composer once lived, their spirits are buoyed. The one thing that can stir Breitwieser as much a magnificent artwork is a sublime sweep of nature, and this museum is on a lake cupped in the spiked mountains of Switzerland. He feels for a moment, as Kleinklaus opens her door, a handkerchief and a bottle of rubbing alcohol in her bag, that maybe they can again find their love.</p><p>“Stay in the car,” she pleads.</p><p>“I'm just going to take a little walk,” he says. “Don't worry.” And he, too, gets out, handing her the car keys to hold in her purse.</p><p>She enters the museum, pays the entry fee, and walks up to the second floor. Breitwieser, circling around the outside of the building, watches her progress as she appears in one window, then another. There's only one other person around, an older man walking a dog, who seems to stare curiously at Breitwieser before moving away.</p><p>A few minutes later, Kleinklaus exits the museum. She walks quickly toward him, nearly jogging, which is odd. They never wanted to appear as if they were fleeing. He has the impression that she's attempting to tell him something, but she is too far away to hear. He tries to decipher the anxious expression on her face as the police car pulls to a stop behind him. Two officers approach, handcuff Breitwieser, who is startled but doesn't resist, and place him in the back seat of the squad car and drive off.</p><hr><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_12\">
            <figure>
            <img alt=\"The Richard Wagner Museum in Lucerne, Switzerland, where Breitwieser was taken into custody.
            Martin Siepmann/Getty Images\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c6f259f6b78e35f4434cc2a%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-Richard-Wagner-Museum-gq.jpg\">
            <figcaption>The Richard Wagner Museum in Lucerne, Switzerland, where Breitwieser was taken into custody.
            Martin Siepmann/Getty Images</figcaption>
            </figure>
            </div></picture></span></div><p><strong>He spends that night,</strong> November 20, 2001, in jail, and the next morning the interrogation begins. At the start of the questioning, says Breitwieser, he denied everything. After all, he didn't have any stolen items on him when he was arrested. But both the cashier at the museum and the dog walker who'd been on the grounds, says Breitwieser, have provided formal statements to the police.</p></div></div><div><div><p>The dog walker, a retired journalist, had read in that morning's paper about the Richard Wagner Museum theft, and when he saw a man there acting oddly, he went inside and mentioned it to the cashier. She looked out the window. The day the bugle was stolen, a total of three visitors had come to the museum, and this, she was certain, was one of them. He was wearing the same jacket. So she called the police. No one realized that Kleinklaus, who had overheard the conversation and was trying to warn him, had traveled with Breitwieser, and she was able to drive off in her car unnoticed.</p><p>Breitwieser realizes that to wriggle free from this jam, he needs to ensure that the authorities do not find out who he really is or send anyone to search his home. He tells the police that he'd come to Switzerland by train, alone, and admits to stealing the bugle. He explains, sorrowfully, that he is short of money and just wanted a nice Christmas gift for his mother. He has no idea, he adds, that the bugle is valuable; he was only attracted to it because of how shiny it was.</p><p>In the course of his conversation with the officers, he learns that the police never even considered dusting for prints.</p><div></div><p>Days drip by, then weeks, as he waits alone in his cell, worry mounting. He's not permitted to make phone calls, and he has the impression, he says, that the entire world has abandoned him. No one will give him any news.</p><p>What's happened is that the police have uncovered the report of Breitwieser's previous brush with the law in Switzerland. This was very intriguing. They'd at first assumed that Breitwieser was nothing more than a small-time thief who'd hoped to make an easy profit from a lightly guarded museum. Could he be something more?</p><p>Swiss authorities pursue an international search warrant for Breitwieser's residence in France. It takes a while to complete the warrant, but four weeks after his arrest, it's ready. A group of French and Swiss officers arrive at the house, hoping to find the bugle, and perhaps more. Breitwieser's mother is there and says she has no idea what they're talking about.</p><p>The officers enter the house, climb the stairs to the hidden lair, and open the door. And there, inside, they see no hunting bugle, no silver objects, no Renaissance paintings, no musical instruments. Not so much as the trace of a picture hook. Nothing but clean, empty walls surrounding a lovely four-poster bed.</p><hr><p><strong>Breitwieser remains in jail, knowing nothing.</strong> No one visits or writes. Christmas comes and goes without even a holiday card. He feels sick; he cries frequently. He has admitted to only the theft of the bugle, but he knows that he's close to breaking.</p><p>Soon after New Year's Day 2002, he is escorted from his cell and seated in an interrogation room, across the desk from a Swiss police lieutenant named Roland Meier. The officer opens a drawer, removes a single photo, and places it in front of Breitwieser. It's of a large commemorative medal that he had stolen from a different Swiss museum, a week before he'd taken the bugle. Breitwieser had imagined it could serve as a good-luck charm. The medal appears a little rusty and worn, and Breitwieser wonders what happened to it.</p><p>“We know you also stole this,” says Lieutenant Meier. “Tell us, and after that everything will be okay. We'll let you go home.”</p><p>Breitwieser swiftly confesses.</p><div></div><p>Just one more thing, says Lieutenant Meier, opening the drawer again and placing another photo before Breitwieser. This one is of a golden snuffbox, also slightly oxidized.</p><p>Breitwieser confesses to taking it as well.</p><p>And then, according to Breitwieser's version of these events, the officer pulls out a huge stack of photos, and Breitwieser realizes it's checkmate. There are pictures of an ivory flute from Denmark, an enameled goblet from Germany, silver pieces from Belgium, and even the very first item he stole, nearly eight years before—the flint-lock pistol from France.</p></div></div><div><div><p>He confesses to every one of them, providing details and dates. When the stack of photos is exhausted, he's admitted to stealing 140 objects. The lieutenant is staggered—he'd doubted this kid had stolen a single one of the items, let alone all of them.</p><p>Only now does Breitwieser see the police report that accompanied the photos. At the top it says “Objects found in the Rhone-Rhine Canal.” He's confused. The canal, part of the system built under Napoleon to connect the rivers of France, is a murky, slow-moving waterway not far from his home.</p><p>Then he realizes why the pieces seemed discolored—they must have been rescued from water. One more thing dawns on him as well. There were no photos of any paintings he stole. “What about the paintings?” he asks the lieutenant. And it's only then that he starts to find out.</p><hr><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_13\">
            <figure>
            <img alt=\"In a partially-drained section of the Rhone-Rhine Canal, crews search for stolen artwork that had been tossed into the murky water.
            Cedric Joubert/AP\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c66fdef86fbe52d8e189c3c%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-gq-Ste%252525CC%25252581phane-Breitwieser-search-and-rescue.jpg\">
            <figcaption>In a partially-drained section of the Rhone-Rhine Canal, crews search for stolen artwork that had been tossed into the murky water.
            Cedric Joubert/AP</figcaption>
            </figure>
            </div></picture></span></div><p><strong>What happened exactly</strong> remains a mystery. And because Breitwieser's mother and girlfriend have never talked to the media, the details may never be fully revealed. Breitwieser himself, though, has learned as much as he can, and combining his insights with police investigations and interviews, it's possible for him to piece together the events as he believes they may have occurred. Some specifics are lacking, and the precise time line is hazy, but not the result. The end, Breitwieser says, is always the same.</p><p>He envisions his girlfriend driving back from Switzerland, alone in the car, terrified. She's just witnessed his arrest and has not been caught herself. At least not yet. When she gets home, Breitwieser suspects, she tells his mother at least some part of the truth about the extent of the crimes. The fact that Breitwieser is in custody means the authorities will surely soon arrive and probably arrest both of them as well.</p><p>It's now, Breitwieser presumes, that his girlfriend takes his mother upstairs to their hideout. When Breitwieser visualizes his treasures through his mother's eyes, they look different. She's not spellbound by color or entranced by beauty. His mother works full-time to house and feed her 30-year-old unemployed son and his girlfriend, and he's repaid her by breaking the law in a way that will likely ruin her life.</p><div></div><p>To her, his treasure is poison. She's always had a temper, and his mother's reaction, he's sure, is a boiling rage. Once she decides something, there's no bending her will. “She's like a wall,” Breitwieser says. And she makes a decision now, one of finality and force.</p><p>It likely began that evening. First, Breitwieser thinks, his mother and possibly his girlfriend clear off the furniture, empty the closet, and collect everything under the bed. It's all piled in bags and boxes, then carried downstairs and crammed into his mother's car until the vehicle is completely full.</p><p>It must be very late, Breitwieser believes, when they drive to the canal. They go to a spot where the waterway runs plumb straight through a quiet, rural area, bordered on both sides by sheltering trees, the trail alongside it often busy by day with cyclists and joggers. The two women, Breitwieser thinks, then toss piece after piece into the dark water. Even in these panicked, angry actions, Breitwieser sees a filament of love—his mother, in some way, is trying to protect him, to hide what he's done.</p></div></div><div><div><p>Some pieces aren't thrown far enough from shore, and a few days later a passerby notices an intriguing shimmer in the water. He returns with a rake and finds a gold-plated chalice. Then he rakes out three more pieces of silver and a jewel-handled dagger. He tells the police, and they eventually drain a section of the canal and discover a collection of objects likely worth millions.</p><p>Back at Breitwieser's house, probably the same night as the canal dump, his mother and perhaps his girlfriend again load the car, possibly this time with the bigger items, including the heavy Madonna and Child, the tapestry, and three paintings on copper panels. The Madonna and Child is deposited in front of a local church—his mother is observant—while the tapestry is discarded aside a road and the coppers are tossed into a wooded area.</p><p>All these items are eventually recovered. A passing motorist spots the tapestry and turns it in to the local police, who are not aware of its significance and unfurl it on the floor of their break room and play billiards on it for a while. The three 17th-century coppers are found by a logger, who brings them home and hammers them onto the roof of his henhouse, which had been leaking. They remain there until Breitwieser's story hits the news.</p><p>The paintings, Breitwieser believes, were the final step. His Renaissance paintings formed the heart of his collection and represented the majority of its value. Breitwieser is sure that as the pictures are pulled from the walls, Kleinklaus is in shock—all he'd wanted to do was protect them from an uncaring planet—but his mother, he knows, is unstoppable. Later his mother will purchase putty and wall paint to cover the holes, and she will also throw away everything else in the rooms, including his clothing and books. But for now his mother drives all the paintings to a secluded area.</p><p>She creates a big pile, Breitwieser imagines, the portraits and still lifes and landscapes all jumbled, the luminaries of Renaissance art—Cranach, Brueghel, Teniers, Dürer, van Kessel, Dou—gathered as one. Every piece has survived some 300 years, through Europe's bloody centuries, carrying its singular image to the world. Sixty-six paintings in total. In a haphazard heap.</p><div></div><p>A lighter is sparked and the flames rise, slowly at first and then wildly, oil paint bubbling, picture frames crackling, the great mass burning and burning until there's almost nothing left but ash.</p><hr><p><strong>After that, what does anything matter?</strong></p><p>Breitwieser is so shattered that he's medicated and placed on suicide watch in the jail. Later he's just numb. He is charged with theft and goes to trial twice, in Switzerland and in France, and serves a total of four years in prison, the punishment modest because no one has been physically injured, and the value of his loot, which some sources placed at over a billion dollars, didn't affect the penalty—in the eyes of the law, there's little difference between mass-produced baubles and Renaissance masterworks.</p><p>In prison he meets with several psychologists. He's described in reports as an “arrogant” and “hypersensitive” man who believes he is “indispensable to humankind” but is never given a diagnosis and is not considered mentally ill at his trials. Because he specifically selected his loot, rather than randomly grabbing, and never displayed guilt about his actions, he doesn't fit the criteria for being a kleptomaniac.</p><p>Breitwieser's mother goes to trial for her role in destroying the works and is found guilty. She spends just a few months in jail. In court it was stated that she thought it was “just a bunch of junk” and that until her son's arrest, she had no clue he'd been stealing. Breitwieser supports these claims, testifying that his mother is unfamiliar with the art world and that he told her he'd picked up trinkets at flea markets. Even though he'd shared a house with her, he'd made sure, he adds, to keep his mother mostly shut out of his life and completely shut out of his room.</p></div></div><div><div><p>Anne-Catherine Kleinklaus spends just a single night in jail. The story she tells the court strains credulity. She had no idea, she says, that her boyfriend was a thief. “I never played the role of the lookout,” she adds. “There were paintings and objects in his room, but nothing struck me as unusual.” Breitwieser, testifying at the trial, doesn't contradict her, gallantly trying to protect her. If he can spare her some punishment, he will.</p><p>He believes his gesture may have worked, at least for her. She is never charged with destroying the art or convicted for direct involvement in the thefts, only for handling and knowledge of the stolen goods. Breitwieser realizes he's still in love and writes her repeatedly from jail. She's his last hope that something worthwhile will remain in his life. But there's never a reply to his letters, and eventually he finds out why. Shortly after his arrest, Kleinklaus had started another relationship, and soon thereafter she was pregnant. By the time Breitwieser learns this, she's the mother of a baby, and he vows never to see her again.</p><hr><p><strong>He's released from prison in 2005,</strong> and at the age of 33 he feels defeated. He had lived a hundred lifetimes while stealing, and now everything is colorless and dumb. He cuts trees for a while, he drives a delivery truck, he mops floors. The relationship with his mother is mended, though he rents a cheap apartment of his own.</p><div></div><p>As a result of his crimes, he says, he's not permitted to enter a museum or any other place showing art. He muddles away a couple of years, the bare walls of his apartment a kind of slow-drip torture, until, as it must with a mania like his, the deep-seated desire breaks through.</p><p>He goes to Belgium, and at an antiques fair, he sees a landscape that slays him—three people strolling through a wintry forest, by one of his favorites, Pieter Brueghel the Younger. He doesn't even try to stop himself and finds that his skills are still sharp.</p><p>With the painting hanging in his apartment, suddenly there's joy in his life. “One beautiful piece,” he says, “makes everything different.” A relationship blooms with a woman he's met, and he admits to her what he's done. She seems to accept the one theft—and, he insists, it's just this one theft—but when the romance ends, she informs the police, and Breitwieser is put in prison again.</p><p>By the time he gets out, he's 41 years old, creases at his eyes and a hairline in retreat. He has an idea that he'll launch a career as a museum-security consultant, but he's the only one who doesn't find this a joke. To hell with everyone, he thinks. “I can live on an island like Robinson Crusoe and it wouldn't bother me,” he says. He eats lunch most days with his mother and then wanders alone in the woods.</p><p>The problem is that he knows exactly what he wants. Just one more sensual blast like the thump he felt every time he unlocked the door to his lair. But when he closes his eyes and tries to conjure the scene, all he can see is a fire.</p><p>Then one day in early 2018, he comes across a brochure for the Reubens House Museum. And there it is, like a slap in the face—a photo of the Adam and Eve ivory, the first thing he'd once regarded every morning. It had been thrown in the canal, but ivory is sturdy and it hadn't been damaged. Now the piece is evidently back on display.</p></div></div><div><div><p>Just looking at the photo pries open some box inside him that he'd hoped had been forever sealed. He's not sure if he ever wants to see the ivory again or if he has to run immediately to the museum. For more than a month, he fights an internal battle before deciding that he needs to go.</p><div><span><picture><div class=\"RIL_IMG\" id=\"RIL_IMG_14\">
            <figure>
            <img alt=\"In 2018, Breitweiser returned to the Reubens House Museum in Belgium and came face-to-face with the Adam and Eve ivory sculpture that he had stolen two decades prior. The ivory had been recovered, undamaged, from the Rhone-Rhine Canal.
            Michael Finkel\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmedia.gq.com%2Fphotos%2F5c66fd5586fbe52d8e189c3a%2Fmaster%2Fw_1600%252Cc_limit%2Fart-thief-gq-Ste%252525CC%25252581phane-Breitwieser-adam-and-eve.jpg\">
            <figcaption>In 2018, Breitweiser returned to the Reubens House Museum in Belgium and came face-to-face with the Adam and Eve ivory sculpture that he had stolen two decades prior. The ivory had been recovered, undamaged, from the Rhone-Rhine Canal.
            Michael Finkel</figcaption>
            </figure>
            </div></picture></span></div><p>He travels to Belgium, enters the Rubens House Museum, and heads to the rear gallery. And there it is, in the same spot, in a reinforced case. Twenty-one years have elapsed since he'd stolen it, but the ivory's power to enchant is unlimited. Breitwieser leans forward, knees bent, so that his face is directly in front of the carving. His eyes widen, his forehead scrunches—the look on his face a jumble of awe and distress. An electric intensity seems to build in him until it appears as if he's ready to combust.</p><div></div><p>He doesn't want to make a scene in the gallery, so he hurries out to the museum's courtyard. The air is warm, spring is coming. He shuffles foot to foot on the pale cobblestones; the wisteria on the walls is just starting to bud. The last time he'd been here, the ivory was under his jacket. This time he stands with nothing at all, tears blurring his eyes, mourning the lost years of his life—not when he was stealing, but since he's stopped.</p><p>He says he only realizes now, in hindsight, what he couldn't possibly have known then: His previous visit to this museum may have marked the high point of his entire life. The absolute pinnacle.</p><p>He aches for what he once was—“a master of the world,” as he puts it—and he weeps for what will never be again. The paintings especially. But also the sheer thrill of it. “Art has punished me,” he says.</p><p>Then he heads to the exit, through the gift shop, where the museum catalog is sold, with a photo of the ivory and a story of its theft. He has no cash—just to get here, he'd borrowed gas money from his mother—and out of habit he notes the positions of the cashier, the security guard, the customers. He checks to see if there are any security cameras. There aren't. He picks up a copy of the catalog and walks discreetly out the door.</p><hr><p><em>Just recently, in early February of 2019, Breitwieser was arrested yet again. French police had reportedly been suspicious for several years that Breitwieser had resumed stealing and searched his residence in northern France. There, French authorities allegedly discovered Roman coins and other objects that police say may have been taken from museums in France and Germany. Breitwieser is currently incarcerated, pending further investigation, and has yet to respond to these newest allegations.</em></p><p><strong>Michael Finkel's</strong> <em>recent book, ‘The Stranger in the Woods,’ was a best-seller—and grew out of his GQ story, “<a href=\"https://www.gq.com/story/the-last-true-hermit\">The Strange &amp; Curious Tale of the Last True Hermit</a>.”</em></p><p><em>A version of this story originally appeared in the March 2019 issue with the title \"The Secrets Of The World's Greatest Art Thief.\"</em></p></div></div></div><aside></aside><div aria-live=\"polite\" aria-hidden=\"true\" role=\"presentation\"></div><div></div><div></div></div></div><div class=\"RIL_IMG\" id=\"RIL_IMG_15\"></div></div>"
    },
    {
        title: "Stripe: Platform of Platforms",
        author: "Ben Thompson",
        url: 'stratechery.com',
        full_url: 'https://stratechery.com/2020/stripe-platform-of-platforms/', 
        reading_time: '8 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-5.png",
        content: 
            "<div lang=\"en\">
            <p>Today <a href=\"https://www.wsj.com/articles/stripe-to-offer-banking-services-in-deal-with-goldman-sachs-citigroup-11607007608\">Stripe is announcing Stripe Treasury</a>; from the company’s <a href=\"https://stripe.com/newsroom/news/treasury\">press release</a>:</p>
            <blockquote>
            <p>Stripe, the technology company building economic infrastructure for the Internet, today announced that it is launching Stripe Treasury. This gives Stripe’s platform users powerful APIs to embed financial services, enabling their customers to easily send, receive and store funds…</p>
            <p>Stripe Treasury…enabl[es] platforms like Shopify to easily offer its merchants access to critical financial products to manage their businesses’ finances. With Stripe Treasury, platforms can offer their users interest-earning accounts eligible for FDIC insurance in minutes, enabled by Evolve Bank &amp; Trust. Platform business customers can have near-instant access to revenue earned through Stripe, spend this directly from their balance with a dedicated card, transfer it via ACH or wire transfer, pay bills, and more.</p>
            </blockquote>
            <p>Stripe Treasury, <a href=\"https://stripe.com/treasury\">as its website notes</a>, is banking-as-a-service, but, critically, Stripe is not a bank; look carefully at the product’s press image:</p>
            <a href=\"https://stripe.com/treasury\"><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-3-1024x614.png\">
            </figure>
            </div></a>
            <p>That is an API call creating a bank account at Goldman Sachs for a pilot on the <a href=\"https://rocketrides.io/\">Rocket Rides</a> platform. Notably, Goldman Sachs is not the only big bank on board; again from the press release:</p>
            <blockquote>
            <p>Stripe is enabling standardized access to global banking capabilities via APIs by developing its bank partner network to include Goldman Sachs Bank USA and Evolve Bank &amp; Trust as US partners, and Citibank N.A. and Barclays as global expansion partners. Stripe will fulfill compliance and regulatory requirements in partnership with its US banking partners to make it easy for platform customers using Stripe Treasury to embed banking experiences into their products. And through Stripe, these banks are able to extend their reach to millions of businesses.</p>
            </blockquote>
            <p>This is a textbook example of the power of platforms; consider an operating system like Windows: any number of applications can run on any number of computers thanks to there being an abstraction layer in the middle:</p>
            <a href=\"https://stratechery.com/2017/amazons-operating-system/\"><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2017%2F01%2FFullSizeRender-1024x861.jpg\">
            </figure>
            </div></a>
            <p>This is analogous to the layer for banking that Stripe is offering with Treasury:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-2-1024x767.png\">
            </figure>
            </div>
            <p>This explains that API call above: a Rocket Rides pilot doesn’t have the wherewithal to open a business bank account at Goldman Sachs, and Goldman Sachs doesn’t have the flexibility to offer a banking account to individual entrepreneurs. This, though, is the exact sort of problem platforms solve: they provide an abstraction layer that connects different sides of a market, even if those different sides have dramatically different needs and capabilities.</p>
            <h4>Stripe and Shopify</h4>
            <p>It is Stripe’s partnership with Shopify, though, that is particularly compelling, and emblematic of both how powerful Treasury can be, and how extensive Stripe’s platform ambitions are. Again from the press release:</p>
            <blockquote>
            <p>For businesses today, accessing financial services can typically involve a series of bureaucratic hoops and a lengthy application process. According to recent Stripe research, setting up an account takes 5 and a half days on average (and 7 days on average for online businesses), around one in four (23%) businesses have to send a fax to open an account, and over half of businesses (55%) are required to visit a branch in person to open a bank account. Financial services simply weren’t designed for the modern internet, and this is a pain point for businesses today: nearly half (46%) of companies report that their banking experience has hindered their company growth.</p>
            </blockquote>
            <p>This is a pain point I know quite well; Stratechery is incorporated in the U.S., and I had to fly back to the U.S. for the express purpose of opening a business bank account!</p>
            <blockquote>
            <p>This kind of off-line banking experience is increasingly incongruous in a world where 76% of businesses (e.g. retailers) use an industry-specific software platform to manage their business, a figure that increases to 92% for businesses with more than 500 employees. The feedback from Stripe’s users is that they want a digital solution for financial services available directly within the software platform that powers their operations. On the flipside, Stripe’s platform customers are increasingly looking to embed financial services into their own product, but oftentimes face barriers to doing so.</p>
            </blockquote>
            <p>Now with Treasury, someone starting up a new Internet business can simply start selling goods, services, or yes, subscriptions, and have their banking needs met by the same software which is powering their business. Stripe co-founder and President John Collison explained in <a href=\"https://stratechery.com/2020/an-interview-with-stripe-president-john-collison\">an interview</a>:</p>
            <blockquote>
            <p>Which seems more ergonomic for a business? That they decide they’re going to start an online store and the very first thing they do is go down to a bank, and maybe in person, and they’re going through that process, they’re setting up their accounts, then they come back and do some white boarding. And they’re like, “Hmm, what should our business be?” That’s not how it works.</p>
            <p>How it works is they have this cool idea and they try it out and they open a Shopify store for it and they have this money coming in. So we need a way to access those funds, now they will be able to, with Shopify Balance, manage their funds directly within Shopify. That latter thing sounds like a much more natural and ergonomic way to handle the cash flows of their business.</p>
            </blockquote>
            <p>What is notable about Shopify is that it too is a platform, and a very powerful one at that. This is how I described the company’s then-new logistics offering in 2019’s <a href=\"https://stratechery.com/2019/shopify-and-the-power-of-platforms/\">Shopify and the Power of Platforms</a>:</p>
            <blockquote>
            <p>What Shopify is doing is what platforms do best: act as an interface between two modularized pieces of a value chain.</p>
            <figure aria-describedby=\"caption-attachment-4282\"><a href=\"https://stratechery.com/2019/shopify-and-the-power-of-platforms/\"><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"Every referral partner, developer, theme designer, and now 3PL provider are simultaneously incentivized to compete with each other narrowly and ensure that Shopify succeeds broadly, because that means the pie is bigger for everyone.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2019%2F07%2FIMG_272058378EFF-2.jpeg\">
            <figcaption>Every referral partner, developer, theme designer, and now 3PL provider are simultaneously incentivized to compete with each other narrowly and ensure that Shopify succeeds broadly, because that means the pie is bigger for everyone.</figcaption>
            </figure>
            </div></a></figure><p>On one side are all of Shopify’s hundreds of thousands of merchants: interfacing with all of them on an individual basis is not scalable for those 3PL companies; now, though, they only need to interface with Shopify.</p>
            </blockquote>
            <p>Thus the title of this Article: Stripe isn’t simply a platform, it is a platform for platforms.</p>
            <h4>Stripe Capital</h4>
            <p>This broader understanding of Stripe’s ambition became clear to me earlier this week with another announcement, <a href=\"https://stripe.com/capital/platforms\">Capital for platforms</a>. Stripe Capital itself is not new; launched in 2019 the service lends money to businesses that use Stripe’s payments processor; as <a href=\"https://www.bloomberg.com/news/articles/2019-09-05/fintech-startup-stripe-will-lend-money-to-customers\">Bloomberg noted at the time</a>:</p>
            <blockquote>
            <p>As the industry has become more digital, PayPal Holdings Inc., Square and even Amazon have introduced small business lending programs, as have a slew of startups including SoftBank Group Corp.-backed Kabbage Inc. and public company OnDeck Capital Inc. Though lending poses risks, Stripe, much like other payment services, says the extra data it has on customers will give it a better idea of whether borrowers can repay loans. The company believes that edge will protect it from significant losses during an economic downturn.</p>
            </blockquote>
            <p>Stripe Capital seemed both obvious and, as the article notes, rather unoriginal; this week’s expansion — which was announced with a <a href=\"https://stripe.com/blog/capital-for-platforms\">29-word blog post</a> — makes clear it is much more. Carefully read this tweet from founder and CEO Patrick Collison:</p>
            <div>
            <blockquote>
            <p dir=\"ltr\" lang=\"en\">Mundane though it sounds, access to capital is the primary bottleneck that limits the growth and expansion of most small businesses.</p>
            <p>So we built Capital for platforms: <a href=\"https://t.co/3BvZhCz9Bo\">https://t.co/3BvZhCz9Bo</a>. Help *your* customers grow faster by using our lending infrastructure. <a href=\"https://t.co/z7xxfM9GFD\">pic.twitter.com/z7xxfM9GFD</a></p>
            <p>— Patrick Collison (@patrickc) <a href=\"https://twitter.com/patrickc/status/1333834822330896384?ref_src=twsrc%5Etfw\">December 1, 2020</a></p>
            </blockquote>
            </div>
            <p>Note the word Patrick Collison emphasized: <em>*your*</em>. Capital for platforms is not for Stripe’s customers, but rather the customer of Stripe’s customers, which is to say, Stripe is asserting itself as the platform of platforms; go back to the news that <a href=\"https://www.shopify.com/balance\">Shopify Balance</a> will be powered by Treasury:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-1-1024x767.png\">
            </figure>
            </div>
            <p>Stripe does not have a customer relationship with all of the Shops on Shopify; that is exactly what Shopify is good at, so why would they? Instead, Stripe is focusing on what it is good at: providing that API layer to banks that will never have the capability to serve Shopify Shops, and exposing said layer to Shopify to incorporate into their product.</p>
            <p>Notably, Treasury skipped the intervening step that Capital started with: Stripe isn’t exposing banking-as-a-service to customers directly on Stripe, but rather making an API available to those customers to offer to their customers. John Collison explained to me:</p>
            <blockquote>
            <p>We have a lot of conviction about this idea that the financial services that a plumber needs will be different from financial services that an e-commerce company needs will be different than financial services that a gym or a yoga studio needs, and they will be provisioned by different companies. Given that we have lots and lots of exposure to those kinds of businesses with our platform partners, this is a great way to get started with that.</p>
            </blockquote>
            <p>This means the above illustration, fully realized, looks a bit like this:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_6\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-5-1024x767.png\">
            </figure>
            </div>
            <h4>Stripe’s Ambition</h4>
            <p>Here I think Stripe’s goal — building the economic infrastructure for the Internet — is instructive. Consider the Internet itself: you are reading this Article on the Internet via a connection provided by an Internet Service Provider, which is a relatively local affair that is designed for a particular geography. It is Internet Service Providers that connect to a grand network of cables that is known as the Internet backbone; this map from <a href=\"https://www.submarinecablemap.com/\">Telegeography</a>, for example, shows the world’s submarine cables:</p>
            <a href=\"https://www.submarinecablemap.com/\"><div class=\"RIL_IMG\" id=\"RIL_IMG_7\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstratechery.com%2Fwp-content%2Fuploads%2F2020%2F12%2Fstripe-4-1024x629.png\">
            </figure>
            </div></a>
            <p>This image is incomplete — major portions of the Internet backbone obviously run overland — but is sufficient for the analogy: Stripe isn’t necessarily competing with other fintech providers — ISPs in this analogy — but instead is seeking to be the backbone for all of them, as well as an entirely new universe of platforms that can offer their unique customers financial services that are perfectly tuned to their needs.</p>
            <hr><p>Stripe is ten years old now, but the ambition implied by these announcements explain why the founders claim they are just getting started. John Collison noted:</p>
            <blockquote>
            <p>We are still very early in developing the set of Stripe products beyond the core payments engine, things like Treasury. We’re building a global payments and treasury network, and we are in November of 2020 launching the Treasury part of it, and so we are just now filling out all the acronyms in our product suite, and that’s the version one of the product. And from a growth point of view, our business is growing really rapidly in APAC and EMEA, and so we’re just early in the business trajectory with all the helter-skelter-ness that comes from that.</p>
            </blockquote>
            <p>Speaking as an analyst, I would like nothing more than to see an S-1 from Stripe, but it sounds like it’s not coming anytime soon (and I can state with a high degree of confidence that Stripe will not be doing a SPAC with any of its rumored suitors); the company is <a href=\"https://www.bloomberg.com/news/articles/2020-11-24/payments-startup-stripe-is-said-in-talks-to-raise-new-funding\">reportedly raising more money</a>, but is increasingly spending the money it raises on acquisitions and investments (one would certainly assume that the core payments business is not only profitable but also has a very attractive cash conversion cycle).</p>
            <p>Instead the company is busy building, well, exactly what it has said it was building all along: economic infrastructure. And, I will freely admit, until this week I didn’t completely appreciate just how mammoth an undertaking that was.</p>
            <p><em>Stratechery subscribers can read the full interview with John Collison <a href=\"https://stratechery.com/2020/an-interview-with-stripe-president-john-collison\">here</a>; it is also available for subscribers <a href=\"https://stratechery.com/podcasts\">via podcast</a>.</em></p>
            </div>"
    },
    {
        title: "Hash Functions",
        author: "Haseeb Qureshi",
        url: 'nakamoto.com',
        full_url: 'https://nakamoto.com/hash-functions/', 
        reading_time: '10 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fnakamoto.com%2Fcontent%2Fimages%2F2020%2F01%2Fhash-functions.png",
        content: 
            "<div lang=\"en\">
            <p>We'll now begin our foray into cryptography. Cryptography is a deep and vibrant field, and there's a lot more to it than we can cover in a single module. But the next four lessons should equip us with the building blocks we need to understand cryptocurrencies.</p>
            <p>The first and most important building block for any cryptocurrency is the hash function. Hash functions are used in almost every component of Bitcoin, so in this lesson we'll explore this cryptographic primitive in depth.</p>
            <h3>What is a hash function?</h3>
            <p>A hash function is a fuanction that deterministically maps an arbitrarily large input space into a fixed output space. That's a pretty abstract description, so instead I like to imagine a hash function as a fingerprinting machine. It takes in an input (often a string of characters) and returns a corresponding cryptographic \"fingerprint\" for that input (often another string of characters). That fingerprint is should be unique to that input, but if you were given some random fingerprint, you shouldn't be able to figure out the original input that produced the fingerprint.</p>
            <figure></figure><p>The input to a hash function is usually called the <strong>preimage</strong>, while the output is often called a <strong>digest</strong>, or sometimes just a \"hash.\" You may have come across terms like SHA-2, MD5, or CRC32. These are names of common hash functions.</p>
            <p>All good hash functions have three important properties:</p>
            <p>First, they are <strong>deterministic</strong>. This means that given the same preimage (input), a hash function always produces the same digest. The digest may be \"random-looking,\" but it's completely deterministic. <code>sha1(\"foo\")</code> should output <code>0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33</code>, now and forever.</p>
            <p>Second, a hash function has a <strong>fixed output size</strong>. For SHA-256, every hash digest is exactly 256 bits. Different hash functions have different output sizes: MD5 digests are always 128 bits, while SHA-1 hashes are 160 bits.</p>
            <p>Third, hash functions should be <strong>uniform</strong>, meaning the digests should be distributed uniformly over the output space. Imagine a histogram of your outputs—you want your digests spread evenly across all possible values.</p>
            <p>You can try out a hash function yourself in a <a rel=\"noopener noreferrer\" href=\"https://repl.it/languages/python3\">Python3 REPL</a> using the following code:</p>
            <pre><code>from hashlib import sha1
            sha1(b\"your string here\").hexdigest()</code>
            </pre>
            <p><em>(Remember to include the <code>b</code> prefix to your input so Python3 knows to treat it as a <code>bytes</code> object rather than a <code>string</code>. Otherwise the hash function will complain.)</em></p>
            <p>Good hash functions tend to have complicated inner workings. But we can build a crappy hash function pretty easily. Here's one:</p>
            <pre><code>def hash(n): 
                return n % 256</code>
            </pre>
            <p>This only works with numbers of course, but we could make it work with strings by interpreting any string's bytes as one giant number and feeding it into this function.</p>
            <p>This function technically satisfies all three hash function properties: it's deterministic, the output size is fixed (0-255), and given large random inputs, the digests will be uniform. Just don't use it in a real application!</p>
            <p>There are many data structures that require hash functions internally, such as hash maps. For most of these data structures, the inputs are not expected to be adversarial; i.e., no one is trying to break your hash map. But if you need stronger security (like Bitcoin does), you should use a more robust subset of hash functions known as <strong>cryptographic hash functions</strong>.</p>
            <p>You would not use a cryptographic hash function in a hash map, since cryptographic hash functions pay for their security with worse performance. But if you're looking for security, only a cryptographic hash function will do.</p>
            <h3>Cryptographic hash functions</h3>
            <p>Cryptographic hash functions come with three additional requirements over normal hash functions.</p>
            <p>The first requirement is that a cryptographic hash function should be <strong>one-way</strong>. This means that given a digest, it should be computationally intractable to invert the hash function and compute its preimage. For example, given <code>88ebe8bf78169fcfe6d8a68b040ca600284bacd3</code>, there should be no way to figure out the preimage was <code>grandma</code>, other than by brute-forcing potential inputs. This is not technically true for all hash functions—for example, checksums like <a rel=\"noopener noreferrer\" href=\"https://en.wikipedia.org/wiki/Cyclic_redundancy_check\">CRC32</a> and <a rel=\"noopener noreferrer\" href=\"https://en.wikipedia.org/wiki/Locality-sensitive_hashing\">locality-sensitive hashes</a> are easily invertible.</p>
            <p>The second requirement is that a cryptographic hash function should exhibit an <strong>avalanche effect</strong>. The avalanche effect basically says that if any single bit changes in the preimage, it should trigger an \"avalanche\" that jumbles the other bits. Thus, two very similar inputs that differ in only one bit should have no discernible relationship between their outputs.</p>
            <p>You can see that SHA-1 has strong avalanching properties:</p>
            <pre><code>import binascii # to convert binary to ASCII bytes
            from hashlib import sha1
            sha1(binascii.b2a_uu(b\"11111111\")).hexdigest()
            # 'a31518892830eec9ea21762e8bb101ce13890aee'
            # let's flip a single bit and see what happens
            sha1(binascii.b2a_uu(b\"11011111\")).hexdigest()
            # 'c68b4d1bb5154c76370f33895d5d9350a4c73ba9'</code>
            </pre>
            <p>The last property that a cryptographic hash function needs is <strong>collision-resistance</strong>. When two inputs hash to the same output, that's known as a collision.</p>
            <figure></figure><p>We never want to see a collision—they are the first sign that a hash function is broken. Given how deeply hash function security depends on collision-resistance, let's take a deeper dive into this property.</p>
            <h3>Collision-resistance</h3>
            <p>Our hash function of <code>def hash(n): return n % 256</code> is not very collision-resistant. This should be obvious when you realize how small its range is: there are only 256 possible digests. Note that we generally measure ranges in the number of bits, in which case our function would have a range of 8 bits, for \(2^{8}\) possible values.</p>
            <p>With a range this small, we could guarantee a hash collision by simply iterating through 257 inputs. By the <a rel=\"noopener noreferrer\" href=\"https://en.wikipedia.org/wiki/Pigeonhole_principle\">pigeonhole principle</a>, we will have to see at least one collision.</p>
            <p>You can scale this argument up. Even for a hash function like SHA-256, which has a range the size of \(2^{256}\), if you tried \(2^{256} + 1\) numbers, you'd have to get at least one collision. Thus, we know that all hash functions must have collisions—after all, all hash functions have finite output sizes, yet infinitely many possible inputs.</p>
            <p>But for all practical purposes, \(2^{256}\) is so big that it's effectively unenumerable. For comparison, the number of atoms in the universe is roughly \(2^{260}\). So while we know that collisions must exist, we're never going to find one in the lifetime of the universe—and, indeed, a collision has never been found for SHA-256.</p>
            <p><strong>So how much collision-resistance do we need?</strong> The <a rel=\"noopener noreferrer\" href=\"https://crypto.stackexchange.com/a/13302\">frontier of computational tractability today</a> is roughly around \(2^{80}\). Below that, it's plausible that a powerful enough actor could brute-force your cryptography. Add some extra headroom for a couple decades of future-proofing, and that rounds us up to 128-bit security as a good rule of thumb.</p>
            <p>So if we want to increase the size of our range, we could level up our hash function! Instead of <code>n % 256</code>, instead we'll do:</p>
            <pre><code>def hash(n): return n % (2 ** 128)</code>
            </pre>
            <p>Now we have a range of \(2^{128}\). Problem solved, right?</p>
            <p>Of course, even with this beefed up range, we don't actually need to brute-force this function to produce a collision. We can find one <strong>analytically</strong>—that is, by reverse-engineering the structure of the function. Knowing the function <code>n % (2 ** 128)</code> wraps around after \(2^{128}\), we can manufacture a collision by simply adding \(2^{128}\) to any number. Thus, \(0\) and \(2^{128}\) would both collide to \(0\).</p>
            <p>So collision-resistance is not just about the size of the range, it's also about the structure of the hash function. This is why robust cryptographic hash functions are harder to build than normal hash functions. For example, while CRC32 is a fine checksum, it's not suitable as a cryptographic hash function. Thanks to its simple mathematical structure, it's trivial to analytically generate collisions for CRC32:</p>
            <pre><code>from zlib import crc32
            crc32(b\"squeamish ossifrage\") == crc32(b\"deltaTvJZx\")
            # True
            crc32(b\"buckeroo\") == crc32(b\"plumless\")
            # True</code>
            </pre>
            <h3>Generating a collision</h3>
            <p>It's not hard to imagine why we want to avoid collisions. If I know the hash of a program you intend to install is <code>d306c9f6c5...</code>, if I generate some other file that hashes to that value, I could wreak all sorts of havoc.</p>
            <p>We saw that we can use analytic methods to generate collisions for simple hash functions like <code>n % (2 ** 128)</code>. But for more robust hash functions, you might assume you'd have to use brute-force: keep trying two random inputs at a time until they collide. For a 128-bit hash function, this should take \(2^{128}\) tries.</p>
            <p>But it so happens that for any hash function <strong>you can always do better than brute-force</strong>. This is a consequence of a simple technique known as a <strong>birthday attack</strong>, so-named after a famous statistical paradox known as the birthday paradox.</p>
            <p>The birthday paradox goes like this: on average, how many guests need to be at a party so that two guests will share the same birthday?</p>
            <p>The answer is, surprisingly, 23. At 23 guests, there is a 50.7% probability that two people share a birthday. At 70 guests, it is 99.9% likely that two share the same birthday.</p>
            <p>Why?</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_1\"></div><p>You can grasp the intuition if you realize that sharing a birthday is a pair-wise relationship, and pairs grow quadratically. So 23 guests means there are \(253\) possible pairs, each of which has a \(\frac{1}{365}\) chance of sharing a birthday. (Recall that the number of unique pairs among \(n\) elements is \(\frac{n(n - 1)}{2}\).)</p>
            <p>\(\sqrt{n}\) turns out to be a good approximation for the 50% likelihood threshold for a birthday collision. This analysis applies to hash functions just as well. For a 128-bit hash function, you should be able to find a collision using a birthday attack in \(O(\sqrt{n})\) time. (This also requires maintaining a table of \(O(\sqrt{n})\) space, since you need to remember every hash you've previously looked at so you can compare all pairs.)</p>
            <p>The birthday attack has many important implications, not least of which being our computational security threshold. Remember how we claimed that \(2^{128}\) security is a good default level of security? The birthday attack is one reason why we don't use 128-bit hash functions—otherwise they could be attacked by simply enumerating over ~\(2^{64}\) iterations (plus \(2^{64}\) space). Instead, if we want 128-bit security, we need a 256-bit hash function.</p>
            <p>MD5 has 128-bit digests, so hypothetically it should have 64-bit security against birthday attacks. That's weak enough to be brute-forceable. But this is only the upper bound of security—if there are weaknesses discovered in the hash function, an analytic attack can make this even easier. In fact, attacks have been discovered that can produce MD5 collisions in roughly \(2^{24}\) complexity. That's about 16 million iterations, which can be done in less than a minute on a laptop. The MD4 hash function, MD5's predecessor, <a rel=\"noopener noreferrer\" href=\"https://eprint.iacr.org/2004/199.pdf\">can have collisions <em>generated by hand</em></a>.</p>
            <p>Collisions are scary. But a collision alone is usually difficult to weaponize in the real world. If the preimages an attacker generates are random and can't be tailored to a specific application, the colliding preimages will probably both just be random garbage. More often, a collision is a sign that the hash function is weakening, and soon people will be able to perform more powerful attacks against it.</p>
            <p>Cryptographic hash functions only get weaker over time as cryptanalysts develop attacks against them. MD5 is now considered broken and should not be used in any new software. SHA-1 has recently joined MD5 in the cryptographic scrap heap: it has a small digest size (160 bits), and Google researchers <a rel=\"noopener noreferrer\" href=\"https://shattered.io/\">demonstrated a collision against it</a> in 2017. Then in January 2020, the first <a href=\"https://sha-mbles.github.io/\">practical chosen-prefix collision</a> was demonstrated against SHA-1, rendering it completely unusable.</p>
            <p>SHA-2 is still considered a strong hash function, and is the default hash function for many cryptographic applications. It also happens to be the major hash function used in Bitcoin and in many Bitcoin-derived cryptocurrencies. In the next two lessons, we'll explore important components of Bitcoin that are built on top of SHA-2: Merkle trees and Hashcash. You should now be equipped to understand why they work.</p>
            <h3>Assignment</h3>
            <p>For your first coding assignment, you'll be producing a hash collision in a new hash function we'll call <code>MD1.25</code>. This hash function is just defined as MD5 truncated to the first 4 bytes (the first 8 hex digits). Thus, <code>MD1.25</code> only has a digest size of 32 bits (\(\frac{1}{4}\) of MD5). You should be able to produce a collision in no more than a couple seconds.</p>
            <p>If you don't have a Repl.it account, first you'll need to <a href=\"https://repl.it/signup\">sign up and create an account</a>. It should take you about 2 minutes.</p>
            <p>Once your account is set up, <a rel=\"noopener noreferrer\" href=\"https://repl.it/classroom/invite/b7VTlAI\">click here to join the Repl.it classroom and get started with the coding exercises</a>. (If you get redirected to set up your account first, click the link again to go back to the course enrollment page.) From there, you'll be able to work on the assignments, run the tests, and submit them when you've completed them. The below video should give you a sense of how to navigate the Repl.it exercises.</p>
            <figure></figure><p>Once you're done with assignment 2.0, you should be set up for future assignments and be ready to move on.</p>
            </div>"
    },
    {
        title: "Systems design for advanced beginners",
        url: 'robertheaton.com',
        full_url: 'https://robertheaton.com/2020/04/06/systems-design-for-advanced-beginners/', 
        reading_time: '41 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Frobertheaton.com%2Fimages%2Fsystems-cover.png",
        content: 
            "<div lang=\"en\">
            <blockquote>
            <p>This post is part of my <a href=\"https://advancedbeginners.substack.com/\">Programming for Advanced Beginners series</a>. <a href=\"https://advancedbeginners.substack.com/\">Subscribe now</a> to receive specific, actionable ways to make your code cleaner, every other week, entirely free.</p>
            </blockquote>
            <p>You’ve started <a href=\"https://robertheaton.com/2018/07/09/how-tinder-keeps-your-location-a-bit-private/\">yet another</a> <a href=\"https://robertheaton.com/2017/10/09/tracking-friends-and-strangers-using-whatsapp/\">company</a> with your good friend, Steve Steveington. It’s an online marketplace where people can buy and sell things and where no one asks too many questions. It’s basically a rip-off of Craigslist, but with Steve’s name instead of Craig’s.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Frobertheaton.com%2Fimages%2Fsystems-cover.png\">
            </figure>
            </div>
            <p>You’re going to be responsible for building the entire Steveslist technical platform, including all of its websites, mobile apps, databases, and other infrastructure. You’re excited, but also very nervous. You figure that you can probably cobble together a small website, since you’ve done that a few times before as part of your previous <a href=\"https://robertheaton.com/2018/07/09/how-tinder-keeps-your-location-a-bit-private/\">entertaining</a>-<a href=\"https://robertheaton.com/2017/10/09/tracking-friends-and-strangers-using-whatsapp/\">if</a>-<a href=\"https://robertheaton.com/2014/12/08/fun-with-your-friends-facebook-and-tinder-session-tokens/\">morally</a>-<a href=\"https://robertheaton.com/2016/10/22/a-tale-of-love-betrayal-social-engineering-and-whatsapp/\">questionable</a> <a href=\"https://robertheaton.com/2019/01/15/a-brief-history-of-wi-fi-privacy-vulnerabilities/\">escapades</a> with the Stevester. But you have no idea how to even start building out all of the other infrastructure and tools that you assume lie behind large, successful online platforms.</p>
            <p>You are in desperate need of a detailed yet concise overview of how real companies do this. How do they store their data? How do their different applications talk to each other? How do they scale their systems to work for millions of users? How do they keep them secure? How do they make sure nothing goes wrong? What are APIs, webhooks and client libraries, when you really get down to it?</p>
            <hr><p>You send a quick WhatsApp to your other good friend, Kate Kateberry, to see if she can help. You’ve <a href=\"https://robertheaton.com/2019/01/15/a-brief-history-of-wi-fi-privacy-vulnerabilities/\">worked together very effectively in the past</a>, and she has decades of experience creating these types of systems at Silicon Valley’s biggest and most controversial companies.</p>
            <p>She instantly accepts your job offer. You had actually only been ringing for some rough guidance and a good gossip, but you nonetheless instantly accept her acceptance. No point looking a gift horse in the mouth, even when you don’t have any money to pay her. Kate proposes that her first day be 5 weeks ago in order to help her smooth over some accounting irregularities. She can come into the office sometime next week. You feel encouraged and threatened by her eagerness.</p>
            <hr><p>Kate bounces into your offices in the 19th Century Literature section of the San Francisco Public Library. “OK let’s do this!” she shouts quietly. “What have we got so far? How are all our systems set up? What’s the plan?” You lean back in your chair and close your laptop, which was not turned on because you have left your charger at home. You steeple your fingers in a manner that you hope can be described as “thoughtful”.</p>
            <p>“Let me flip that question around, Kate. What do <em>you</em> think the plan should be?”</p>
            <p>Kate takes a deep breath and paints an extremely detailed vision of the Steveslist platform five years into the future and the infrastructure that will power it.</p>
            <hr><p>Before we start (says Kate), I want to make it clear that I’m not saying that any of this is necessarily the “right” way to set up our infrastructure. If someone you trust more than me says something different then you should probably do what they say. There are many tools out there, each with different strengths and weaknesses, and many ways to build a technology company. The real, honest reasons that we will make many of our technological choices will be “we chose X because Sara knows a lot about X” and “we chose Y on the spur of the moment when it didn’t seem like a big decision and we never found the time to re-evaluate.”</p>
            <p>Nonetheless, let’s fast-forward five years into the future. Now Steveslist has two main consumer-facing products:</p>
            <ul><li>The Steveslist web app</li>
            <li>The Steveslist smartphone apps</li>
            </ul><p>These are the main ways in which users directly interact with the Steveslist platform. In addition, we also provide an API that allows programmers to build power-tools on top of the Steveslist platform that, for example, create listings for hundreds of items programmatically. To support this, we offer:</p>
            <ul><li>A Steveslist API</li>
            <li>Steveslist API <em>client libraries</em> that make it easy for programmers to write code that talks to our API</li>
            </ul><p>Here, I’ll draw a diagram on the whiteboard:</p>
            <div>
            <div>
            <pre><code>+-----------+   +--------------+   +-----------------+
            |Web Browser|   |Smartphone App|   |Client Libraries/|
            +-----+-----+   +------+-------+   |Other API code   |
            |                |           +-------+---------+
            |                v                   |
            |          +-----+------+            |
            +---------&gt;+ Steveslist +&lt;-----------+
            |  Servers   |
            +------------+
            </code>
            </pre></div>
            </div>
            <p>Finally, we have many, many services running in the background that provide the data and power to these external-facing applications:</p>
            <ul><li>Webhooks - to notify users when something happens to their account, such as “order placed”</li>
            <li>User password authentication - to securely log users in</li>
            <li>SQL database - the main Steveslist data store. Needs to be highly scalable and reliable</li>
            <li>Free-text searching system - to power the search box where people can look for broad search terms like “TVs” or “motorbikes”</li>
            <li>Internal tools - to help us administer the Steveslist platform, and to take actions like issuing sternly-worded warnings to malicious users</li>
            <li>Cron jobs - to run regular tasks that do anything from generating invoices, to billing customers, to sending as much of our users’ data as possible to third-party ad networks</li>
            <li>“Pubsub” system - to allow us to take asynchronous actions on different trigger events (such as “when a new user signs up, send them a welcome email”)</li>
            <li>Big data analytics system - to allow us to run enormous queries over the entirety of Steveslist’s data</li>
            <li>And many more that we’ll talk about another day</li>
            </ul><p>Let’s go through each of these systems in turn. <a href=\"https://robertheaton.com/about/\">Let me know</a> if anything isn’t clear, if you have any questions, or if you think I’ve got something wrong.</p>
            <h2>Before we start - what is a server really?</h2>
            <p>Before we start, let’s define some important terms. In fact, let’s just define one. We’re going to talk a lot about “servers” today. But what is a server, when you really get down to it?</p>
            <p>For our purposes, a server is a computer that runs on a network and listens for communications from other computers. When it receives some data from another computer it performs some sort of action in response and - usually - sends back some data of its own. For example, a web server listens on a network for HTTP requests and sends back webpages and information in response. A database server listens for database queries and reads and writes data to the database that it is running.</p>
            <p>This brief description skips out entire degrees and careers of detail, and there are of course far more precise and accurate ways to define the word “server”. But this should get us through until dinnertime. What did you say? What’s a “network” really? A good question for another day.</p>
            <p>Now we’re ready to talk about the Steveslist platform.</p>
            <h2>Steveslist web app</h2>
            <p>This is the main Steveslist product. It’s just a normal web app, very similar to any website that you’ve built before, except much bigger. It’s a modern “single-page app” (SPA). The “single” in “single-page app” refers to the fact that the user’s browser almost never has to fully reload the page as the user clicks around our site. Instead, when the browser makes its first <em>HTTP request</em> to our servers, we send it back a basic, skeleton HTML page and a big pile of <em>JavaScript</em> code. This JavaScript code executes inside the browser, and updates the view of the page in response to the user’s actions. When the JavaScript wants to send or retrieve data from Steveslist, it sends an Asynchronous JavaScript XML Request (almost always called AJAX for short) in the background to a URL. When our server responds, the JavaScript uses the response to update the browser view accordingly.</p>
            <div>
            <div>
            <pre><code>+----------+1.User's browser visits steveslist.com. +----------+
            |          |  It sends an HTTP request to the       |          |
            |          |  Steveslist servers                    |          |
            |          +---------------------------------------&gt;|          |
            |User's Web|                                        |Steveslist|
            | Browser  |2.Steveslist server responds with a     | Servers  |
            |          |  skeleton HTML page that instructs the |          |
            |          |  browser to request additional         |          |
            |          |  JavaScript files.                     |          |
            |          |&lt;---------------------------------------+          |
            |          |                                        |          |
            |          |3.User's browser requests and receives  |          |
            |          |  these JavaScript files from the       |          |
            |          |  Steveslist server.                    |          |
            |          +---------------------------------------&gt;|          |
            |          |&lt;---------------------------------------+          |
            |          |                                        |          |
            |          |4.User's browser executes the JavaScript|          |
            |          |  code. The code sends more requests for|          |
            |          |  the user's data to the Stevelist      |          |
            |          |  server, and updates the browser UI to |          |
            |          |  display it.                           |          |
            |          +---------------------------------------&gt;|          |
            |          |&lt;---------------------------------------+          |
            +----------+                                        +----------+
            </code>
            </pre></div>
            </div>
            <p><code>robertheaton.com</code> is not a single-page app. Whenever you click on a link, the browser has to reload the entire page. <code>twitter.com</code> <em>is</em> a single-page app. Whenever you click on a link, the browser dynamically updates a small portion of tthe page without forcing a full refresh.</p>
            <p>SPAs are a lot of work to build and maintain, but they sure look good.</p>
            <h2>Steveslist smartphone apps</h2>
            <p>We provide Steveslist smartphone apps for both iOS and Android. They are conceptually very similar to our single-page web app. Both our smartphone and web apps make HTTP requests to our servers. Then our servers receive these requests, do some work and return an HTTP response. Finally, both our smartphone and web apps update their display in order to communicate with the user.</p>
            <p>Since our smartphone apps are performing the same operations as the web app (for example, create listing, send message, etc), they can usually even send their requests to the exact same URLs as the web app. The only extra work that we have to do is to develop the frontends of the apps themselves. Some frameworks even make it possible to write mobile apps using JavaScript, allowing you to reuse code and logic across platforms.</p>
            <h2>Steveslist API</h2>
            <p>We allow users and third-parties to write code that programmatically interacts with our platform. In the same way that people can use the Twitter API to write code that reads; likes; and creates Tweets, we allow them to use the Steveslist API to search; buy; and list items.</p>
            <p>Programmers use our API by writing code that makes HTTP requests to our <em>API endpoints</em>. For example, in order to retrieve a list of all their listings, the programmer sends an HTTP <code>GET</code> request to <code>api.steveslist.com/v1/listings</code>. We respond with the data they requested, formatted as <em>JSON</em>. JSON stands for JavaScript Object Notation, but JSON is not specific to JavaScript and can easily be interpreted by any programming language. A JSON response to a request to retrieve all of a user’s listings might look something like this:</p>
            <p>This structured response format is very easy for a program to parse, which means that the code that made the request can trivially interpret and use the data from our API.</p>
            <p>Users identify or <em>authenticate</em> themselves to our API using an API key. This is, roughly speaking, the API equivalent of a password. It is a long, random string that we generate and display on a user’s “Settings” page. Users include their API key as an <em>HTTP header</em> with every HTTP request that they (or their code) makes to the API. When we receive an API request we check to see whether the attached API key corresponds to a Steveslist user. If it does then we perform the request on behalf of that user.</p>
            <p>If a programmer wants to, they can manually build HTTP requests to our API themselves, using their language’s standard HTTP library. For example, in Python they might write:</p>
            <p>However, we make life easier for programmers by providing <em>client libraries</em>.</p>
            <h2>Steveslist client libraries</h2>
            <p>A client library is a library that “wraps” the functionality of the Steveslist API. This means that anyone using the client library doesn’t need to know anything about the fine details of the Steveslist API. Instead, they can just write:</p>
            <p>Our libraries turn the parameters that they are given into appropriately formatted HTTP requests, which they send to the Steveslist API as normal. We’ve written client libraries for every major programming language that we could think of, and they are by far the most common way that people interact with our API.</p>
            <h2>Webhooks</h2>
            <p>We’ve seen how Steveslist users can use our API to programmatically interact with their account. In addition, many users also want <em>us</em> to proactively tell <em>them</em> whenever something happens to their Steveslist profile. For example, suppose that someone wanted to fully automate the process of selling items on Steveslist. Whenever a customer pays for an item using our new, mostly-secure StevePay system, they want to send the customer a thank you email and automatically instruct their warehouse to ship a stolen TV to the order address. Our seller could constantly and repeatedly query the Steveslist API asking “Any new sales? Any new sales?” However, this would be very inefficient, and would put a lot of unnecessary load on our servers.</p>
            <p>Instead, we provide an industry standard system called <em>webhooks</em>. A webhook is an HTTP request that we send to our users whenever something interesting happens to their account. It contains all the data describing the event that just happened - for example, the item ID, the price, the buyer ID, buyer address, and so on. Webhooks allow users to automatically perform response actions, such as the aforementioned email and auto-shipping.</p>
            <p>To use webhooks, the user tells us the URL to which they would like us to send their webhooks (for example, <code>steveslistwebhooks.robertheaton.com</code>. They set up a web server at that URL that will receive and action these webhook notifications.</p>
            <div>
            <div>
            <pre><code>1.Buyer purchases an      3.The seller's server receives the webhook,
            item as normal.           and uses the information in it to
                automatically process the order.
            +---------------+           +-----------------+
            |Buyer's Browser|           |Seller's Webhook-+
            +------+--------+           |Receiving Server |
            |                    +------+----------+
            |                           ^
            |                           | 
            |                           |   
            |    +-----------+          |
            +---&gt;+ Steveslist+----------+
            |   Server  |
            +-----------+
            2.Once the transaction has been completed,
            Steveslist looks up the seller's
            webhook URL (if set). We then send
            an HTTP request to this URL,
            containing the details of the purchase.
            </code>
            </pre></div>
            </div>
            <p>The user deploys code on their web server that performs the appropriate response actions whenever it receives a webhook from us. We don’t only send webhooks when an item is purchased - we also send them when a user receives a message; when one of their items is removed by an admin; or when a buyer makes a complaint. This enables Steveslist sellers to automate not just the listing of items, but also the selling and shipping of them too.</p>
            <h3>Webhook complications</h3>
            <p>Webhooks come with two main complications - security and reliability. First, let’s talk security. The endpoint to which the seller instructs us to send their webhooks is accessible to anyone on the internet. Anyone who knows the URL can send it fake webhooks, and if our seller isn’t careful this could allow an attacker to trick them into, for example, sending the attacker free stuff. A seller’s webhook URL should be difficult to find, since the seller won’t publicize its existence, but obscurity is not the same as security.</p>
            <p>To allow our sellers to verify that a webhook really was sent by Steveslist, we <em>cryptographically sign</em> our webhook contents.</p>
            <h4>Cryptographic signing</h4>
            <p>Cryptographic signing is a deep and subtle topic. Here’s a condensed version of how we use it to secure our webhooks.</p>
            <p>When a seller enables webhooks for their account, we generate a random “shared secret key”. We tell the seller to copy this key over to their webhook-receiving server and keep it secure, so that its value is known only by us and the seller. Whenever we send a webhook, we take this shared secret, combine it with the contents of the webhook by using a <em>cryptographic hash function</em> called <em>HMAC</em>, and get back a long, random-seeming, but entirely deterministic “signature” for the webhook.</p>
            <div>
            <div>
            <pre><code>Webhook contents   +----------+
            (eg. {\"id\": 123...})          |
                    v
                +---+----------+
                |HMAC Algorithm|-----&gt;Webhook signature
                +---+----------+
                    ^
            Shared secret key             |
            (eg. 123mhu23jy8xdwgmd...)+---+
            </code>
            </pre></div>
            </div>
            <p>We include this signature in our webhook body, for example:</p>
            <p>When the seller’s webhook-receiving server receives a webhook, it takes the shared secret and webhook contents and calculates what it expects the signature to be in exactly the same way that we did. It compares the result to the signature attached to the webhook; if they match then it accepts and processes the webhook. Since the signature can only be generated using the shared secret known only by us and the seller, the webhook-receiving server can be confident that the webhook was sent by us. If the signatures don’t match, however, the server rejects the webhook.</p>
            <p>Note that all of the signature verification code must be written and maintained by the sellers. We can provide them with encouragement and examples, but we can’t force them to verify signatures correctly, or even at all. For some real-world examples, look at how <a href=\"https://stripe.com/docs/webhooks/signatures\">Stripe</a> and <a href=\"https://developer.github.com/webhooks/securing/\">GitHub</a> sign their webhooks.</p>
            <h3>Reliability</h3>
            <p>We also need to consider what happens when our webhooks go wrong and what guarantees we want to make to our users about them. If we try to send a webhook but can’t connect to the user’s server, what should we do? What if we successfully connect to their server and send the webhook, but their server sends back an error? What if we send the webhook but the server hangs for twenty seconds before disconnecting without telling us what happened?</p>
            <p>There are tradeoffs involved here that require clear communication and a surprising amount of infrastructure to manage. We at Steveslist choose to guarantee that we will deliver webhooks “at least once”. This means that if a webhook fails to send then we will keep trying (a large but not infinite number of times) until it does. If we’re not sure whether a webhook succeeded or failed, we will keep trying until we are sure that an attempt has succeeded. This might occasionally result in us sending the same webhook twice, but it’s the seller’s responsibility to have their code handle this gracefully instead of sending the same customer five stolen TVs instead of the one that they ordered.</p>
            <hr><p>“What do you think so far?” asks Kate. “Is this roughly what you’ve been thinking?” You make a non-committal face and take a big bite of a nearby sandwich in order to preclude any further discussion. Kate continues:</p>
            <hr><p>Let’s talk about the backend systems that will power some of our most important features.</p>
            <h2>User authentication via a password</h2>
            <p>Most users sign into the Steveslist website and smartphone apps using a username and password. There are other ways of signing into the website, like <em>OAuth</em> - we’ll cover them another time.</p>
            <p>We will have to store our users’ passwords very securely. Not only is a user’s password the thing they use to sign in (or <em>authenticate</em>) to Stevelist, but for many users they’re probably the same password that they use to sign in to many other services too, despite all the warnings that this is a bad idea. They only have so many children with so many birthdays, after all.</p>
            <p>Rupert Herpton has written <a href=\"https://robertheaton.com/2019/08/12/programming-projects-for-advanced-beginners-user-logins/\">the seminal tutorial</a> on how to secure passwords, which I’m sure you’ve read already. Just in case you need a refresher, the crux of the matter is that we mustn’t ever store passwords in their original, <em>plaintext</em> form, anywhere. We mustn’t store them in plaintext in a database, in a log file, or in any other part of our system. Instead, before storing a password, we must first <em>hash</em> it using a <em>hash function</em> such as <em>bcrypt</em>.</p>
            <p>A hash function is a “one-way” function that takes an input and deterministically converts it into a new, seemingly-random string. Calculating a hash value from an input is computationally very easy, but reversing the transformation and recovering the original input from its hash value takes so much time and computing power that it is, practically-speaking, impossible.</p>
            <div>
            <div>
            <pre><code>+---------+     Easy    +----------+
            |         |------------&gt;|          |
            |Plaintext|             |Hash value|
            |         |&lt;------------|          |
            +---------+ Essentially +----------+
            Impossible
            </code>
            </pre></div>
            </div>
            <p>Since we only store the hash values of our users’ passwords, if a hacker somehow stole our password database (heaven forbid) (touch wood) then they would only be able to see the passwords’ hash values. They wouldn’t be able to easily turn these hash values back into <em>plaintext</em> passwords, meaning that they wouldn’t be able to use them to login to Steveslist, and they won’t be able to take advantage of all the people who re-use passwords across different services.</p>
            <p>Since we only store password hash values, when we want to check whether a login password that a user has given us is correct we first calculate its hash value, and compare the result to the hash value in our password database.</p>
            <div>
            <div>
            <pre><code>                 +--------------+
            |User's Browser|
            +-----------+--+
            1.User attempts to  |        ^
            log in:             |        |  3. Server logs the user in
            |        |  if password matches, and
            username: steve  |        |  returns an error if it does not.
            password: 12345  v        |  
            +--+--------+--+
            |  Steveslist  |
            |    Server    |
            +--------------+
            |
            2.Server computes       |     |username|password_hash|
            the hash of the         |     +----------------------+
            password and compares   +----&gt;+steve   |23jy7213y7jO |
            it to the value in the        |kate    |21897213hh21 |
            database.                     |...     |...          |
            </code>
            </pre></div>
            </div>
            <p>As previously mentioned, we also have to be careful not to log passwords in any of our debug output. This can be an easy mistake to make - if you log all of the parameters of every HTTP request, just in case you need it, you will certainly be logging user passwords. Facebook logged passwords in plaintext for many years, and whilst it didn’t seem to affect their stock price I’m sure it made them feel pretty silly for a day or two.</p>
            <hr><p>Kate pauses for breath. You pretend to take notes on your out-of-battery laptop.</p>
            <hr><p>Let’s talk about infrastructure.</p>
            <h2>Database servers</h2>
            <p>Steveslist’s primary data store runs a common flavor of SQL database called <em>MySQL</em>. I won’t talk much about SQL here - the specifics of how it works aren’t too important, and you can find them on Google if you’re interested. Since we are so successful, we have a huge and growing amount of data to manage. We’ve started to become acutely aware that databases aren’t magic. They run on computers, just like any other program. As the volume of data in a database grows, the computer’s hard drive and memory starts to fill up. At the start of the company the easiest way to deal with this problem was to run our database on a single computer (or <em>machine</em>) with an enormous hard drive. But this could only stave off the problem for so long. Eventually our database contained more data than could be stored on any reasonable hard drive, and the database started to get slower and slower as we forced it to search through more and more records.</p>
            <p>We had to take an entirely new approach, and reconfigure our database to store its data on multiple computers. We did this using <em>sharding</em>.</p>
            <h3>Sharding</h3>
            <p>Sharding a database means splitting its data into chunks (or “shards”) and storing each chunk on a separate machine. All of the machines that make up a database are together known as a <em>database cluster</em>. How you split data between machine in your cluster depends on your application and the types of operations you will be performing. For Steveslist we chose to split out our data by user. This means that all of the data for a given user is stored on the same machine. This includes their profile information, their listings, their messages, and so on. Data that doesn’t have a corresponding user (like “Deals of the Day”) can be sharded using a <em>shard key</em> other than user, or not sharded at all if the dataset is sufficiently small.</p>
            <p>This means that we need an extra “routing” layer in front of our database, which knows which database machine is able to service which queries. We could either make our <em>application servers</em> (the servers that execute our code) responsible for knowing the mapping of user IDs to database shards, or have a centralized “router” that all servers send their requests to, and which is responsible for working out the appropriate machine to forward the request on to. Both approaches have their advantages. Making application servers responsible for maintaining the mapping reduces the number of hops that a request has to make, speeding them up. But having a centralized router makes updating the shard mapping much easier, since you only have to update it in one place.</p>
            <p>If application servers are responsible for knowing the shard layout, we have a system that looks like this:</p>
            <div>
            <div>
            <pre><code>            +          +          +
            |          |          |
            | Requests from users |
            |          |          |
            v          v          v
            +------+----------+--------------------+
            |            Application               |
            |               Server                 |
            |                                      |
            |Database sharding config:             |
            |* DB Server 1: IDs between 1-10000    |
            |* DB Server 2: IDs between 10000-20000|
            |* DB Server 3: IDs between 20000-30000|
            +-+----------------+-----------------+-+
            |                |                 |
            |ID:501          |ID:15362         |ID:22361
            |                |                 |
            v                v                 v
            +-----------+     +-----------+     +-----------+
            |DB Server 1|     |DB Server 2|     |DB Server 3|
            +-----------+     +-----------+     +-----------+
            </code>
            </pre></div>
            </div>
            <p>If we have a centralized database router, we have a system that looks like this:</p>
            <div>
            <div>
            <pre><code>            +          +          +
            |          |          |
            | Requests from users |
            |          |          |
            v          v          v
            +------+----------+--------------------+
            |            Application               |
            |               Server                 |
            |                                      |
            |Database sharding config:             |
            |* ??? Application Server has no idea  |
            +------------+-----------+-------------+
            |           |
            Application server sends all database
            queries to the database router, which
            forwards them to the correct shard.
            |           |
            v           v
            +--------------+-----------+-------------+
            |            Database router             |
            |                                        |
            |Database sharding config:               |
            |* DB Server 1: IDs between 1-10000      |
            |* DB Server 2: IDs between 10000-20000  |
            |* DB Server 3: IDs between 20000-30000  |
            +---+----------------+---------------+---+
            |                |               |
            |ID:501          |ID:15362       |ID:22361
            |                |               |
            v                v               v
            +-----------+    +-----------+   +-----------+
            |DB Server 1|    |DB Server 2|   |DB Server 3|
            +-----------+    +-----------+   +-----------+
            </code>
            </pre></div>
            </div>
            <p>How do we decide which database to assign each user to? However we want. We could start by saying that users with odd-numbered IDs are assigned to shard machine 1, and those with even-numbered ones are assigned to shard machine 2. We could hope that this random assignment results in balancing our data relatively evenly between our shards.</p>
            <p>However, it’s possible that we might have a small number of power users who create much, much more data than others. Maybe they create so much data that we want to allocate them a shard of their own. This is completely fine - we have complete control over how users are assigned to shards. Random assignments are usually easiest, but are by no means the only option. We can choose to assign users to shards randomly - except for user 367823, who is assigned to shard 15, which no other user is assigned to.</p>
            <p>If a shard gets too big and starts to fill up its machine’s hard-drive, we can split it up into multiple, smaller shards. How do we migrate the data to these new shards without having to turn off our platform for a while? Probably using a sequential process like:</p>
            <ol><li>“Double-write” new data to both the old and new shards. Continue reading data from the old shard</li>
            <li>Copy over all existing data from the old shard to the new shard. Continue double-writing to old and new shards</li>
            <li>Convince ourselves that the old and new shards contain the same data. We could do this by comparing snapshots of the databases, and/or by reading every query from both databases, comparing them, and alerting if the data is different</li>
            <li>Once we’re confident that the new and old shards contain the same data, we switch to reading from the new shards. We continue double-writing in case we need to switch back</li>
            <li>Once we’re confident that this step has been successful, stop double-writing and delete the old shard</li>
            </ol><p>Rupert Herpton has written <a href=\"https://robertheaton.com/2015/08/31/migrating-bajillions-of-database-records-at-stripe/\">a great article about a broadly-similar type of migration</a> that goes into much more detail. In short, the process of migrating data between database shards is detailed and finicky, but also entirely doable and logical. Some types of database can even automatically take care of sharding for you.</p>
            <h3>Replication</h3>
            <p>We will take great care to make sure that our data doesn’t get accidentally deleted and our database servers don’t randomly stop working. But accidents happen, and sometimes computers do randomly stop working. To mitigate the fallout of a database-related disaster, we <em>replicate</em> our data across multiple machines in (close to) realtime.</p>
            <div>
            <div>
            <pre><code>       Client writes new
            data to DB Server A
            |
            v
            +-----+-----+
            |DB Server A|
            ++---------++
            |         | DB Server A quickly replicates
            v         v the new data to DB Servers B and C
            +----------++      +-+---------+
            |DB Server B|      |DB Server C|
            +-----------+      +-----------+
            </code>
            </pre></div>
            </div>
            <p>Realtime replication has two main benefits. First, it means that if one of our database servers explodes, catches fire, or otherwise stops working, we have multiple almost-perfect copies of it that can seamlessly pick up the slack. In a straightforward, vanilla failure, users of our services may never know that anything has gone wrong. The second benefit of replication is that we can distribute queries for the same data across multiple database servers. If lots of users are asking to read data from the same database, we can split their queries up across all of our copies, meaning that we can handle more queries in parallel.</p>
            <div>
            <div>
            <pre><code>  Clients can query the same data
            from any of DB Servers A,B,or C
            |  |    |   |   |    |  |
            |  |    v   v   v    |  |
            |  |  +-+---+---+-+  |  |
            |  |  |DB Server A|  |  |
            |  |  +-----------+  |  |
            |  |                 |  |
            v  v                 v  v
            +---+-------+      +-----+-+---+
            |DB Server B|      |DB Server C|
            +-----------+      +-----------+
            </code>
            </pre></div>
            </div>
            <p>Note that database replication is not the same concept as making database backups. Replication happens in (close to) realtime, and is designed to deal on-the-fly with isolated problems in your live (or <em>production</em>) systems. Database backups are performed on a schedule (for example, once a day), and the copies of the data are stored separately, away from production systems. Backups are designed as a final safeguard against widespread, catastrophic data loss, as might happen if all your data centers burned down.</p>
            <p>Pretend that you work in the call centre of a big bank, where people are constantly calling you up in order to send money transfers and ask about their current balance. Replication is the equivalent of hurriedly writing down transfer details into multiple books, as you receive them, just in case you lose one of the books. Making backups is the equivalent of photocopying those books once a day and storing the copies in your filing cabinet.</p>
            <h3>Replication</h3>
            <p>The process of replication is conceptually quite straightforward. Whenever new data is written to our database, we copy it to multiple machines. This means that if/when a machine fails we can continue querying its siblings, with no user-facing impact. It also means that we can spread our queries out across multiple copies of the same data, resulting in faster query response times.</p>
            <p>However, the details of how we do this are important, subtle, and situation-dependent. There’s no right way to do replication - as is so often the case, the exact approach that you take depends on the specific requirements and constraints of your system.</p>
            <p>Replication is complicated by two inconvenient facts about the real world. First, database operations randomly fail sometimes. When we attempt to replicate our data from one server to another, things <em>will</em> occasionally go wrong, causing our machines to become out of sync, at least for a period. Second, even in periods of smooth operation, replication is not instantaneous. When new data is written to our database cluster, some servers in the cluster will always know about the update sooner than others. When choosing a replication strategy we must be aware of these limitations and how they impact our particular application. For example, maybe it’s OK to risk the number of “likes” on a post being slightly out of sync and out of date, but not the amount of money in a user’s bank account.</p>
            <p>One of the biggest decisions we must make when choosing a replication strategy is whether we want our replication to be <em>synchronous</em> or <em>asynchronous</em>.</p>
            <h3>Asynchronous vs synchronous replication</h3>
            <p>Replication can be handled either synchronously or asynchronously. These words come up in a lot of different places in systems design, but they always fundamentally mean the same thing. If an action is performed “synchronously” then this means that it is performed while something waits for it. If you wanted to send a letter synchronously then you would got to the Post Office, give your letter to a postal worker, sit and wait in the corner of the Post Office for a few days, and only go home when the postal worker confirmed that your letter had arrived safely.</p>
            <p>By contrast, if an action is performed “asynchronously” then this means that the initiator of the action doesn’t wait around for it to be finished. Instead, they just continue with the rest of their work while the action is worked on in the background. The real Postal Service is asynchronous; you give your letter to a postal worker, then leave and continue with your life while the Postal Service delivers your letter. Asynchronous actions can send back notifications of the outcome of the action if they want (“Your letter has arrived and was signed for” or “Mr. Heaton, your dry-cleaning is ready”), but they don’t have to.</p>
            <p>How do these principles apply to database replication? In synchronous replication a client writes its new data to a database server and then waits. This server doesn’t tell the client whether their write has been completed successfully until it has finished fully replicating the client’s data across all of its sibling servers. Then, and only then, does the client continue executing the rest of its code.</p>
            <p>In asynchronous replication the client writes its data to the first database server, as before. But this time the first server tells the client that the write was successful as soon as it has written the data to its own data store. It kicks off the replication process in the background, and doesn’t wait for it to complete, or even start, before it tells the client that the write was successful. This means that the asynchronous operation appears much faster than the synchronous one, because the client doesn’t have to wait for all the replication to complete before it can continue on with the rest of its work. However, if the background replication fails then the client will believe that it has successfully written its data to the database when it actually has not. This could cause problems, the imagination of which is left as an exercise for the reader.</p>
            <p>The synchronous approach is slower but “safer” than an asynchronous approach. The database never claims to have successfully received and stored any data until it has finished every single step of doing so. Because the client waits for full confirmation before proceeding, it is guaranteed to never encounter a situation where the client believes that it has written some data to the database, but the database has secretly partially dropped the data on the floor.</p>
            <p>Which approach is better? It depends. Do you care more about speed or correctness? Is it OK to occasionally have inconsistent data if it significantly speeds up the system’s operation? Or will even a single mistake cause airplanes to start falling out of the sky?</p>
            <p>Database replication systems face other interesting problems too. Many of these problems are “logical” rather than “technical”, and don’t require a detailed understanding of the complex software and networking protocols underlying the system. To help picture some of them, imagine that you’re back in the call centre from a few paragraphs ago. You work with many other colleagues. Together you play the role of database servers. People call you up to transfer money and ask about their current balance. You and your colleagues write information about new transfers down on paper, and replicate it between yourselves by calling each other up.</p>
            <p>The problems faced by our database cluster are identical to those faced by our call-centre. What happens if two people call in and try to update the same piece of data at the same time by talking to two different employees? Maybe we solve that by having a “leader” employee who is the only one authorized to serve calls from people trying to create transfers. All the other employees are only allowed to answer queries about current balances. OK, so what happens if the leader has a stroke and dies after they’ve received some new data, but before they’ve told everyone else about it? What if a “follower” employee has a temporary seizure for a few seconds and misses some data updates from the leader? What if we’re never entirely sure which of our employees are dead or alive?</p>
            <p>This is not an approximate metaphor that breaks down if you look too hard or start asking awkward questions. Understand this weird call-centre and you understand database replication. <a href=\"https://www.brianstorti.com/replication/\">Read this blog post</a> and you <em>really</em> understand replication.</p>
            <h3>Backups</h3>
            <p>In addition to realtime replication, we also store regular backups of our entire database in order to protect against catastrophic disaster. We copy all of the data in our database, store it somewhere safe, and mark it as something like “database backup, 2020-05-11:01:00”.</p>
            <div>
            <div>
            <pre><code>+----------+  
            | Database +----------------&gt; database backup, 2020-05-11:01:00
            +----------+                  database backup, 2020-05-10:01:00
                    database backup, 2020-05-09:01:00
            </code>
            </pre></div>
            </div>
            <p>If our entire database - or some fraction of it - somehow gets wiped out, we grab the most recent backup available and load it into new database machines.</p>
            <div>
            <div>
            <pre><code>+----------+
            |   New    |
            | Database +&lt;---------------- database backup, 2020-05-11:01:00
            +----------+                  database backup, 2020-05-10:01:00
                    database backup, 2020-05-09:01:00
            </code>
            </pre></div>
            </div>
            <p>Such a calamity could be caused by a cyberattack, a disaster in our data centre, a database migration gone horribly wrong, or any number of unlikely but potentially company-destroying reasons. Unlike realtime replication, restoring a database from a backup is a slow process that our users will definitely notice. Our entire system will likely be offline until the restoration is complete. We will have lost all database updates that occurred after the last backup but before the disaster. And there will no doubt be a large number of knock-on effects as users and systems try to access data that used to be there but is now lost forever. Nonetheless, these are all much less terrible than the complete, irrevocable loss of all our company’s data and with it, our company.</p>
            <p>That’s a whole lot of detail about our primary SQL database. Let’s talk about some other ways in which we store and query data.</p>
            <h2>Free-text searching</h2>
            <p>A standard SQL database is very good at answering well-defined and specific queries. For example:</p>
            <ul><li>Give me all the item listings created by user #145122 in the last 90 days</li>
            <li>Give me the listing details for item #237326921</li>
            <li>Give me the count of new listings per day created in San Francisco with the label “Electronics”</li>
            </ul><p>You can retrieve this kind of data using SQL queries that look something like this:</p>
            <p>You’ll notice that these queries contain a lot of very “precise” operators, like equals- and greater-than-signs. But how would you write a SQL query that could return a list of items that match a Google-style “full-text” search, like “second-hand TV”?</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Frobertheaton.com%2Fimages%2Fsystems-design-search.png\">
            </figure>
            </div>
            <p>This would be difficult. You could try the SQL <code>LIKE</code> operator, which allows you to find records containing a pattern, such as a sub-string. For example, the following query finds all items with a description containing the sub-string <code>second-hand TV</code>:</p>
            <p>However, this query will only match the very specific sub-string that it was given. It won’t match “TV that is second-hand” or “2nd-hand TV”, or “sceond-hnad TV”. Given enough perseverance you could theoretically construct a giant SQL-query that covered as many of these permutations as you had patience for:</p>
            <p>However, the resulting query would be slow and cumbersome, and would still miss many important edge-cases that you hadn’t thought of. And even if you did get back a useful list of search results, you’d still have a lot of work left to do in order to decide how to order them. The ordering you want isn’t something strict like “most-recent first” or “alphabetically by title”. Instead it’s some much woolier notion of “best” or “most-relevant” first.</p>
            <p>All of this means that SQL databases are generally not very good at full-text searches. Fortunately, other types of database are, including one called <em>Elasticsearch</em>. Elasticsearch is often described as a <em>document-oriented database</em>. We don’t need to go into the details of how exactly it works, but the important part for us is that, unlike SQL databases, Elasticsearch is very good at taking queries like “second-hand TV” and returning a list, ordered by “relevance,” of the fuzzy matches that users have come to expect from their search engines.</p>
            <p>“Sounds like Elasticsearch is just better than SQL,” you might say. “Should we throw away our SQL databases and put everything in Elasticsearch instead?” Not so fast. Elasticsearch, and other databases like it, have their strengths, but they also have their weaknesses. They’re typically somewhat less reliable than SQL databases, and are somewhat more likely to accidentally lose data at large scale. They’re also often much slower to write new data to.</p>
            <p>As we’ve already noted, there’s rarely a single, universally “best” tool for any type of job. There’s certainly no such thing as “the best” database. Instead, different databases have different strengths and weaknesses, and different solutions are appropriate for different tasks. At Steveslist, we care so much about making sure that we use the right tool for the right job that we store our data in both a SQL database <em>and</em> a document-oriented database like Elasticsearch.</p>
            <p>We use our SQL database as our primary data store. It is our authoritative “source-of-truth”, and we write all our new data to it before we write it anywhere else. We do all our simple, precise reads from it, especially when accuracy and up-to-date-ness are our principal requirements. If we want to show a user a list of all their open listings, we read it from our SQL database. If we want to validate a users password, we read that from our SQL database too. This plays to the strengths of SQL databases.</p>
            <p>However, in the background we also copy data from our SQL database into an Elasticsearch database, and send any full-text search queries made via our search box to Elasticsearch. This means that we benefit from the strengths of Elasticsearch, and are not impacted too much by its weaknesses. We copy over batches of records every few minutes, hours, or days, depending on the requirements of the specific dataset. Alternatively, we could watch the logs of our SQL database for new or updated records, and create or update the corresponding Elasticsearch records in near-realtime.</p>
            <div>
            <div>
            <pre><code>                   |      |                   |
            New data is always |      |Queries for        |Full-text search 
            written to the SQL |      |specific data go   |queries go to
            DB first           |      |to the SQL DB too  |Elasticsearch
            V      V                   V
            +--------------+       +---------------+
            | SQL Database +------&gt;+ Elasticsearch |
            +--------------+       +---------------+
            New data is copied over from
            the SQL DB to Elasticsearch
            </code>
            </pre></div>
            </div>
            <p>When scouting Steveslist’s competition, I noticed that when you create a new Craigslist listing it says “your listing will be visible in search within the next 15 minutes”. I’ll bet pesos to pizza that this is because their primary datastore is a SQL database, but their search box is powered by an Elasticsearch-like engine. Their pipeline for replicating data from SQL to search probably takes around 15 minutes, which they’ve decided (very reasonably) is an acceptable delay for their particular use-case.</p>
            <p>We’ve said that Elasticsearch is somewhat less reliable than most SQL databases. If Elasticsearch accidentally loses a few of our records then they won’t appear in search results. This would be a shame that would make our product a bit worse, and we try our hardest to avoid it happening. However, it wouldn’t be a disaster in the same way that losing data from our primary data store would be. We only copy data over to Elasticsearch once it has been written to and safely captured by our source-of-truth SQL database, which is what really matters.</p>
            <hr><p>It’s 6pm, and the library is closing. A librarian tries to ask you to leave. Kate shoos him away with a barrage of crumpled-up balls of paper.</p>
            <hr><h2>Internal tools</h2>
            <p>Managing the Steveslist platform requires bespoke internal tools. We need to perform a wide range of administrative tasks, like deleting listings that are too illegal (even for us), issuing refunds, viewing a user’s personal details in order to help with support requests, and so on. Many of these tools will be used by people who work on other teams at Steveslist, like finance, compliance, legal, sales, and support. Since the tasks they need to perform are so specific to the way that Steveslist works, we can’t do them using third-party tools, and have to build our own instead.</p>
            <p>In the early days, we baked this functionality into the main Stevelist product and only exposed it to users who had the <code>is_steveslist_employee = True</code> database flag set. This was an OK approach for a small company, but was also fragile and easy to mess up. It was scary to think that we were exposing all of our superuser powers through the same servers that run our main product. It made the potential consequences of a security flaw in our application much worse, and created new types of mistake for us to make, such as forgetting to disable the <code>is_steveslist_employee</code> flag for an acrimoniously fired employee.</p>
            <p>Once Steveslist reached a certain level of maturity, we built ourselves an entirely separate admin platform, with separate, hardened authentication and authorization. The service runs on entirely separate servers, and requires a <em>VPN</em> and a <em>TLS client certificate</em> to access (which we’ll talk about another day). This separated out our internal- and external-facing products, and drastically reduced the chance of a boneheaded error exposing our admin tools to the world. We’re frequently building new internal tools - one for user administration, one for server management, one for fraud detection, and so on. These services all talk to the same database as our user-facing products, so they all have access to the same data. They just run on different servers that are completely inaccessible to the outside world.</p>
            <h2>Cron jobs</h2>
            <p>There are lots of tasks that we’ll want our system to perform at fixed times and intervals. For example:</p>
            <ul><li>Emailing ourselves weekly usage reports</li>
            <li>Sending marketing emails to users</li>
            <li>Charging the credit cards users who have a subscription plan with us</li>
            <li>Copying data from our SQL database to Elasticsearch</li>
            </ul><p>The most common tool used for running scheduled jobs is called <em>cron</em>, a tool that is built into Unix operating systems. It’s so common that people will often call any kind of scheduled job a “cron job”, even when it’s not actually being run by cron.</p>
            <p>You can set up a cron job on your own computer by running:</p>
            <div>
            <div>
            <pre><code>crontab -e
            </code>
            </pre></div>
            </div>
            <p>Then typing into the prompt:</p>
            <div>
            <div>
            <pre><code>*/5 * * * * $YOUR_COMMAND
            </code>
            </pre></div>
            </div>
            <p>This will cause your computer to execute <code>$YOUR_COMMAND</code> every 5 minutes.</p>
            <p>Steveslist currently has a very simple and slightly fragile cron setup. We have a single “scheduled jobs server”. We use <code>crontab</code> on this server (as above) to tell it the commands that we want it to run, and the schedule that we want it to run them on. This setup isn’t scaling very well - if the scheduled jobs server explodes or even hiccups then we don’t always know which jobs it did and didn’t run, and have to scramble to bring up a replacement server. And even though the server is very beefy and powerful, eventually we’re going to want to run more jobs than it can handle at once. We’re considering either splitting up our cron jobs into multiple servers, or setting up a new system using a modern tool like <em>Kubernetes</em>.</p>
            <h2>Pubsub</h2>
            <p>Actions have consequences. This is both the subject line of the sinister emails that we send to people who write mean things about us on the internet, and the reason we have a <em>pubsub</em> system.</p>
            <p>When a new user signs up, we want to send them an email welcoming them to Steveslist and reminding them about the action-consequence relationship I just described. When a new listing is added for a stolen TV in San Francisco, we want to notify users who have set up search alerts for TVs in the Bay Area. When a card is declined for a subscription, we want to email the responsible user to politely threaten them. When a new listing is added, we want to perform some extremely cursory anti-fraud checks. When an item is purchased, we want to sent a webhook to its seller. And so on.</p>
            <p>Technically, all of these actions could be performed synchronously (rememberer that word?) by the server executing the initiating action. However, this is often a bad idea, for two reasons. First, the response action (such as emailing all the users who are subscribed for notifications about new stolen TVs) may be very slow. Performing the response action synchronously means that if the initiating action was performed by a user then they will have to wait for all the response actions to finish too. This might not be the end of the world if the response action is small and quick, like sending a single email. But if it’s larger - like searching for and notifying all users who might be interested in a new item - then, well, it still won’t be the end of the world, but it will be a bad user experience.</p>
            <p>To mitigate this problem we have built a “publish-subscribe” or “pubsub” system. When a trigger action is performed, the code that performs it “publishes an event” describing the action, such as <code>NewListingCreated</code> or <code>SubscriptionCardDeclined</code>. The words “event” and “publish” are quite loosely defined in this context, and don’t have a rigorous technical definition. An event is just some sort of record of something that happened, and to publish an event just means that you somehow make a note that something happened. The details of how you do so depend entirely on the pubsub system in question. In a simple system events might be stored in tables in a SQL database, and code would publish events by writing new records to a table.</p>
            <p>If a programmer at Steveslist wants a response action to be performed whenever a new event of a particular type is published, they can write a <em>consumer</em>. This is a piece of code that “subscribes” to a type of event, and which is executed whenever a new event of that type is published. It uses the details of the event (for example, the user whose subscription payment failed) to asynchronously execute whatever actions the programmer wants (for example, sending the user a menacing email).</p>
            <p>Pubsub systems are often managed by a central <em>message broker</em>. Systems publish events to the broker, and the broker is then responsible for getting the events to any subscribed consumers. This can be achieved using either a <em>push</em> or a <em>pull</em> mechanism. Consumers can pull messages from the broker by polling it and repeatedly asking “any new events? any new events?” or they can wait and listen and the broker can push notifications of new events out to them, for example by sending them an HTTP request.</p>
            <div>
            <div>
            <pre><code>1. New user signs|
            up to Steveslist |
            v
            +-----------------+               +------------------------+
            |Steveslist Server|           +--&gt;+SendWelcomeEmailConsumer|
            +---------+-------+           |   +------------------------+
            |                   v
            2. Server reports|     +-------------++  4. Consumers send welcome
            a NewUserSignup  +----&gt;+Pub/Sub Broker|  emails, check for spam,
            event to the           +-------------++  etc.
            Pub/Sub system                       ^
            3. Pub/Sub broker      |   +------------------------+
            notifies consumers     +--&gt;+NewUserSpamCheckConsumer|
            that have subscribed to    +------------------------+
            NewUserSignup events.
            </code>
            </pre></div>
            </div>
            <p>Pubsub systems have many benefits:</p>
            <ul><li>Non-critical actions are performed asynchronously, keeping the user experience snappy</li>
            <li>Our code is kept clean and well-separated. The code that publishes an event doesn’t have to care about what the events’ subscribers do in response</li>
            <li>If a subscriber’s action fails for some reason (for example, the email-sending system has a hiccup), the pubsub system can note this failure and retry again later</li>
            </ul><p>Next, let’s talk big data.</p>
            <h2>Big data and analytics</h2>
            <p>In order to understand and optimize our business, we need to be able to calculate complex statistics across the entire Steveslist platform. How many listings are created every day, segmented by country and city? How many users who signed up each month have created a listing within 90 days of signing up?</p>
            <p>To do this, we need to write database queries that aggregate over the entirety of our data. We don’t want to run these queries against our production SQL database, because they could put an enormous amount of load on it. We don’t want a huge query issued by an internal analyst to be able to bring our production database to a grinding halt, but we do want to provide this analyst with a tool that is well-suited to their needs. To complicate matters further, database engines that are quick at small queries (like returning all the listings belonging to a single user) are typically unacceptably slow (or indeed incapable) of answering giant queries (like calculating the total dollars spent on listings in each category, per day, for the last 90 days).</p>
            <p>Despite this, for the first year or so after Steveslist was founded, we took a risk and simply ran our analytics queries against the main production database. This was a gamble, but it just about worked. We didn’t have much data anyway, and we had more important things to focus on, like attracting the customers who would create the big data that would one day mean that we had to find a more scalable solution.</p>
            <p>Eventually we did bring down the production database with an overly-ambitious query, and decided that the time had come to invest in a data warehouse. A data warehouse is a data store that is well-suited to large, system-wide queries. Our warehouse is powered by a database engine called <em>Hive</em>, but we could also have chosen Presto, Impala, Redshift, or any number of competing alternatives. Hive accepts queries written in SQL, but is much better at executing these queries against giant datasets than our MySQL database is.</p>
            <p>We replicate our data from our production SQL database to Hive, once a day, overnight. Steveslist analysts and programmers can query the same dataset using whichever database engine best suits their needs. They can use the production SQL database for small, precise, source-of-truth queries from production systems; Elasticsearch for full-text search queries; or the data warehouse for huge queries that aggregate over vast volumes of data.</p>
            <hr><p>It’s dark outside and you’re hungry. Is that pretty much it? you ask.</p>
            <p>“Oh god no,” Kate replies, “we could keep going forever. But this is a pretty good start. It’s helpful to have a broad understanding of a wide range of topics, but no one needs to know the details about everything. I find that once you know some basics you can keep picking up more basics and even a couple of details as you go along.”</p>
            <p>You ask if Kate could perhaps continue to elaborate on her five-year vision for Steveslist tomorrow.</p>
            <p>“Absolutely,” says Kate. We’ll talk about more of what goes on inside a real, large online platform, including:”</p>
            <h3>Security</h3>
            <ul><li>HTTPS and other forms of encryption</li>
            <li>Two-factor authentication</li>
            <li>SAML</li>
            <li>Secret key management</li>
            </ul><h3>Server management</h3>
            <ul><li>Deploying new code</li>
            <li>AWS and competitors</li>
            <li>Terraform</li>
            <li>Load balancers</li>
            <li>Containers</li>
            <li>Alerting when code breaks</li>
            </ul><h3>Big data</h3>
            <ul><li>Map-reduce and other big data jobs</li>
            <li>Machine learning</li>
            <li>User tracking</li>
            </ul><p>“See you tomorrow at 7am?”</p>
            </div>"
    },
    {
        title: "How to Think for Yourself",
        author: "Paul Graham",
        url: 'paulgraham.com',
        full_url: 'http://paulgraham.com/think.html', 
        reading_time: '16 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Frobertheaton.com%2Fimages%2Fsystems-cover.png",
        content: 
            "<div lang=\"en\"><p>November 2020</p>
            <p>There are some kinds of work that you can't do well without thinking differently from your peers. To be a successful scientist, for example, it's not enough just to be correct. Your ideas have to be both correct and novel. You can't publish papers saying things other people already know. You need to say things no one else has realized yet.</p>
            <p>The same is true for investors. It's not enough for a public market investor to predict correctly how a company will do. If a lot of other people make the same prediction, the stock price will already reflect it, and there's no room to make money. The only valuable insights are the ones most other investors don't share.</p>
            <p>You see this pattern with startup founders too. You don't want to start a startup to do something that everyone agrees is a good idea, or there will already be other companies doing it. You have to do something that sounds to most other people like a bad idea, but that you know isn't like writing software for a tiny computer used by a few thousand hobbyists, or starting a site to let people rent airbeds on strangers' floors.</p>
            <p>Ditto for essayists. An essay that told people things they already knew would be boring. You have to tell them something <a href=\"http://paulgraham.com/useful.html\"><u>new</u></a>.</p>
            <p>But this pattern isn't universal. In fact, it doesn't hold for most kinds of work. In most kinds of work to be an administrator, for example all you need is the first half. All you need is to be right. It's not essential that everyone else be wrong.</p>
            <p>There's room for a little novelty in most kinds of work, but in practice there's a fairly sharp distinction between the kinds of work where it's essential to be independent-minded, and the kinds where it's not.</p>
            <p>I wish someone had told me about this distinction when I was a kid, because it's one of the most important things to think about when you're deciding what kind of work you want to do. Do you want to do the kind of work where you can only win by thinking differently from everyone else? I suspect most people's unconscious mind will answer that question before their conscious mind has a chance to. I know mine does.</p>
            <p>Independent-mindedness seems to be more a matter of nature than nurture. Which means if you pick the wrong type of work, you're going to be unhappy. If you're naturally independent-minded, you're going to find it frustrating to be a middle manager. And if you're naturally conventional-minded, you're going to be sailing into a headwind if you try to do original research.</p>
            <p>One difficulty here, though, is that people are often mistaken about where they fall on the spectrum from conventional- to independent-minded. Conventional-minded people don't like to think of themselves as conventional-minded. And in any case, it genuinely feels to them as if they make up their own minds about everything. It's just a coincidence that their beliefs are identical to their peers'. And the independent-minded, meanwhile, are often unaware how different their ideas are from conventional ones, at least till they state them publicly. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f1n\">1</a>]</p>
            <p>By the time they reach adulthood, most people know roughly how smart they are (in the narrow sense of ability to solve pre-set problems), because they're constantly being tested and ranked according to it. But schools generally ignore independent-mindedness, except to the extent they try to suppress it. So we don't get anything like the same kind of feedback about how independent-minded we are.</p>
            <p>There may even be a phenomenon like Dunning-Kruger at work, where the most conventional-minded people are confident that they're independent-minded, while the genuinely independent-minded worry they might not be independent-minded enough.</p>
            <p></p>
            ___________<br><p>Can you make yourself more independent-minded? I think so. This quality may be largely inborn, but there seem to be ways to magnify it, or at least not to suppress it.</p>
            <p>One of the most effective techniques is one practiced unintentionally by most nerds: simply to be less aware what conventional beliefs are. It's hard to be a conformist if you don't know what you're supposed to conform to. Though again, it may be that such people already are independent-minded. A conventional-minded person would probably feel anxious not knowing what other people thought, and make more effort to find out.</p>
            <p><span annotation_id=\"707AF767-F321-429B-A2CB-725F37D91C48\" class=\"highlight\">It matters a lot who you surround yourself with. If you're surrounded by conventional-minded people, it will constrain which ideas you can express, and that in turn will constrain which ideas you have. But if you surround yourself with independent-minded people, you'll have the opposite experience: hearing other people say surprising things will encourage you to, and to think of more.</span></p><span annotation_id=\"707AF767-F321-429B-A2CB-725F37D91C48\" class=\"highlight\">
            </span><p>Because the independent-minded find it uncomfortable to be surrounded by conventional-minded people, they tend to self-segregate once they have a chance to. The problem with high school is that they haven't yet had a chance to. Plus high school tends to be an inward-looking little world whose inhabitants lack confidence, both of which magnify the forces of conformism. And so high school is often a <a href=\"http://paulgraham.com/nerds.html\"><u>bad time</u></a> for the independent-minded. But there is some advantage even here: it teaches you what to avoid. If you later find yourself in a situation that makes you think \"this is like high school,\" you know you should get out. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f2n\">2</a>]</p>
            <p>Another place where the independent- and conventional-minded are thrown together is in successful startups. The founders and early employees are almost always independent-minded; otherwise the startup wouldn't be successful. But conventional-minded people greatly outnumber independent-minded ones, so as the company grows, the original spirit of independent-mindedness is inevitably diluted. This causes all kinds of problems besides the obvious one that the company starts to suck. One of the strangest is that the founders find themselves able to speak more freely with founders of other companies than with their own employees. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f3n\">3</a>]</p>
            <p>Fortunately you don't have to spend all your time with independent-minded people. It's enough to have one or two you can talk to regularly. And once you find them, they're usually as eager to talk as you are; they need you too. Although universities no longer have the kind of monopoly they used to have on education, good universities are still an excellent way to meet independent-minded people. Most students will still be conventional-minded, but you'll at least find clumps of independent-minded ones, rather than the near zero you may have found in high school.</p>
            <p>It also works to go in the other direction: as well as cultivating a small collection of independent-minded friends, to try to meet as many different types of people as you can. It will decrease the influence of your immediate peers if you have several other groups of peers. Plus if you're part of several different worlds, you can often import ideas from one to another.</p>
            <p>But by different types of people, I don't mean demographically different. For this technique to work, they have to think differently. So while it's an excellent idea to go and visit other countries, you can probably find people who think differently right around the corner. When I meet someone who knows a lot about something unusual (which includes practically everyone, if you dig deep enough), I try to learn what they know that other people don't. There are almost always surprises here. It's a good way to make conversation when you meet strangers, but I don't do it to make conversation. I really want to know.</p>
            <p>You can expand the source of influences in time as well as space, by reading history. When I read history I do it not just to learn what happened, but to try to get inside the heads of people who lived in the past. How did things look to them? This is hard to do, but worth the effort for the same reason it's worth travelling far to triangulate a point.</p>
            <p>You can also take more explicit measures to prevent yourself from automatically adopting conventional opinions. The most general is to cultivate an attitude of skepticism. When you hear someone say something, stop and ask yourself \"Is that true?\" Don't say it out loud. I'm not suggesting that you impose on everyone who talks to you the burden of proving what they say, but rather that you take upon yourself the burden of evaluating what they say.</p>
            <p>Treat it as a puzzle. You know that some accepted ideas will later turn out to be wrong. See if you can guess which. The end goal is not to find flaws in the things you're told, but to find the new ideas that had been concealed by the broken ones. So this game should be an exciting quest for novelty, not a boring protocol for intellectual hygiene. And you'll be surprised, when you start asking \"Is this true?\", how often the answer is not an immediate yes. If you have any imagination, you're more likely to have too many leads to follow than too few.</p>
            <p>More generally your goal should be not to let anything into your head unexamined, and things don't always enter your head in the form of statements. Some of the most powerful influences are implicit. How do you even notice these? By standing back and watching how other people get their ideas.</p>
            <p>When you stand back at a sufficient distance, you can see ideas spreading through groups of people like waves. The most obvious are in fashion: you notice a few people wearing a certain kind of shirt, and then more and more, until half the people around you are wearing the same shirt. You may not care much what you wear, but there are intellectual fashions too, and you definitely don't want to participate in those. Not just because you want sovereignty over your own thoughts, but because <a href=\"http://paulgraham.com/nov.html\"><u>unfashionable</u></a> ideas are disproportionately likely to lead somewhere interesting. The best place to find undiscovered ideas is where no one else is looking. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f4n\">4</a>]</p>
            <p></p>
            ___________<br><p>To go beyond this general advice, we need to look at the internal structure of independent-mindedness at the individual muscles we need to exercise, as it were. It seems to me that it has three components: fastidiousness about truth, resistance to being told what to think, and curiosity.</p>
            <p>Fastidiousness about truth means more than just not believing things that are false. It means being careful about degree of belief. For most people, degree of belief rushes unexamined toward the extremes: the unlikely becomes impossible, and the probable becomes certain. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f5n\">5</a>] To the independent-minded, this seems unpardonably sloppy. They're willing to have anything in their heads, from highly speculative hypotheses to (apparent) tautologies, but on subjects they care about, everything has to be labelled with a carefully considered degree of belief. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f6n\">6</a>]</p>
            <p>The independent-minded thus have a horror of ideologies, which require one to accept a whole collection of beliefs at once, and to treat them as articles of faith. To an independent-minded person that would seem revolting, just as it would seem to someone fastidious about food to take a bite of a submarine sandwich filled with a large variety of ingredients of indeterminate age and provenance.</p>
            <p>Without this fastidiousness about truth, you can't be truly independent-minded. It's not enough just to have resistance to being told what to think. Those kind of people reject conventional ideas only to replace them with the most random conspiracy theories. And since these conspiracy theories have often been manufactured to capture them, they end up being less independent-minded than ordinary people, because they're subject to a much more exacting master than mere convention. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f7n\">7</a>]</p>
            <p>Can you increase your fastidiousness about truth? I would think so. In my experience, merely thinking about something you're fastidious about causes that fastidiousness to grow. If so, this is one of those rare virtues we can have more of merely by wanting it. And if it's like other forms of fastidiousness, it should also be possible to encourage in children. I certainly got a strong dose of it from my father. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f8n\">8</a>]</p>
            <p>The second component of independent-mindedness, resistance to being told what to think, is the most visible of the three. But even this is often misunderstood. The big mistake people make about it is to think of it as a merely negative quality. The language we use reinforces that idea. You're <i>un</i>conventional. You <i>don't</i> care what other people think. But it's not just a kind of immunity. In the most independent-minded people, the desire not to be told what to think is a positive force. It's not mere skepticism, but an active <a href=\"http://paulgraham.com/gba.html\"><u>delight</u></a> in ideas that subvert the conventional wisdom, the more counterintuitive the better.</p>
            <p>Some of the most novel ideas seemed at the time almost like practical jokes. Think how often your reaction to a novel idea is to laugh. I don't think it's because novel ideas are funny per se, but because novelty and humor share a certain kind of surprisingness. But while not identical, the two are close enough that there is a definite correlation between having a sense of humor and being independent-minded just as there is between being humorless and being conventional-minded. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f9n\">9</a>]</p>
            <p>I don't think we can significantly increase our resistance to being told what to think. It seems the most innate of the three components of independent-mindedness; people who have this quality as adults usually showed all too visible signs of it as children. But if we can't increase our resistance to being told what to think, we can at least shore it up, by surrounding ourselves with other independent-minded people.</p>
            <p>The third component of independent-mindedness, curiosity, may be the most interesting. To the extent that we can give a brief answer to the question of where novel ideas come from, it's curiosity. That's what people are usually feeling before having them.</p>
            <p>In my experience, independent-mindedness and curiosity predict one another perfectly. Everyone I know who's independent-minded is deeply curious, and everyone I know who's conventional-minded isn't. Except, curiously, children. All small children are curious. Perhaps the reason is that even the conventional-minded have to be curious in the beginning, in order to learn what the conventions are. Whereas the independent-minded are the gluttons of curiosity, who keep eating even after they're full. [<a href=\"http://paulgraham.com/think.html?mc_cid=660107d2ee&amp;mc_eid=6c46884444#f10n\">10</a>]</p>
            <p>The three components of independent-mindedness work in concert: fastidiousness about truth and resistance to being told what to think leave space in your brain, and curiosity finds new ideas to fill it.</p>
            <p>Interestingly, the three components can substitute for one another in much the same way muscles can. If you're sufficiently fastidious about truth, you don't need to be as resistant to being told what to think, because fastidiousness alone will create sufficient gaps in your knowledge. And either one can compensate for curiosity, because if you create enough space in your brain, your discomfort at the resulting vacuum will add force to your curiosity. Or curiosity can compensate for them: if you're sufficiently curious, you don't need to clear space in your brain, because the new ideas you discover will push out the conventional ones you acquired by default.</p>
            <p>Because the components of independent-mindedness are so interchangeable, you can have them to varying degrees and still get the same result. So there is not just a single model of independent-mindedness. Some independent-minded people are openly subversive, and others are quietly curious. They all know the secret handshake though.</p>
            <p>Is there a way to cultivate curiosity? To start with, you want to avoid situations that suppress it. How much does the work you're currently doing engage your curiosity? If the answer is \"not much,\" maybe you should change something.</p>
            <p>The most important active step you can take to cultivate your curiosity is probably to seek out the topics that engage it. Few adults are equally curious about everything, and it doesn't seem as if you can choose which topics interest you. So it's up to you to <a href=\"http://paulgraham.com/genius.html\"><u>find</u></a> them. Or invent them, if necessary.</p>
            <p>Another way to increase your curiosity is to indulge it, by investigating things you're interested in. Curiosity is unlike most other appetites in this respect: indulging it tends to increase rather than to sate it. Questions lead to more questions.</p>
            <p>Curiosity seems to be more individual than fastidiousness about truth or resistance to being told what to think. To the degree people have the latter two, they're usually pretty general, whereas different people can be curious about very different things. So perhaps curiosity is the compass here. Perhaps, if your goal is to discover novel ideas, your motto should not be \"do what you love\" so much as \"do what you're curious about.\"</p>
            <p></p>
            <p></p>
            <p></p>
            <p>Notes</p>
            <p>[<a name=\"f1n\">1</a>] One convenient consequence of the fact that no one identifies as conventional-minded is that you can say what you like about conventional-minded people without getting in too much trouble. When I wrote <a href=\"http://paulgraham.com/conformism.html\"><u>\"The Four Quadrants of Conformism\"</u></a> I expected a firestorm of rage from the aggressively conventional-minded, but in fact it was quite muted. They sensed that there was something about the essay that they disliked intensely, but they had a hard time finding a specific passage to pin it on.</p>
            <p>[<a name=\"f2n\">2</a>] When I ask myself what in my life is like high school, the answer is Twitter. It's not just full of conventional-minded people, as anything its size will inevitably be, but subject to violent storms of conventional-mindedness that remind me of descriptions of Jupiter. But while it probably is a net loss to spend time there, it has at least made me think more about the distinction between independent- and conventional-mindedness, which I probably wouldn't have done otherwise.</p>
            <p>[<a name=\"f3n\">3</a>] The decrease in independent-mindedness in growing startups is still an open problem, but there may be solutions.</p>
            <p>Founders can delay the problem by making a conscious effort only to hire independent-minded people. Which of course also has the ancillary benefit that they have better ideas.</p>
            <p>Another possible solution is to create policies that somehow disrupt the force of conformism, much as control rods slow chain reactions, so that the conventional-minded aren't as dangerous. The physical separation of Lockheed's Skunk Works may have had this as a side benefit. Recent examples suggest employee forums like Slack may not be an unmitigated good.</p>
            <p>The most radical solution would be to grow revenues without growing the company. You think hiring that junior PR person will be cheap, compared to a programmer, but what will be the effect on the average level of independent-mindedness in your company? (The growth in staff relative to faculty seems to have had a similar effect on universities.) Perhaps the rule about outsourcing work that's not your \"core competency\" should be augmented by one about outsourcing work done by people who'd ruin your culture as employees.</p>
            <p>Some investment firms already seem to be able to grow revenues without growing the number of employees. Automation plus the ever increasing articulation of the \"tech stack\" suggest this may one day be possible for product companies.</p>
            <p>[<a name=\"f4n\">4</a>] There are intellectual fashions in every field, but their influence varies. One of the reasons politics, for example, tends to be boring is that it's so extremely subject to them. The threshold for having opinions about politics is much <a href=\"http://paulgraham.com/identity.html\"><u>lower</u></a> than the one for having opinions about set theory. So while there are some ideas in politics, in practice they tend to be swamped by waves of intellectual fashion.</p>
            <p>[<a name=\"f5n\">5</a>] The conventional-minded are often fooled by the strength of their opinions into believing that they're independent-minded. But strong convictions are not a sign of independent-mindedness. Rather the opposite.</p>
            <p>[<a name=\"f6n\">6</a>] Fastidiousness about truth doesn't imply that an independent-minded person won't be dishonest, but that he won't be deluded. It's sort of like the definition of a gentleman as someone who is never unintentionally rude.</p>
            <p>[<a name=\"f7n\">7</a>] You see this especially among political extremists. They think themselves nonconformists, but actually they're niche conformists. Their opinions may be different from the average person's, but they are often more influenced by their peers' opinions than the average person's are.</p>
            <p>[<a name=\"f8n\">8</a>] If we broaden the concept of fastidiousness about truth so that it excludes pandering, bogusness, and pomposity as well as falsehood in the strict sense, our model of independent-mindedness can expand further into the arts.</p>
            <p>[<a name=\"f9n\">9</a>] This correlation is far from perfect, though. Gdel and Dirac don't seem to have been very strong in the humor department. But someone who is both \"neurotypical\" and humorless is very likely to be conventional-minded.</p>
            <p>[<a name=\"f10n\">10</a>] Exception: gossip. Almost everyone is curious about gossip.</p>
            <p></p>
            <p><b>Thanks</b> to Trevor Blackwell, Paul Buchheit, Patrick Collison, Jessica Livingston, Robert Morris, Harj Taggar, and Peter Thiel for reading drafts of this.</p>
            <p></p>
            </div>"
    },
    {
        title: "The Ultimate Productivity Hack is Saying No",
        author: "James Clear",
        url: 'jamesclear.com',
        full_url: 'https://jamesclear.com/saying-no', 
        reading_time: '6 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com/684x440/filters:no_upscale()/https%3A%2F%2Fjamesclear.com%2Fwp-content%2Fuploads%2F2015%2F06%2FUntitled-design-1.png",
        content: 
            "<div lang=\"en\">
            <p>The ultimate productivity hack is saying no.</p>
            <p>Not doing something will always be faster than doing it. This statement reminds me of the old computer programming saying, “Remember that there is no code faster than no code.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-1-33502\">1</a></p>
            <p>The same philosophy applies in other areas of life. For example, there is no meeting that goes faster than not having a meeting at all.</p>
            <p>This is not to say you should never attend another meeting, but the truth is that we say yes to many things we don't actually want to do. There are many meetings held that don't need to be held. There is a lot of code written that could be deleted.</p>
            <p>How often do people ask you to do something and you just reply, “Sure thing.” Three days later, you're overwhelmed by how much is on your to-do list. We become frustrated by our obligations even though we were the ones who said yes to them in the first place. <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-2-33502\">2</a></p>
            <p>It's worth asking if things are necessary. Many of them are not, and a simple “no” will be more productive than whatever work the most efficient person can muster.</p>
            <p>But if the benefits of saying no are so obvious, then why do we say yes so often?   <br><span></span></p>
            <h2>Why We Say Yes</h2>
            <p>We agree to many requests not because we want to do them, but because we don't want to be seen as rude, arrogant, or unhelpful. Often, you have to consider saying no to someone you will interact with again in the future—your co-worker, your spouse, your family and friends. <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-3-33502\">3</a></p>
            <p>Saying no to these people can be particularly difficult because we like them and want to support them. (Not to mention, we often need their help too.) Collaborating with others is an important element of life. The thought of straining the relationship outweighs the commitment of our time and energy.</p>
            <p>For this reason, it can be helpful to be gracious in your response. Do whatever favors you can, and be warm-hearted and direct when you have to say no.</p>
            <p>But even after we have accounted for these social considerations, many of us still seem to do a poor job of managing the tradeoff between yes and no. We find ourselves over-committed to things that don't meaningfully improve or support those around us, and certainly don't improve our own lives.</p>
            <p>Perhaps one issue is how we think about the meaning of yes and no.</p>
            <h2>The Difference Between Yes and No</h2>
            <p>The words “yes” and “no” get used in comparison to each other so often that it feels like they carry equal weight in conversation. In reality, they are not just opposite in meaning, but of entirely different magnitudes in commitment.</p>
            <p>When you say no, you are only saying no to <em>one</em> option. When you say yes, you are saying no to <em>every other</em> option.</p>
            <p>I like how the economist Tim Harford put it, “Every time we say yes to a request, we are also saying no to anything else we might accomplish with the time.”<a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-4-33502\">4</a> Once you have committed to something, you have already decided how that future block of time will be spent.</p>
            <p>In other words, saying no saves you time in the future. Saying yes costs you time in the future. No is a form of time credit. You retain the ability to spend your future time however you want. Yes is a form of time debt. You have to pay back your commitment at some point.</p>
            <p>No is a decision. Yes is a responsibility.</p>
            <h2>The Role of No</h2>
            <p>Saying no is sometimes seen as a luxury that only those in power can afford. And it is true: turning down opportunities is easier when you can fall back on the safety net provided by power, money, and authority. But it is also true that saying no is not merely a privilege reserved for the successful among us. It is also a strategy that can help you <em>become</em> successful.</p>
            <p>Saying no is an important skill to develop at any stage of your career because it retains the most important asset in life: your time. As the investor Pedro Sorrentino put it, “If you don’t guard your time, people will steal it from you.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-5-33502\">5</a></p>
            <p>You need to say no to whatever isn't leading you toward your goals. You need to say no to distractions. As one reader told me, “If you broaden the definition as to how you apply no, it actually is the <em>only</em> productivity hack (as you ultimately say no to any distraction in order to be productive).”</p>
            <p>Nobody embodied this idea better than Steve Jobs, who said, “People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-6-33502\">6</a></p>
            <p>There is an important balance to strike here. Saying no doesn't mean you'll never do anything interesting or innovative or spontaneous. It just means that you say yes in a focused way. Once you have knocked out the distractions, it can make sense to say yes to any opportunity that could potentially move you in the right direction. You may have to try many things to discover what works and what you enjoy. This period of exploration can be particularly important at the beginning of a project, job, or career.</p>
            <h2>Upgrading Your No</h2>
            <p>Over time, as you continue to improve and succeed, your strategy needs to change.</p>
            <p>The opportunity cost of your time increases as you become more successful. At first, you just eliminate the obvious distractions and explore the rest. As your skills improve and you learn to separate what works from what doesn't, you have to continually increase your threshold for saying yes.</p>
            <p>You still need to say no to distractions, but you also need to learn to say no to opportunities that were previously good uses of time, so you can make space for great uses of time. It's a good problem to have, but it can be a tough skill to master.</p>
            <p>In other words, you have to upgrade your “no's” over time.</p>
            <p>Upgrading your no doesn't mean you'll never say yes. It just means you default to saying no and only say yes when it <em>really</em> makes sense. To quote the investor Brent Beshore, “Saying no is so powerful because it preserves the opportunity to say yes.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-7-33502\">7</a></p>
            <p>The general trend seems to be something like this: If you can learn to say no to bad distractions, then eventually you'll earn the right to say no to good opportunities.</p>
            <h2>How to Say No</h2>
            <p>Most of us are probably too quick to say yes and too slow to say no. It's worth asking yourself where you fall on that spectrum.</p>
            <p>If you have trouble saying no, you may find the following strategy <a href=\"http://timharford.com/2015/01/the-power-of-saying-no/\">proposed by Tim Harford</a>, the British economist I mentioned earlier, to be helpful. He writes, “One trick is to ask, “If I had to do this today, would I agree to it?” It’s not a bad rule of thumb, since any future commitment, no matter how far away it might be, will eventually become an imminent problem.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-8-33502\">8</a></p>
            <p>If an opportunity is exciting enough to drop whatever you're doing right now, then it's a yes. If it's not, then perhaps you should think twice.</p>
            <p>This is similar to the well-known “Hell Yeah or No” method from Derek Sivers. If someone asks you to do something and your first reaction is “Hell Yeah!”, then do it. If it doesn't excite you, then say no. <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-9-33502\">9</a></p>
            <p>It's impossible to remember to ask yourself these questions each time you face a decision, but it's still a useful exercise to revisit from time to time. Saying no can be difficult, but it is often easier than the alternative. As writer Mike Dariano has pointed out, “It’s easier to avoid commitments than get out of commitments. Saying no keeps you toward the easier end of this spectrum.” <a rel=\"footnote\" href=\"https://jamesclear.com/saying-no#footnote-10-33502\">10</a></p>
            <p>What is true about health is also true about productivity: an ounce of prevention is worth a pound of cure.</p>
            <h2>The Power of No</h2>
            <p>More effort is wasted doing things that don't matter than is wasted doing things inefficiently. And if that is the case, elimination is a more useful skill than optimization.</p>
            <p>I am reminded of the famous Peter Drucker quote, “There is nothing so useless as doing efficiently that which should not be done at all.”</p>
            </div>"
    },
    {
        title: "Deep Inside Taco Bell’s Doritos Locos Taco",
        author: "Austin Carr",
        url: 'fastcompany.com',
        full_url: 'https://www.fastcompany.com/3008346/deep-inside-taco-bells-doritos-locos-taco', 
        reading_time: '13 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-image005-concept-drawing1.jpg",
        content: 
            "<div class='article-content'><h2>From handshake deals to experiments at Home Depot, the history of Taco Bell’s disruptive faux cheese-dusted taco.
            </h2><div><div><aside></aside></div><div><div><span>long Read</span></div><article><div></div><div><p>In early 2009, three years prior to Taco Bell’s 50th anniversary, CEO Greg Creed was already experiencing something of a midlife crisis. “Our target audience is [customers] in their 20s. Turning 50 makes us sound old, and I didn’t want to sound old,” Creed explains. “I said, ‘When we have our birthday, I don’t want a cake or a celebration.'”</p></div><div><div></div></div><div></div><div><p>So he issued a bold directive to his team instead: “I said, ‘[let’s] reinvent the crunchy taco,'” Creed recalls.</p><p>“He was really looking for a big innovation to coincide with our anniversary,” says Taco Bell brand marketing director Stephanie Perdue, who helped Creed write the original team brief. Creed explains: “If you look at all the buns the burger boys sell, and the bread at Subway, they are forever coming up with a new bread bun. The crunchy taco: It was yellow and made of corn. We sold a couple billion of them, but there had been no innovation.” He gave his staff until March 2012–slightly under three years–to pull off a complete rethink of traditional Mexican cuisine.</p><q>If you look at all the buns the burger boys sell, and the bread at Subway, they are forever coming up with a new bread bun.</q><p>The team soon assembled for an all-day ideation session at Taco Bell headquarters, where 30 different product concepts were considered, Perdue says, including new forms of burritos, nachos, and taquitos. But one idea, from Doritos-maker Frito-Lay, stuck out: a Doritos-based taco shell pocketed with Taco Bell ingredients. “It was basically an image [of this taco] on a piece of paper, with a written description. I don’t know what technology they use. We didn’t even taste it; it was just more of, ‘Hey, this is what it could look like,'” Perdue says. “It was like, ‘Holy crap!’ Nobody had ever done this before: turning a Dorito into a taco shell. It was just mind-blowing at the idea stage.” Steve Gomez, Taco Bell’s food innovation expert, recalls seeing the first mock-up. “Every day I see a lot of concepts–sketches on paper, written words about products–and my job is to turn those products into reality,” he says. “But in all my years as a product developer, I’ve never seen a concept like this. The product didn’t even exist yet, and already people knew this idea was going to be huge.”</p><figure><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"The DLT concept illustration\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-image005-concept-drawing1.jpg\">
            <figcaption>The DLT concept illustration</figcaption>
            </figure>
            </div></figure><p>Like any serious renovation, Taco Bell’s started with a trip to Home Depot. It was April 2009. To show executives how the companies could fuse the flavor of Doritos with taco shells, the dev teams “basically went out to Home Depot to buy a paint-spray gun, and then sprayed [Doritos] flavoring onto our existing yellow corn tacos,” recalls Creed, with a chuckle. “It was pretty funny watching people from behind glass spraying our tacos with a paint gun. But it was enough for us to know conceptually that we had a big idea.”</p><p>Creed jokes now that “really smart people got together with a spray gun and the rest is history.” But that glosses over the true story behind the Doritos Locos Taco, as it came to be known. After the company completed a round of concept testing with 200 consumers, which indicated stellar demand for the Doritos Locos Taco, Gomez’s team created an edible version using Taco Bell’s yellow corn shell and Doritos’ Nacho Cheese seasoning. “The way you might put powder sugar on cookies, that’s basically the way we applied this seasoning [to our tacos],” Gomez recalls.</p><figure><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"The first Doritos Locos Taco prototype\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-inline-1-prototype-doritos-loco-taco.jpg\">
            <figcaption>The first Doritos Locos Taco prototype</figcaption>
            </figure>
            </div></figure><p>At last, consumers got a taste. But after all the concept drawings and testing, trips to Home Depot, and prototype development, initial consumer taste tests flopped, to the disappointment of the team. “They completely called us out,” Perdue says. Gomez recalls feeling crushed. “It was total buzzkill in the room,” he says.</p></div><div></div><div><h2><a name=\"Por_Qu\">¿Por Qué?</a></h2><p>Even though the first tests failed to impress sample consumers, Taco Bell had forged the foundation for its biggest innovation in decades: the Doritos Locos Taco–either a revolutionary product or a low point for fast food, depending on your perspective. But there’s no debating the success of the DLT, as it’s called internally. Since it launched in early 2012, Taco Bell has sold more than 450 million Doritos Locos Tacos. A Cool Ranch follow-up, introduced in March, has already sold millions of units. “We had to hire about 15,000 people last year–two to three per restaurant–in order to handle the sales growth and demand of the Doritos Locos Tacos business,” Creed says. Taco Bell is now planning to roll out more Doritos-flavored products, and Frito-Lay has even announced that it will offer Taco Bell-flavored Doritos.</p><q>I mean, it was actually important that we left the orange dusting on your fingers because otherwise, we’re not delivering the genuine Doritos [experience].</q><p>The real tale of how the DLT emerged from disappointing tests to become a massive hit involves a range of challenges and innovations, especially in engineering and manufacturing. “We knew this was a breakthrough idea, so we put on our relentless hats and were determined to not let [this thing] beat us,” Creed says.</p><p>For the first group of testers, the problem was the taste. The combination of Doritos with Taco Bell’s shells didn’t capture either the zest of Doritos chips or the punch of Taco Bell’s tacos. Rather, they formed a displeasing amalgamation of the two flavor profiles. “This idea of merging a chip and a shell together–it sounds simple, but it’s very hard to make a reality,” Gomez says. “To tackle this huge challenge, for months we shared know-how between the technical teams at Frito-Lay and Taco Bell.”</p><h2><a name=\"Think_Outside_The_Bun\">Think Outside The Bun</a></h2><p>The central issue was that Taco Bell’s shells used a different type of corn masa than Doritos chips. But it wasn’t simply a matter of adjusting the recipe. In order to create the DLT, the teams had to consider everything from seasoning mechanics to the taco’s structural integrity throughout 2010 and 2011. “Frito-Lay wanted what’s called a ‘teeth-rattling crunch,’ so they wanted it to snap and crunch more than the current Taco Bell shell snaps and crunches,” Creed says. “So we had to get that formula changed, then we had to find a way to deliver the flavoring, and then the seasoning. I mean, it was actually important that we left the orange dusting on your fingers because otherwise, we’re not delivering the genuine Doritos [experience].”</p><q>There were some [prototypes] where we would barely even touch them and the shell would break.</q><p>All in, the teams experimented with more than 40 recipes over two years. During that time, the teams faced several roadblocks. “Remember, a taco shell has to bend, so we had to make this crispy [like a chip], but we also had to make it be able to bend so it didn’t crack,” Creed says. “This was a really big engineering challenge, [especially considering] we would have to make hundreds of millions of these shells.”</p><p>“When you buy a bag of Doritos and you open it, and some of the corners are broken off, you’re probably not going to be that mad, because they’re still Doritos,” Gomez says. “But if our taco shells are broken in transit or in the restaurants, we can’t do anything with them. That was a big obstacle for us. How do we make these shells chip-like, but also be able to ship them and still be able to build a taco without having them break? There were some [prototypes] where we would barely even touch them and the shell would break.”</p></div><div></div><div><figure><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-inline-2-deep-inside-doritos-loco-taco.jpg\">
            </figure>
            </div>   <br></figure><p>Seasoning was another major issue, which was as much of a challenge of taste as it was for manufacturing. “A Doritos chip is a flat triangle, and it gets seasoned by being tumbled around in a huge seasoner barrel that rotates. But we couldn’t do that with a taco shell because they would break. We could get the seasoning to stick to the top but we couldn’t get it to stick to the bottom–we just couldn’t get it evenly coated,” Creed recalls. “You don’t want to take one big bite at one end, and it has flavor and the other end has nothing. We had to make sure it was evenly distributed.”</p><p>“That was the really tricky part,” Gomez says. “Just like anything in manufacturing, it’s all about speed and efficiency. So our seasoner had to season shells fast and it had to season them right every single time. We had teams of engineers working day and night to get the seasoner working.”</p><q>In the facilities, we couldn’t have all that stuff in the air. It would’ve been too much seasoning and flavor for our workers.</q><p>In fact, the companies ended up creating a proprietary seasoner in the process, not least because for workers on the manufacturing line, the plumes of Doritos seasoning would create an almost Nacho Cheese gas chamber. “We realized pretty quickly that we had to seal that all in, because in the facilities, we couldn’t have all that stuff in the air,” Creed says. “It would’ve been too much seasoning and flavor for our workers. We had to enclose it so the seasoning wouldn’t escape. It would’ve been overpowering.” [<em>Eds. note: Not a bad way to go.</em>]</p><p>Gomez admits to losing faith at times. “There was a tremendous amount of pressure. There were days when we would get bummed out and worried that this would be too hard to pull off,” he says. But eventually his team refined and narrowed down the recipes to three prototypes, which were readied for market testing. Creed says “there’s a little bit of black box magic” that went into the final prototype of the DLT, “which not many people have seen.” He won’t elaborate, but regardless of the secret recipe, the true beauty of the DLT is its deceptively basic concept.</p><p>“It’s really one of those breathtakingly simple but huge ideas,” Creed says. “I remember trying to sell guacamole in the Midwest and people were like, ‘What’s all this green stuff in my burrito?’ But this was a fastball, down the middle. It’s what you’d expect from Taco Bell, but supercharged.”</p><h2><a name=\"I_Would_Walk_900_Miles\">I Would Walk 900 Miles</a></h2><p>By September 2011, Taco Bell had rolled out the DLT prototype at a handful of restaurants in a few cities around the country for testing. Soon, hype around the DLT spread like lukewarm baja sauce. Customers began blogging about their experience; a slew of video reviews hit YouTube; and one Taco Bell addict even drove 900 miles from New York to Toledo, OH for an early taste of the DLT. “They were just fanatical, and the results were off the charts,” Stephanie Perdue says. “I’ve never seen so much word of mouth generated for one single product.”</p></div><div></div><div><figure><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"The Doritos Loco Taco prototype, left, next to the finalized shell\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-inline-add-1-deep-inside-taco-bells-doritos-locos-taco.jpg\">
            <figcaption>The Doritos Loco Taco prototype, left, next to the finalized shell</figcaption>
            </figure>
            </div></figure><p>But while buzz for the DLT’s national launch was locked in, a deal between Taco Bell and Frito-Lay was not. As Taco Bell legend has it, though the companies had spent years working together on the DLT, no official contracts had ever been signed. Taco Bell’s 50th birthday was fast approaching when Greg Creed and then-Frito-Lay CEO Al Carey met in Creed’s office to hash out final details. “We both realized that if we let the lawyers get involved, this thing would get slowed down and bogged down. So we did a handshake deal–that’s all we had: You’re going to spend the money, and I’m going to spend the money [on the DLT],” Creed recalls. “Everyone was like, ‘You can’t launch without a contract.’ And we were like, ‘Just watch us.'”</p><p>Legal wrangling aside, meeting demand for the DLT was another problem for Taco Bell. “We were going from three cities to 6,000 restaurants,” Perdue says. “We didn’t have the manufacturing capacity.” Adds Gomez, “How do we stockpile millions of shells to be ready for a launch of this magnitude?”</p><p>Initially, the company thought its suppliers could build two dedicated DLT lines to produce enough shells to meet demand. “We built a pilot plant, but then we sold so many that we were like, ‘Whoops, we got a problem,'” Creed says. Taco Bell’s suppliers ended up building six dedicated manufacturing lines for the DLT, which are handled by roughly 600 employees. In fact, the Cool Ranch version of the DLT was originally supposed to launch late last year, instead of in March 2013, but it was delayed to add more manufacturing lines. “We realized we needed more capacity, because we couldn’t slow down the Nacho Cheese line in order to create capacity for Cool Ranch,” Creed says.</p><p>When the DLT went nationwide, it was a smash hit, with millions of tacos sold in the first week on the market alone. “When it launched, there was a significant impact–Taco Bell’s [sales] numbers were up 13% in the second quarter of last year, which is big,” explains Morgan Stanley analyst John Glass. “Brands like Taco Bell or McDonald’s or Wendy’s–you just don’t see double-digit [growth] in same-store sales.”</p><q>It’s not just a product; it’s now a platform–Nacho Cheese, Cool Ranch, Flamas.</q><p>Soon, a contract was written up between Taco Bell and Frito-Lay, according to Creed.</p><p>“When we met in my office [before launch], we said that if either one of us gets sacked or promoted, we would actually have to write a contract,” Creed recalls. “When [then-Frito-Lay CEO] Al [Carey] got promoted to run the PepsiCo beverage business, I phoned him up and said, ‘So I guess we better write that contract then.’ Well guess what? We sold 100 million tacos in the first 70 days. If we waited for those contracts to be finished, we would’ve sold 100 million less.”</p></div><div></div><div><h2><a name=\"Flamas_Future\">Flamas Future</a></h2><p>After Nacho Cheese and Cool Ranch, a spicy Doritos Flamas-flavored taco is next on the docket. But that’s not all the company has in the works. “Someone said to me, ‘Well, when you launch three or four of these things you’ll run out of ideas,'” Creed says. “I’m like, ‘Not really.'”</p><p>To Creed, the partnership between Taco Bell and Frito-Lay is more than a one-off collaboration. Like Android is to Google or iOS is to Apple, Doritos-based flavors represent a whole new framework for Taco Bell to build on. “It’s not just a product; it’s now a platform–Nacho Cheese, Cool Ranch, Flamas,” Creed beams. “We’re going to blow everyone away in the next few years in terms of how big this idea and platform will become.”</p><p>The company is now considering crowdsourcing the next iteration of the DLT, and with 123 flavors of Doritos, there’s certainly no shortage of possibilities. But perhaps more compelling than the company’s innovations in taco shells are its plans to rethink what goes inside the DLT. Creed gives <em>Fast Company</em> the scoop: “At the moment, all the ingredients we have inside these shells are the same. The shells keep changing, but the ingredients are still ground beef, sour cream, lettuce, tomatoes,” he says. “Well, we got a load of other flavors at Taco Bell, and we could put a whole lot of different ingredients inside those shells that would create a different taste and sensation.”</p><q>We’re going to blow everyone away in the next few years in terms of how big this idea and platform will become.</q><p>When pressed, Creed would only say it could be different “proteins or a special sauce.” Additionally, customers themselves are already experimenting with their own reinventions of the DLT. “The Cheesy Gordita Crunch people are asking us to make one with either a Nacho Cheese or Cool Ranch shell,” Creed says. That’s likely to come down the road.</p><figure><div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fimages.fastcompany.net%2Fimage%2Fupload%2Fw_596%2Cc_limit%2Cq_auto%3Abest%2Cf_auto%2Ffc%2F3008346-inline-inline-1-deep-inside-doritos-loco-taco.jpg\">
            </figure>
            </div>   <br></figure><p>John Glass, the Morgan Stanley analyst, imagines there are more Doritos-based products on the horizon, but questions whether they will be as successful as the first Doritos Locos Taco. “I don’t know how many variants there will be, but I think it’s the type of platform that could come back year after year,” he says. “It doesn’t just have to be a taco. There’s already a burrito with Fritos in it. Could you do Doritos nachos? A Doritos taco salad? Maybe. Creative guys will come up with different ways to use the same idea in ways we hadn’t thought of before.”</p><p>However complex the DLT line eventually becomes, the innovation behind the idea is its elegant simplicity, according to Creed. The combination of Doritos and Taco Bell, he says, is just so universally appealing that it’s feeding its success in the market and driving its place in pop culture.</p></div><div></div><div><p>“It’s like sliced bread,” Creed explains. “If you like bread, why wouldn’t you want it sliced?”</p><p>[<em>Images Courtesy of Taco Bell</em>]</p></div></article></div></div></div>"
    },
    {
        title: "Why Do Dogs Love Us? Science Explains",
        author: "Rae Paoletta",
        url: 'getpocket.com',
        full_url: 'https://getpocket.com/explore/item/why-do-dogs-love-us-science-explains', 
        reading_time: '2 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5ec55a2c6e64d.jpg",
        content: 
            "<div class='article-content'>
            <div id=\"RIL_header\">
            <h1>Why Do Dogs Love Us? Science Explains</h1>
            <cite>by <a href=\"\" class=\"RIL_author\">Rae Paoletta</a>, <a href=\"http://getpocket.com\">getpocket.com</a></cite>
            </div>
            <div id=\"RIL_body\">
            <div id=\"RIL_less\">
            <div lang=\"en\"><div><h2>Dogs really do love their owners—here’s the research to back it up.</h2><article><article><div><div><section><div><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5ec55a2c6e64d.jpg\">
            </figure>
            </div></div><p>
            You’re not just imagining it: There’s substantial research to support the claim that <a href=\"https://www.inverse.com/article/42055-why-dogs-like-when-we-talk-to-them-like-babies-science\">dogs truly adore</a> their owners. An animal behaviorist confirms to <em>Inverse</em> there are all sorts of chemical goodness going on in puppies’ brains when they’re around us. It’s even purer than you think.
            </p><p>
            While we don’t know exactly how long ago humans started domesticating dogs, <a rel=\"noreferrer\" href=\"http://www.cell.com/current-biology/abstract/S0960-9822(15)00432-7\">some scientists</a> think our friendship could go as far back as 40,000 years. Dogs have continued to grow alongside humanity, from helping us hunt mammoths to chasing after sticks. We’ve loved them all along the way, and apparently, the feeling is mutual.
            </p><p>
            “Of course dogs love their people!” animal behavior consultant <a rel=\"noreferrer\" href=\"https://twitter.com/amyshojai\">Amy Shojai</a> tells <em>Inverse</em>. “The hormone oxytocin is released (in both dogs <em>and</em> people) when they interact/have contact with someone they like. This ‘love hormone’ helps cement and increase the bond we share … it’s also the hormone that floods the system of new moms to amp up attachment to new babies.”
            </p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5ec55a6255846.png\">
            </figure>
            </div></div><p>
            Just the scent of their person is enough to make a dog happy. A 2015 study <a rel=\"noreferrer\" href=\"https://www.sciencedirect.com/science/article/pii/S0376635714000473\">published</a> in the journal <em>Behavioural Processes</em> found that dogs connect their owner’s unique smell to pleasure. By utilizing functional magnetic resonance imaging (fMRI) scans — which measure brain nerve cell levels — the researchers got an inside look at how dogs responded to their humans’ scent versus familiar dogs, unfamiliar dogs, and unfamiliar people. The team found that when the pups smelled their owners, it activated a reward center in their brain called the caudate nucleus. They didn’t react the same to any other scent.
            </p><p>
            We also know that dogs respond positively when we talk to them in that ridiculous high-pitched voice we <em>all</em> do. As <em>Inverse</em> previously <a rel=\"noreferrer\" href=\"https://link.springer.com/article/10.1007/s10071-018-1172-4\">reported</a>, researchers at the University of York <a rel=\"noreferrer\" href=\"https://link.springer.com/article/10.1007/s10071-018-1172-4\">recently found</a> that dogs respond more positively to dog-directed speech (DDS) than when we talk to them like people.
            </p><p>
            Scientists had 37 dogs listen to people talking to them in “dog-speak” — that high-pitched voice, coupled with “dog-relevant” phrases (e.g. “Who’s a good dog? You are!”). Participants would then talk to dogs in a flat tone about ordinary things (e.g. “So, I went to the movies last night”). The dogs overwhelmingly preferred dog-speak, which the researchers compared to the way people talk to babies.
            </p><p>
            We may never understand all the mysteries swirling in our puppies’ minds. But we do know one thing for sure: Dogs are good, and we’re better humans because of them.
            </p><p>
            Here’s one more picture of a dog. You deserve it.
            </p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5ec55a921152f.jpg\">
            </figure>
            </div></div><p><i>Rae Paoletta is the Senior Editor at Inverse, overseeing the space science vertical. She previously worked at Gizmodo as a space writer and launched astronomy coverage at MTV News and Revelist. She unapologetically shills for Saturn and cats.</i></p></section></div></div></article></article></div></div>
            </div>
            </div>
            </div>"
    },
    {
        title: "Top 10 Design Flaws in the Human Body",
        author: "Chip Rowe",
        url: 'getpocket.com',
        full_url: 'https://getpocket.com/explore/item/top-10-design-flaws-in-the-human-body', 
        reading_time: '7 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttp%253A%252F%252Fstatic.nautil.us%252F6102_7bec7e63a493e2d61891b1e4051ef75a.png",
        content: 
            "<div class='article-content'><p><span>T</span>he Greeks were obsessed with the mathematically perfect body. But unfortunately for anyone chasing that ideal, we were designed not by Pygmalion, the mythical sculptor who carved a flawless woman, but by MacGyver. Evolution constructed our bodies with the biological equivalent of duct tape and lumber scraps. And the only way to refine the form (short of an asteroid strike or nuclear detonation to wipe clean the slate) is to jerry-rig the current model. “Evolution doesn’t produce perfection,” explains Alan Mann, a physical anthropologist at Princeton University. “It produces function.”</p><p>With that in mind, I surveyed anatomists and biologists to compile a punch list for the human body, just as you’d do before buying a house. Get out your checkbook. This one’s a fixer-upper.</p><p><b>1. An unsound spine</b></p><p><b>Problem:</b> Our spines are a mess. It’s a wonder we can even walk, says Bruce Latimer, director of the Center for Human Origins at Case Western Reserve University, in Cleveland. When our ancestors walked on all fours, their spines arched, like a bow, to withstand the weight of the organs suspended below. But then we stood up. That threw the system out of whack by 90 degrees, and the spine was forced to become a column. Next, to allow for bipedalism, it curved forward at the lower back. And to keep the head in balance—so that we didn’t all walk around as if doing the limbo—the upper spine curved in the opposite direction. This change put tremendous pressure on the lower vertebrae, sticking about 80 percent of adults, according to one estimate, with lower back pain.</p><p><b>Fix:</b> Go back to the arch. “Think of your dog,” Latimer says. “From the sacrum to the neck, it’s a single bow curve. That’s a great system.” Simple. Strong. Pain-free. There’s only one catch: To keep the weight of our heads from pitching us forward, we’d need to return to all fours.</p><p><b>2. An inflexible knee</b></p><p><b>Problem:</b> As Latimer says, “You take the most complex joint in the body and put it between two huge levers—the femur and the tibia—and you’re looking for trouble.” The upshot is your knee only rotates in two directions: forward and back. “That’s why every major sport, except maybe rugby, makes it illegal to clip, or hit an opponent’s knee from the side.”</p><p><b>Fix:</b> Replace this hinge with a ball and socket, like in your shoulders and hips. We never developed this type of joint at the knee “because we didn’t need it,” Latimer says. “We didn’t know about football.”</p><figure><div><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttp%253A%252F%252Fstatic.nautil.us%252F6108_c09b1eadea0efc7914f73ac698494b5e.png\">
            </figure>
            </div></div></figure><p><b>3. A too-narrow pelvis</b></p><p><b>Problem:</b> Childbirth hurts. And to add insult to injury, the width of a woman’s pelvis hasn’t changed for some 200,000 years, keeping our brains from growing larger.</p><p><b>Fix:</b> Sure, you could stretch out the pelvis, Latimer says, but technologists may already be onto a better solution. “I would bet that in 10,000 years, or even in 1,000 years, no woman in the developed world will deliver naturally. A clinic will combine the sperm and egg, and you’ll come by and pick up the kid.”</p><p><b>4. Exposed testicles</b></p><p><b>Problem:</b> A man’s life-giving organs hang vulnerably outside the body.</p><p><b>Fix:</b> Moving the testicles indoors would save men the pain of getting hit in the nuts. To accomplish this, first you’d need to tweak the sperm, says Gordon Gallup, an evolutionary psychologist at the State University of New York at Albany. Apparently the testicles (unlike the ovaries) get thrown out in the cold because sperm must be kept at 2.5 to 3 degrees Fahrenheit below the body’s internal temperature. Gallup hypothesizes that these lower temperatures keep sperm relatively inactive until they enter the warm confines of a vagina, at which point they go racing off to fertilize the egg.<sup>1</sup> This evolutionary hack prevents sperm from wearing themselves out too early. So change the algorithm, Gallup says. Keep the sperm at body temperature and make the vagina hotter. (And, by the way, there’s no need to draw up new blueprints: Elephants offer a pretty good prototype.)</p><p><b>5. Crowded teeth</b></p><p><b>Problem:</b> Humans typically have three molars on each side of the upper and lower jaws near the back of the mouth. When our brain drastically expanded in size, the jaw grew wider and shorter, leaving no room for the third, farthest back molars. These cusped grinders may have been useful before we learned to cook and process food. But now the “wisdom teeth” mostly just get painfully impacted in the gums.</p><p><b>Fix:</b> Get rid of them. At one point, they appeared to be on their way out—about 25 percent of people today (most commonly Eskimos) are born without some or all of their third molars. In the meantime, we’ve figured out how to safely extract these teeth with dental tools, which, Mann notes, we probably wouldn’t have invented without the bigger brains. So you could call it a wash.</p><p><b>6. Meandering arteries</b></p><p><b>Problem:</b> Blood flows into each of your arms and legs via one main artery, which enters the limb on the front side of the body, by the biceps or hip flexors. To supply blood to tissues at a limb’s back side, such as the triceps and hamstrings, the artery branches out, taking circuitous routes around bones and bundling itself with nerves. This roundabout plumbing can make for some rather annoying glitches. At the elbow, for instance, an artery branch meets up with the ulnar nerve, which animates your little finger, just under the skin. That’s why your arm goes numb when the lower tip of your upper arm bone, called the humerus or “funny bone,” takes a sharp blow.</p><p><b>The Fix:</b> Feed a second artery into the back side of each arm and leg, by the shoulder blades or buttock, says Rui Diogo, an assistant professor of anatomy at Howard University, in Washington, DC, who studies the evolution of primate muscles. This extra pipe would provide a more direct route from the shoulder to the back of the hand, preventing vessels and nerves from wandering too close to the skin.</p><p><b>7. A backward retina</b></p><p><b>Problem:</b> The photoreceptor cells in the retina of the eye are like microphones facing backward, <a href=\"http://thehumanevolutionblog.com/2015/01/12/the-poor-design-of-the-human-eye/\">writes Nathan Lents</a>, an associate professor of molecular biology at the City University of New York. This design forces light to travel the length of each cell, as well as through blood and tissue, to reach the equivalent of a receiver on the cell’s backside. The setup may encourage the retina to detach from its supporting tissue—a leading cause of blindness. It also creates a blind spot where cell fibers, akin to microphone cables, converge at the optic nerve—making the brain refill the hole.</p><p><b>Fix:</b> Poach the obvious solution from the octopus or the squid: Just flip the retina.&nbsp;</p><figure><div><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttp%253A%252F%252Fstatic.nautil.us%252F6105_c1d53b7a97707b5cd1815c8d228d8ef1.png\">
            </figure>
            </div></div></figure><p><b>8. A misrouted nerve</b></p><p><b>Problem:</b> The recurrent laryngeal nerve (RLN) plays a vital role in our ability to speak and swallow. It feeds instructions from the brain to the muscles of the voice box, or larynx, below the vocal cords. Theoretically, the trip should be a quick one. But during fetal development, the RLN gets entwined in a tiny lump of tissue in the neck, which descends to become blood vessels near the heart. That drop causes the nerve to loop around the aorta before traveling back up the larynx. Having this nerve in your chest makes it vulnerable during surgery—or a fist fight.&nbsp;</p><p><b>Fix:</b> “This one’s easy,” says Rebecca Z. German, a professor of anatomy and neurobiology at Northeast Ohio Medical University, in Rootstown. While a baby is in utero, develop the RCN <i>after</i> sending that irksome neck lump of vessel tissue to the chest. That way, the nerve won’t get dragged down with it.</p><p><b>9. A misplaced voice box</b></p><p><b>Problem:</b> The trachea (windpipe) and esophagus (food pipe) open into the same space, the pharynx, which extends from the nose and mouth to the larynx (voice box). To keep food out of the trachea, a leaf-shaped flap called the epiglottis reflexively covers the opening to the larynx whenever you swallow. But sometimes, the epiglottis isn’t fast enough. If you’re talking and laughing while eating, food may slip down and get lodged in your airway, causing you to choke.</p><p><b>Fix:</b> Take a cue from whales, whose larynx is located in their blowholes. If we moved the larynx into our nose, says German, we could have two independent tubes. Sure, we’d lose the ability to talk. But we could still communicate in song, as whales do, through vibrations in our nostrils.</p><p><b>10. A klugey brain</b></p><p><b>Problem:</b> The human brain evolved in stages. As new additions were being built, older parts had to remain online to keep us up and running, explains psychologist Gary Marcus in his book <i>Kluge: The Haphazard Evolution of the Mind</i>.<sup>2</sup> And that live-in construction project led to slapdash workarounds. It’s as if the brain were a dysfunctional workplace, where young employees (the forebrain) handled newfangled technologies like language while the old guard (the midbrain and hindbrain) oversaw the institutional memory—and the fuse box in the basement. A few outcomes: depression, madness, unreliable memories, and confirmation bias.</p><p><b>Fix:</b> We’re screwed.</p><figure><div><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttp%253A%252F%252Fstatic.nautil.us%252F6107_29586cb449c90e249f1f09a0a4ee245a.png\">
            </figure>
            </div></div></figure><p><i>Chip Rowe is a writer based in New York.</i><br></p><p>References</p><p>1. Gallup, G.G., Finn, M.M., &amp; Sammis, B. On the origin of descended scrotal testicles: The activation hypothesis. <i>Evolutionary Psychology</i> <b>7</b>, 517-526 (2009).</p><p>2. &nbsp;Marcus, G. <i>Kluge: The Haphazard Evolution of the Human Mind</i> Houghton Mifflin, Boston, MA (2008).</p></div>"
    },
    {
        title: "Universal Quantum Phenomenon Found in Strange Metals",
        author: "Natalie Wolchover",
        url: 'getpocket.com',
        full_url: 'https://getpocket.com/explore/item/universal-quantum-phenomenon-found-in-strange-metals', 
        reading_time: '8 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f75004014952.png",
        content: 
            "<div class='article-content'><h2>Experiments suggest that exotic superconducting materials share a “strange metal” state characterized by a quantum speed limit that somehow acts as a fundamental organizing principle.</h2><article><article><div><div><section><div><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"Strange metals appear to dissipate energy as fast as the laws of quantum mechanics allow. Credit: Boris Vaudran for Quanta Magazine.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f75004014952.png\">
            <figcaption>Strange metals appear to dissipate energy as fast as the laws of quantum mechanics allow. Credit: Boris Vaudran for Quanta Magazine.</figcaption>
            </figure>
            </div></div><p>
                            A ubiquitous quantum phenomenon has been detected in a large class of superconducting materials, fueling a growing belief among physicists that an unknown organizing principle governs the collective behavior of particles and determines how they spread energy and information. Understanding this organizing principle could be a key into “quantum strangeness at its deepest level,” said <a href=\"http://sachdev.physics.harvard.edu/\">Subir Sachdev</a>, a theorist at Harvard University who was not involved with the new experiments.
            </p><p>
                            The <a href=\"https://www.nature.com/articles/s41567-018-0334-2\">findings</a>, reported in 2018 in <em>Nature Physics</em>&nbsp;by a team working at the University of Sherbrooke in Canada and the National Laboratory for Intense Magnetic Fields (LNCMI) in France, indicate that electrons inside a variety of ceramic crystals called “cuprates” seem to dissipate energy as quickly as possible, apparently bumping up against a fundamental quantum speed limit. And past studies, especially <a href=\"http://science.sciencemag.org/content/339/6121/804\">a 2013 paper in <em>Science</em></a>, found that other exotic superconducting compounds — strontium ruthenates, pnictides, tetramethyltetrathiafulvalenes and more — also burn energy at what appears to be a maximum allowed rate.
            </p><p>
                            Strikingly, this speed limit is linked to the numerical value of Planck’s constant, the fundamental quantity of quantum mechanics representing the smallest possible action that can be taken in nature.
            </p><p>
                            “When you see that, you know you’re touching on something very, very deep and fundamental,” said <a href=\"https://www.physique.usherbrooke.ca/taillefergroup/index_eng.html\">Louis Taillefer</a>, a condensed matter physicist at Sherbrooke, who conducted the new cuprate experiment with his graduate student <a href=\"https://www.physique.usherbrooke.ca/pages/en/node/8918\">Anaëlle Legros</a>, <a href=\"http://www.toulouse.lncmi.cnrs.fr/spip.php?article328&amp;lang=en\">Cyril Proust</a> of LNCMI, and 13 collaborators.
            </p><p>
                            This energy-burning behavior occurs when the cuprates and other exotic compounds are in a “strange metal” phase, in which they resist the flow of electricity more than conventional metals.&nbsp;But when they’re cooled to a critical temperature, these strange metals transform into perfect, lossless conductors of electricity. Physicists have been struggling for 32 years to understand and control this powerful form of superconductivity, and the behavior of electrons in the preceding strange-metal phase is increasingly seen as a key part of the story.
            </p><p>
                            “It’s really a major mystery,” said Sachdev, a leader in the field of <a href=\"https://www.quantamagazine.org/tag/condensed-matter-physics/\">condensed matter physics</a>.
            </p><p>
                            Exactly what electrons, the carriers of electricity, are doing in strange metals isn’t known. But experts hypothesize that they may be organizing themselves into a “maximally scrambled” quantum state, in which the properties of each electron depend on those of every other. This state of maximum scrambling might allow the electrons to scatter off one another and spread energy as quickly as the laws of quantum mechanics permit.
            </p><p>
                            This scrambled state is quantum strangeness in the extreme, Sachdev said. In the 1930s, Albert Einstein bristled at the idea of two particles becoming entangled, with properties that stay interdependent even after the particles have traveled far apart. “Here we have entanglement of millions of electrons leading to a whole state of matter,” Sachdev said, “so we are really exploring the frontier of entanglement.”
            </p><p>
                            An organizing principle could be a way in.
            </p><p>
                            “The experiments point to a tantalizing universality across materials, one that would involve a deep idea in quantum mechanics and statistical mechanics,” said <a href=\"https://sitp.stanford.edu/people/sean-hartnoll\">Sean Hartnoll</a>, a theoretical physicist at Stanford University.&nbsp;The effort to pinpoint that deep idea has turned up surprising connections to black holes, gravity and quantum information theory.
            </p><h2>Strange Metals</h2><p>
                            In 1986, when Georg Bednorz and Alex Müller of IBM Research Zurich synthesized the first cuprate and <a href=\"https://link.springer.com/article/10.1007%2FBF01303701\">discovered</a> what’s known as “high-temperature superconductivity,” they noticed something strange about their revolutionary new crystal. As the duo cooled down their cuprate — this one made of lanthanum, barium, copper and oxygen atoms — toward its critical temperature, they observed that the crystal’s electrical resistance decreased linearly with the falling temperature, so that when plotted it formed a downward-trending straight line. For conventional materials, this relationship forms a more complicated curve.
            </p><p>
                            At the time, this observation was overshadowed by the more dramatic result. Bednorz and Müller’s discovery of superconductivity at a higher critical temperature than was previously thought possible quickly won them the physics Nobel Prize and set off a fevered search for similar materials. “It was a pretty mad time,” said <a href=\"http://physics.berkeley.edu/people/faculty/Joseph-Orenstein\">Joseph Orenstein</a>, a physicist who was then at Bell Labs in New Jersey. “The place went crazy.”
            </p><p>
                            Other labs soon discovered cuprates and other compounds that superconducted at even higher temperatures. Since then, physicists have dreamed of finding or synthesizing materials that superconduct electricity all the way up to room temperature. Such materials could make human electrical infrastructure vastly more efficient and could power magnetically levitating vehicles, revolutionizing the way we live.
            </p><p>
                            But to create higher-temperature superconductors, physicists had to strengthen the glue that binds electrons together, allowing the electrons to effortlessly convey electric charge. The problem was, the researchers first had to figure out what that glue is. Theories proliferated, but the striking complexity of cuprates and other high-temperature superconductors confounded every attempt.
            </p><p>
                            Over time, one part of the fuzzy picture came into focus: The mysterious linear resistivity that Bednorz and Müller observed in their first cuprate kept showing up in other cuprates and materials before the onset of superconductivity. This behavior became associated with the strange-metal phase that seems to underlie superconductivity in some way. The phase not only transitions to superconductivity at a critical temperature, but persists at lower temperatures if magnetic fields are used to destroy the superconducting state. The superconducting and strange-metal phases appear to compete, with the critical temperature acting as the tipping point between them. To dial up the critical temperature, physicists need to understand both phases. “We probably won’t understand why the superconducting temperature in cuprates is high until we understand the strange-metal phase out of which the superconductivity emerges,” Hartnoll said.
            </p><p>
                            The straight line indicated the existence of “a beautiful, simple, robust law,” said Taillefer. “There has to be a simple, deep theoretical explanation.”
            </p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f7500869cdd5.gif\">
            </figure>
            </div></div><p>
                            Starting in 1990, researchers began finding evidence of a quantum nature to the linear resistivity. That year, Orenstein and his colleagues at Bell Labs <a href=\"https://journals.aps.org/prb/abstract/10.1103/PhysRevB.42.6342\">studied a cuprate</a> called yttrium barium copper oxide and found that, like Bednorz and Müller’s sample, its electrical resistance dropped linearly as it was cooled toward its critical temperature. By using an alternating current, they were able to measure the rate at which electrons in the material scatter off each other, which is the source of resistance. They discovered that the new straight line representing the electron scattering rate as a function of temperature had a slope strikingly close to the fundamental constant <em>ħ</em>&nbsp;(pronounced “h-bar”), called the reduced Planck’s constant<em>.</em> In quantum mechanics, <em>ħ</em> represents, among other things, the smallest possible action, which is an amount of energy multiplied by an amount of time.
            </p><p>
                            “At that time I thought it was interesting,” said Orenstein, who is now a professor at the University of California, Berkeley, and a senior scientist at Lawrence Berkeley National Laboratory, “but I didn’t realize that 30 years later it would still be a completely unexplained mystery that was being related to black holes and information theory.”
            </p><p>
                            The 2013 <em>Science</em> paper and today’s <em>Nature Physics</em> findings show that the slope of the line relating electron scattering rate to temperature in strange metals is invariably the same: <em>ħ</em>.
            </p><h2>The Quantum Speed Limit</h2><p>
                            In 2004, the Dutch theorist Jan Zaanen gave this curious phenomenon a name: Planckian dissipation. He argued in <a href=\"https://www.nature.com/articles/430512a\">a <em>Nature</em> News &amp; Views article</a> that electrons in these materials, and in other exotic states of matter sometimes referred to as “quantum soup,” are all reaching a fundamental quantum speed limit on how fast they can dissipate energy.
            </p><p>
                            “If you’re on a freeway and all the cars are going at the same speed, it’s not because their engines are identical; it’s just because there’s a speed limit,” Hartnoll said.
            </p><div><div><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"The German physicist Max Planck, who originated quantum theory in 1900 by discovering that energy is quantized in discrete packets. Photo from Wikimedia Commons / Public Domain.\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f7500cc83943.jpg\">
            <figcaption>The German physicist Max Planck, who originated quantum theory in 1900 by discovering that energy is quantized in discrete packets. Photo from Wikimedia Commons / Public Domain.</figcaption>
            </figure>
            </div></div></div><p>
                            To understand why electrons in strange metals push up against the putative speed limit, theorists want to figure out where it comes from. The best argument traces the speed limit to the uncertainty principle, the famous formula introduced by Werner Heisenberg in 1927 that puts an upper limit on the amount of certainty that you can have about the world — or, equivalently, on the amount of definiteness the world itself possesses. This upper limit is determined by <em>ħ</em>.
            </p><p>
                            Conceived and approximated by Max Planck in 1900 and later put in reduced form by Paul Dirac, <em>ħ</em> shows up all over quantum theory. Its extremely small value, now known with high precision, represents the quantum unit of action, but in addition, as Heisenberg showed, <em>ħ</em> is the quantum unit of uncertainty: an inescapable, base-level fuzziness in nature. The fuzziness appears when you try measuring two things at once: the position and momentum of a particle, for instance, or how much energy it possesses and for how long. In other words, position and momentum can’t both be defined to greater accuracy than <em>ħ</em>; nor can energy and time. The better you know one, the less certain the other.
            </p><p>
                            The hypothesis is that electrons in strange metals might be “dissipating as quickly as they can consistent with the uncertainty principle,” Hartnoll explained. The electrons possess an amount of energy that’s proportional to the temperature of the strange metal, and dissipation is a process that takes a certain amount of time. Time and energy can’t both be defined to arbitrary precision because of the uncertainty principle, Hartnoll said, so it’s possible that Planckian dissipation arises “when the dissipation time is as fast as it can be.”
            </p><p>
                            It’s only a rough sketch, he admits. He and other theorists want to prove the quantum bound more rigorously, which might help clarify why hordes of electrons in materials like cuprates so naturally reach it.
            </p><p>
                            For the last few years, Hartnoll, Sachdev and other theorists have been attacking the problem using a surprising <a href=\"https://www.quantamagazine.org/albert-einstein-holograms-and-quantum-gravity-20181114/\">“holographic duality”</a> that mathematically connects systems of scrambled quantum particles, like those in strange metals, to imaginary black holes in one higher dimension. (The black hole pops out of the particle system like a hologram.) Remarkably, physicists find that black holes — incredibly dense, spherical objects whose gravity is so strong that not even light can escape — do the equivalent of Planckian dissipation, reaching a bound on how fast they can possibly scramble information that falls into them. In other words, black holes and strange metals go to extremes in some common way. The holographic duality is enabling the researchers to translate properties of black holes into dual properties of the scrambled-particle systems. They hope this will reveal what electrons are doing in strange metals, what happens in the competing superconducting phase, and potentially how to tip the balance between the two, extending superconductivity to higher temperatures.
            </p><p>
                            As they study the behavior of scrambled electrons using the holographic duality and other methods, researchers are gaining a sense of progress and partial insight. Some feel that the field is on a cusp of a conceptual breakthrough. &nbsp;Hartnoll said of the Planckian dissipation phenomenon, “I think it may be understood soon.”
            </p><p><i>Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences.</i></p></section></div></div></article></article></div>"
    },
    {
        title: "10 Works of Literary Fantasy You Should Read",
        author: "Emily Temple",
        url: 'lithub.com',
        full_url: 'https://lithub.com/10-works-of-literary-fantasy-you-should-read/', 
        reading_time: '5 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttps%253A%252F%252Flithub.com%252Fwp-content%252Fuploads%252F2019%252F01%252F9780241315545.jpg",
        content: 
            "<div class='article-content'><h2>Even if you're a genre snob (but also, if you're a genre snob, stop).</h2><article><article><div><div><section><div><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"--\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-image-cache.com%2Fdirect%3Fresize%3Dw2000%26url%3Dhttps%253A%252F%252Flithub.com%252Fwp-content%252Fuploads%252F2019%252F01%252F9780241315545.jpg\">
            <figcaption>--</figcaption>
            </figure>
            </div></div><p><span>Today marks the release of one of the most anticipated books of 2019: Marlon James’s&nbsp;<a rel=\"noopener noreferrer\" href=\"https://bookmarks.reviews/reviews/all/black-leopard-red-wolf/\"><em>Black Leopard, Red Wolf</em></a>, a sprawling literary fantasy and the first in a projected series. James is one of our best and most interesting contemporary writers, and I suggest that you read his latest—as well as any number of other works of literary fantasy, some of which I will recommend below.</span></p><p><span>But first, what do I mean by “literary fantasy”? There are probably as many definitions for this term (and most genre terms, which are all watery at best) as there are readers, but for the purposes of this list, I am using it to mean works of fantasy that prioritize sentence-level craft and/or complex thematic structures, and/or that play with expectations and fantasy tropes, and/or that focus on characters and interiority as primary goals of the work. I don’t just mean “well-written fantasy” or “literary novels that have magic in them,” though both kinds of books can be found here. What I mean is books that relate to and pull from the conventions of both genres: fantasy and literary fiction. This means there might be dragons, and there might be a hero’s journey, and there might be some lyrical descriptions, and there might be some family conflict. There is also some crossover with SF and literary SF, of course, but I will try my best not to conflate the two.</span></p><p><span>So with all that said, here are a few wonderful works of literary fantasy, that I recommend to all lovers of fantasy and all lovers of literary fiction. (NB that this is of course just a start—feel free to mention more books and writers below.) If these are uncertain waters for you, well, throw off your genre goggles and try something new! You won’t be sorry.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc952757c447.jpg\">
            </figure>
            </div></div><h2>Marlon James, <a href=\"https://bookmarks.reviews/reviews/black-leopard-red-wolf/\">Black Leopard, Red Wolf</a><i></i></h2><p><span>In his newest novel, James has created a surreal imagined Africa, which <a rel=\"noopener noreferrer\" href=\"https://www.nytimes.com/2019/01/31/books/review/black-leopard-red-wolf-marlon-james.html\">Michiko Kakutani called</a> “the literary equivalent of a Marvel Comics universe—filled with dizzying, magpie references to old movies and recent TV, ancient myths and classic comic books, and fused into something new and startling by his gifts for language and sheer inventiveness. . . . [It]&nbsp;feels like a place mapped by Gabriel García Márquez and Hieronymus Bosch with an assist from Salvador Dalí.” It is an epic quest with monsters and mayhem at every turn—but also a complex literary landscape second to none.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc9528c58ed9.jpg\">
            </figure>
            </div></div><h2>Chandler Klang Smith, <a href=\"https://bookmarks.reviews/reviews/the-sky-is-yours/\">The Sky is Yours</a></h2><p><span>Another recent doorstop that I loved: this crazy adventure from Chandler Klang Smith, in which two aging dragons circle a destroyed city, a late capitalist heir bucks tradition, and a rich girl with too many teeth in her mouth finds herself queen of the criminals.&nbsp;Plus, you have wild, ludicrous, wonderful language, references to&nbsp;<em>Infinite Jest</em>, madcap adventure, and all the characters you can handle. It’s an absolute treat.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc952a66e865.png\">
            </figure>
            </div></div><h2>Rachel Ingalls, <a href=\"https://bookmarks.reviews/reviews/mrs-caliban/\">Mrs. Caliban</a></h2><p><span>If you read this website at all, you probably already know that <a rel=\"noopener noreferrer\" href=\"https://lithub.com/hot-sex-with-sea-monsters-a-comparative-study/\">we</a> <a rel=\"noopener noreferrer\" href=\"https://lithub.com/what-we-loved-this-week-7-27-2018/\">all</a> <a rel=\"noopener noreferrer\" href=\"https://lithub.com/daniel-handler-on-the-best-writer-you-dont-know-rachel-ingalls/\">love</a> <a rel=\"noopener noreferrer\" href=\"https://lithub.com/this-thing-of-darkness-an-interview-with-rachel-ingalls/\">Rachel Ingalls</a> and <a rel=\"noopener noreferrer\" href=\"https://lithub.com/mrs-caliban/\"><em>Mrs. Caliban</em></a>. Otherwise, imagine if, a few pages into&nbsp;<em>Revolutionary Road</em>, an enormous, froglike monster appeared and he and April fell in love. I know: it would really improve things.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc952bd7d730.jpg\">
            </figure>
            </div></div><h2>Naomi Novik, <a href=\"https://bookmarks.reviews/reviews/spinning-silver/\">Spinning Silver</a></h2><p><span>Technically, this is a reimagining of the Rumpelstiltskin story, but it’s so much more than that: it’s about the daughter of a moneylender who takes over the family business, and who is so good at it that she catches the eye of a faerie lord who wants eternal winter. It’s also about a woman forced to marry a demon, and what she does about. It’s also about mothers and daughters and religion and honor. It’s probably the closest thing to straight fantasy on this list, and it might be YA, but the deft language and complexity of theme make this a literary fantasy knockout for me.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_6\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc952d207030.jpeg\">
            </figure>
            </div></div><h2>Kelly Link, <a href=\"https://bookmarks.reviews/reviews/magic-for-beginners/\">Magic for Beginners</a></h2><p><span>Kelly Link is the queen of literary fantasy—or, I suppose, literary-horror-SF-fable-surrealist-speculative-fantasy, because she never chooses, and she never has to. It’s all great.&nbsp;<em>Magic for Beginners&nbsp;</em>is probably still my favorite collection of hers, but consider this entry as a gentle suggestion that you read her entire oeuvre, which is replete with faerie handbags, magical television shows, vampires, new Boyfriends, haunted toothbrushes and other glories.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_7\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc952fb27d48.jpg\">
            </figure>
            </div></div><h2>Kazuo Ishiguro, <a href=\"https://bookmarks.reviews/reviews/the-buried-giant/\">The Buried Giant</a></h2><p><span>Let’s face it: anything Ishiguro writes is going to be literary—the man just won the Nobel Prize. But this novel, his foray into Arthurian fantasy—complete with dragons, knights, pixies, and Sir Gawain the Green Knight—was pretty controversial: some loved it, and some thought he had really gone off the rails. Happily, I love it when established authors go off the rails, and this time is no exception. This is a novel about memory, about family, but more importantly, as <a rel=\"noopener noreferrer\" href=\"https://www.nytimes.com/2015/03/01/books/review/kazuo-ishiguros-the-buried-giant.html\">Neil Gaiman wrote</a>, it “does what important books do: It remains in the mind long after it has been read, refusing to leave, forcing one to turn it over and over. On a second reading, and on a third, its characters and events and motives are easier to understand, but even so, it guards its secrets and its world close.”</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_8\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc9531005c6b.jpg\">
            </figure>
            </div></div><h2>N. K. Jemisin, The Fifth Season</h2><p><span>Here’s another book that I am constantly on about (so I’m told): N. K. Jemisin’s stupendous&nbsp;<em>The Fifth Season</em>, and <a rel=\"noopener noreferrer\" href=\"https://bookmarks.reviews/reviews/the-obelisk-gate/\">its</a> <a rel=\"noopener noreferrer\" href=\"https://bookmarks.reviews/reviews/the-stone-sky/\">sequels</a>. It has everything that a traditional fantasy novel would have, and then some: deep character development and interiority, intense consideration of the world and what humans do to it, elevated language. I barely came up for air.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_9\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc953251489f.jpg\">
            </figure>
            </div></div><h2>Susanna Clarke, Jonathan Strange and Mr. Norrell</h2><p><span>You may or may not have just scrolled through this list looking for&nbsp;<em>Jonathan Strange and Mr. Norrell</em>. If so, hi, and I don’t blame you: this is a highly literary work, an alternative history that takes style cues from Dickens and the Brontës but also delves deep into the practice and problems of magic. Very satisfying, no matter your interests.</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_10\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc9533847bd5.jpg\">
            </figure>
            </div></div><h2>George Saunders, Lincoln in the Bardo</h2><p><span>It’s highly experimental, and certainly part of the project is its form and language—but it also takes place in purgatory and most of the characters who aren’t Abraham Lincoln are ghosts, and some of them are silly and some of them are sad. Saunders’ first novel really defies generic description, but it’s not horror, and it’s not science fiction, and it’s not really speculative, so I’m including it here. Why not?</span></p><div><div class=\"RIL_IMG\" id=\"RIL_IMG_11\">
            <figure>
            <img alt=\"--\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5fc95366170c9.jpg\">
            <figcaption>--</figcaption>
            </figure>
            </div></div><h2>Virginia Woolf, Orlando: A Biography<br></h2><p><span>Genre-bending and gender-bending, clever and punchy and strange, internal and magical and entirely Woolf, this classic love letter is the ur-text of literary fantasy.</span></p></section></div></div></article></article></div>”
                        </p><p><i>Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences.</i></p></section></div></div></article></article></div>"
    },
    {
        title: "The Minecraft Generation",
        author: "Clive Thompson",
        url: 'nytimes.com',
        full_url: 'https://www.nytimes.com/2016/04/17/magazine/the-minecraft-generation.html', 
        reading_time: '31 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstatic01.nyt.com%2Fimages%2F2016%2F04%2F17%2Fmagazine%2F17mag-minecraft-1%2F17mag-minecraft-1-articleLarge.gif%3Fquality%3D75%26auto%3Dwebp%26disable%3Dupscale",
        content: 
            "<div class='article-content'>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstatic01.nyt.com%2Fimages%2F2016%2F04%2F17%2Fmagazine%2F17mag-minecraft-1%2F17mag-minecraft-1-articleLarge.gif%3Fquality%3D75%26auto%3Dwebp%26disable%3Dupscale\">
            </figure>
            </div><section name=\"articleBody\"><div><div><p><span>J</span>ordan wanted to build an unpredictable trap.</p><p>An 11-year-old in dark horn-&shy;rimmed glasses, Jordan is a devotee of Minecraft, the computer game in which you make things out of virtual blocks, from dizzying towers to entire cities. He recently read “The Maze Runner,” a sci-fi thriller in which teenagers live inside a booby-&shy;trapped labyrinth, and was inspired to concoct his own version — something he then would challenge his friends to navigate.</p><p>Jordan built a variety of obstacles, including a deluge of water and walls that collapsed inward, Indiana Jones-style. But what he really wanted was a trap that behaved unpredictably. That would really throw his friends off guard. How to do it, though? He obsessed over the problem.</p><p>Then it hit him: the animals! Minecraft contains a menagerie of virtual creatures, some of which players can kill and eat (or tame, if they want pets). One, a red-and-white cowlike critter called a mooshroom, is known for moseying about aimlessly. Jordan realized he could harness the animal’s movement to produce randomness. He built a pen out of gray stones and installed “pressure plates” on the floor that triggered a trap inside the maze. He stuck the mooshroom inside, where it would totter on and off the plates in an irregular pattern.</p><p>Presto: Jordan had used the cow’s weird behavior to create, in effect, a random-&shy;number generator inside Minecraft. It was an ingenious bit of problem-&shy;solving, something most computer engineers I know would regard as a great hack — a way of coaxing a computer system to do something new and clever.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>When I visited Jordan at his home in New Jersey, he sat in his family’s living room at dusk, lit by a glowing iMac screen, and mused on Minecraft’s appeal. “It’s like the earth, the world, and you’re the creator of it,” he said. On-screen, he steered us over to the entrance to the maze, and I peered in at the contraptions chugging away. “My art teacher always says, ‘No games are creative, except for the people who create them.’ But she said, ‘The only exception that I have for that is Minecraft.’&thinsp;” He floated over to the maze’s exit, where he had posted a sign for the survivors: <em>The journey matters more than what you get in the end.</em></p><p>Since its release seven years ago, Minecraft has become a global sensation, captivating a generation of children. There are over 100 million registered players, and it’s now the third-best-&shy;selling video game in history, after Tetris and Wii Sports. In 2014, Microsoft bought Minecraft — and Mojang, the Swedish game studio behind it — for $2.5 billion.</p><p>There have been blockbuster games before, of course. But as Jordan’s experience suggests — and as parents peering over their children’s shoulders sense — Minecraft is a different sort of phenomenon.</p><p>For one thing, it doesn’t really feel like a <em>game</em>. It’s more like a destination, a technical tool, a cultural scene, or all three put together: a place where kids engineer complex machines, shoot videos of their escapades that they post on YouTube, make art and set up servers, online versions of the game where they can hang out with friends. It’s a world of trial and error and constant discovery, stuffed with byzantine secrets, obscure text commands and hidden recipes. And it runs completely counter to most modern computing trends. Where companies like Apple and Microsoft and Google want our computers to be easy to manipulate — designing point-and-click interfaces under the assumption that it’s best to conceal from the average user how the computer works — Minecraft encourages kids to get under the hood, break things, fix them and turn mooshrooms into random-&shy;number generators. It invites them to tinker.</p></div><aside aria-label=\"companion column\"></aside></div><div><div></div><div><figure role=\"group\" aria-label=\"media\"><div></div><figcaption><span><span>Credit...</span><span>Illustration by Christoph Niemann</span></span></figcaption></figure></div></div><div><div><p>In this way, Minecraft culture is a throwback to the heady early days of the digital age. In the late ’70s and ’80s, the arrival of personal computers like the Commodore 64 gave rise to the first generation of kids fluent in computation. They learned to program in Basic, to write software that they swapped excitedly with their peers. It was a playful renaissance that eerily parallels the embrace of Minecraft by today’s youth. As Ian Bogost, a game designer and professor of media studies at Georgia Tech, puts it, Minecraft may well be this generation’s personal computer.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>At a time when even the president is urging kids to learn to code, Minecraft has become a stealth gateway to the fundamentals, and the pleasures, of computer science. Those kids of the ’70s and ’80s grew up to become the architects of our modern digital world, with all its allures and perils. What will the Minecraft generation become?</p><p><strong>“Children,” the social</strong> critic Walter Benjamin wrote in 1924, “are particularly fond of haunting any site where things are being visibly worked on. They are irresistibly drawn by the detritus generated by building, gardening, housework, tailoring or carpentry.”</p><p>Playing with blocks, it turns out, has deep cultural roots in Europe. Colin Fanning, a curatorial fellow at the Philadelphia Museum of Art, points out that European philosophers have long promoted block-&shy;based games as a form of “good” play that cultivates abstract thought. A recent paper Fanning wrote with Rebecca Mir traces the tradition to the English political philosopher John Locke, who was an early advocate of alphabet blocks. A century later, Friedrich Froebel — often called the inventor of kindergarten — developed block-&shy;based toys that he claimed would illustrate the spiritual connectedness of all things. Children would start with simple blocks, build up to more complex patterns, then begin to see these patterns in the world around them. Educators like Maria Montessori picked up on this concept and pioneered the teaching of math through wooden devices.</p><p>During the political cataclysms of the 20th century, European thinkers regarded construction-&shy;play not merely as a way to educate children but also as a means to heal their souls. The Danish landscape architect Carl Theodor Sorensen urged that areas in cities ruined by World War II be turned into “junk playgrounds,” where children would be given pickaxes, hammers and saws and allowed to shape the detritus into a new civilization, at child scale. (Several were in fact created in Europe and were quite popular.) In Sweden, educators worried that industrialization and the mechanization of society were causing children to lose touch with physical skills; they began teaching <em>sloyd</em>, or woodcrafting, a practice that continues today.</p><p>When Fanning first saw Minecraft, he felt a jolt of recognition. Nearly all these historical &shy;impulses were evident in the game. “It’s striking to me how much this mirrors the appeal and the critical reception of Minecraft,” he says. “In Scandinavian toys, the material of wood has had a really long association with notions of timelessness and quality and craftsmanship.” In Minecraft, as he notes, wood is one of the first resources new players gather upon entering the game: chopping trees with their avatar’s hand produces blocks of wood, and from those they begin to build a civilization. Children are turned loose with tools to transform a hostile environment into something they can live in.</p><p>Block-play was, in the European tradition, regarded as a particularly “wholesome” activ&shy;ity; it’s not hard to draw a line from that to many parents’ belief that Minecraft is the “good” computer game in a world full of anxiety about too much “screen time.” In this way, Minecraft has succeeded Lego as the respectable creative toy. When it was first sold in the postwar period, Lego presented itself as the heir to the heritage of playing with blocks. (One ad read: “It’s a pleasure to see children playing with Lego — Lego play is quiet and stimulating. Children learn to grapple with major tasks and solve them together.”) Today many cultural observers argue that Lego has moved away from that open-&shy;ended engagement, because it’s so often sold in branded kits: <a rel=\"noopener noreferrer\" title=\"\" href=\"http://lego.brickinstructions.com/m/lego_instructions/set/4842/Hogwarts_Castle_\">the Hogwarts castle</a> from “Harry Potter,” <a rel=\"noopener noreferrer\" title=\"\" href=\"http://lego.brickinstructions.com/lego_instructions/set/7146/TIE_Fighter\">the TIE fighter</a> from “Star Wars.”</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>“It’s ‘Buy the box, open the box, turn to the instruction sheet, make the model, stick it on the shelf, buy the next box,’&thinsp;” the veteran &shy;game designer Peter Molyneux says in a 2012 documentary about Minecraft. “Lego used to be just a big box of bricks, and you used to take the bricks, pour them on the carpet and then make stuff. And that’s exactly what Minecraft is.”</p><p>As a Swede, Markus Persson, who invented Minecraft and founded Mojang, grew up amid such cultural influences and probably encountered <em>sloyd</em> in school himself. In Minecraft, Persson created what Fanning calls “a sort of digital <em>sloyd</em>.”</p></div><aside aria-label=\"companion column\"></aside></div><div><div></div></div><div><div><p>Persson, now 36, was a child of the ’80s computer scene who learned to program when he was 7 on his father’s Commodore 128. By the time he was in his 20s, he was working for an online photo-&shy;album site and programming games in his spare time at home, an apartment littered with game CDs and soda bottles. He released the first version of Minecraft in 2009. The basic play is fairly simple: Each time you start a new game, Minecraft generates a unique world filled with hills, forests and lakes. Whatever the player chops at or digs into yields building blocks — trees provide wood, the earth dirt and stone. Blocks can be attached to one another to quickly produce structures. Players can also combine blocks to “craft” new items. Take some stone blocks, add a few pieces of wood, and you make a pickax, which then helps you dig more quickly and deeper, till you reach precious materials like gold, silver and diamond. “Mobs,” the game’s creatures (“mob” is short for “mobile”), can be used for crafting, too. Kill a spider, and you get spider silk, handy for making bows and arrows.</p><p>In its first year, Minecraft found popularity mostly among adult nerds. But sometime in late 2011, according to Alex Leavitt, a Ph.D. candidate at the University of Southern California, children discovered it, and sales of the game exploded. Today it costs $27 and sells 10,000 copies a day. (It’s still popular across all age groups; according to Microsoft, the average player is between 28 and 29, and women make up nearly 40 percent of all players.) Persson frequently added new features to the game, like a “survival mode,” in which every 20 minutes evening falls and monsters attack — skeletons shooting arrows, “creepers” blowing themselves up when they get close to you — forcing players to build protective shelters. (“Creative mode” is just about making things.)</p><p>Persson also made it possible for players to share their works. You could package your world as a “map” and post it online for others to download and move around in. Even more sophisticated players could modify Minecraft’s code, creating new types of blocks and creatures, and then put these “mods” online for others to use. Further developments included a server version of Minecraft that lets people play together on the Internet inside the same world. These days, kids can pay as little as $5 a month to rent such a server. They can also visit much larger commercial servers capable of hosting hundreds or thousands of players simultaneously. There is no single, central server: Thousands exist worldwide.</p><p>The game was a hit. But Persson became unsettled by his fame, as well as the incessant demands of his increasingly impassioned fans — who barraged him with emails, tweets and forum posts, imploring him to add new elements to Minecraft, or complaining when he updated the game and changed something. By 2014, he’d had enough. After selling Minecraft to Microsoft, he hunkered down in a $70 million mansion in Beverly Hills and now refuses to talk about Minecraft any more.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>I wanted to know whether the European tradition of block-play had influenced him, but Persson politely declined to be interviewed. Via a public reply to me on Twitter, he explained that he “sold Minecraft to get away from it.”</p><p><strong>Nearly everyone</strong> who plays Minecraft, or even watches someone else do so, remarks on its feeling of freedom: All those blocks, infinities of them! Build anything you want! Players have re&shy;created the Taj Mahal, the U.S.S. Enterprise from “Star Trek,” the entire capital city from “Game of Thrones.” It’s the most obvious appeal of the game. But I first started to glimpse how complex Minecraft culture can be when I saw what kids were doing with what’s called “redstone,” the game’s virtual wiring. My two sons had begun using it: Zev, who is 8, showed me an automated “piston door” and stone gateway he built. Gabriel, who is 10, had created a “minigame” whose actions included a mechanism that dropped anvils from a height, which players on the ground had to dodge.</p><p>Redstone transports energy between blocks, like an electrical connection. Attach a block that contains power — a redstone “torch,” for example, which looks like a forearm-size matchstick — to one end of a trail of redstone, and anything connected to the other end will receive power. Hit a button <em>here</em>, and another block shifts position over <em>there</em>. Persson ingeniously designed redstone in a way that mimics real-world electronics. Switches and buttons and levers turn the redstone on and off, enabling players to build what computer scientists call “logic gates.” Place two Minecraft switches next to each other, connect them to redstone and suddenly you have what’s known as an “AND” gate: If Switch 1 and Switch 2 are both thrown, energy flows through the redstone wire. You can also rig an “OR” gate, whereby flipping either lever energizes the wire.</p><p>These AND and OR gates are, in virtual form, the same as the circuitry you’d find inside a computer chip. They’re also like the Boolean logic that programmers employ every day in their code. Together, these simple gates let Minecraft players construct machines of astonishing complexity.</p><p>One day this winter, I met Sebastian, a 14-year-old, at his home in New Jersey, where he showed off his redstone devices. One was a huge “trading post,” a contraption that allows players on either side of a large wall to trade items through an automated chute. It required a large cluster of AND gates, he said, and took him several days to figure out.</p></div><aside aria-label=\"companion column\"></aside></div><figure role=\"group\" aria-label=\"media\"><div><span>Video</span><div><div><div><div><div></div><div></div></div></div><div><div><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fstatic01.nyt.com%2Fimages%2F2016%2F04%2F13%2Fmultimedia%2F17mag-minecraft-final%2F17mag-minecraft-final-videoSixteenByNine1050.jpg\">
            </figure>
            </div></div><div><div><div></div><div></div></div></div></div></div></div></div><figcaption><span>There’s no better way to understand Minecraft than to get into the game and start exploring. Christoph Niemann, our visual columnist, worked with Hypixel, a team of professional Minecraft tinkerers based in London, to build a Minecraft world just for The New York Times Magazine. To play, you’ll need a computer with Minecraft and a child who’s familiar with the game. Once you have those things, just log on to the nytmag.hypixel.net server (your child will know what this means). If you don’t have Minecraft, instructions for how to get it are at the bottom of this article.</span></figcaption></figure><div><div><p>“Hop down here,” he said, moving down into a subterranean pit beneath the apparatus and looking around. (In Minecraft, you see the world from the viewpoint of your in-game avatar.) It was like being in the bowels of a factory: the redstone sprawled in all directions. He pointed out different parts of the wiring, rattling off components like an architect at a construction site. “Coming in from these two wires are the lever inputs from the side — and from over here, the other side. And what these do is, when they’re both on, they power a piston, which pairs redstone to this block up into this tower dispenser.”</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>Mastering redstone requires rigorously logical thinking, as well as a great deal of debugging: When your device isn’t working, you have to carefully go over its circuitry to figure out what’s wrong. One fifth &shy;grader I visited, Natalie, was assembling a redstone door on her iPad while I watched. But nothing happened when she flicked the “on” lever. “I did that wrong,” she said with a frown, and began tracing her way through the circuit. Eventually the problem emerged: A piece of redstone was angled incorrectly, sending the current in the wrong direction.</p><p>This is what computer scientists call computational thinking, and it turns out to be one of Minecraft’s powerful, if subtle, effects. The game encourages kids to regard logic and if-then statements as fun things to mess around with. It teaches them what computer coders know and wrestle with every day, which is that programs rarely function at first: The work isn’t so much in writing a piece of software but in debugging it, figuring out what you did wrong and coming up with a fix.</p><p>Minecraft is thus an almost perfect game for our current educational moment, in which policy makers are eager to increase kids’ interest in the “STEM” disciplines — science, technology, engineering and math. Schools and governments have spent millions on “let’s get kids coding” initiatives, yet it may well be that Minecraft’s impact will be greater. This is particularly striking given that the game was not designed with any educational purpose in mind. “We have never done things with that sort of intent,” says Jens Bergensten, the lead Minecraft developer at Mojang and Persson’s first hire. “We always made the game for ourselves.”</p><p>Other Minecraft features resemble the work of software engineers even more closely. For example, programmers frequently write code and control their computers through a bare-bones interface known as the “command line,” typing abstruse, text-&shy;based commands rather than pointing and clicking. Many programmers I know complain that while the point-and-click world has made computers easier to use for everyday people, it has also dumbed us down; kids don’t learn the command line the way they would have back when personal-&shy;computer use emerged in the ’70s and ’80s. This is partly why newcomers can find programming alienating: They’re not accustomed to controlling a computer using only text.</p><p>But Minecraft, rather audaciously, includes a command line and requires players to figure it out. Type “t” or “/” while playing the game, and a space appears where you can chat with other players or issue commands that alter the environment. For example, typing “/time set 0” instantly changes the time of day inside the game to daybreak; the sun suddenly appears on the horizon. Complex commands require a player to master chains of sophisticated command-&shy;line syntax.</p><p>One day last fall, I visited Gus, a seventh &shy;grader in Brooklyn. He was online with friends on a server they share together, engaging in boisterous gladiatorial combat. I watched as he typed a command to endow himself with a better weapon: “/give AdventureNerd bow 1 0 {Unbreakable:1,ench:[{id:51,lvl:1}],display:{Name:“Destiny”}}.” What the command did was give a bow-&shy;and-&shy;arrow weapon to AdventureNerd, Gus’s avatar; make the bow unbreakable; endow it with magic; and name the weapon Destiny, displayed in a tag floating over the weapon. Gus had plastered virtual sticky-&shy;notes all over his Mac’s desktop listing the text commands he uses most often. Several commands can be packed into a “command block,” so that clicking on the block activates them, much as clicking on a piece of software launches it.</p><p>Mimi Ito, a cultural anthropologist at the University of California, Irvine, and a &shy;founder of Connected Camps, an online program where kids play Minecraft together, has closely studied gamers and learning. Ito points out that when kids delve into this hackerlike side of the game — concocting redstone devices or creating command blocks — they often wind up consulting discussion forums online, where they get advice from adult Minecraft players. These folks are often full-time programmers who love the game, and so younger kids and teenagers wind up in conversation with professionals.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>“It’s one of the places where young people are engaging with more expert people who are much older than them,” Ito says. These connections are transformative: Kids get a glimpse of a professional path that their schoolwork never illuminates. “An adult mentor opens up these new worlds that wouldn’t be open to them,” she adds. Of course, critics might worry about kids interacting with adults online in this way, but as Ito notes, when there’s a productive task at hand, it’s similar to how guilds have passed on knowledge for ages: knowledgeable adults mentoring young people.</p><div><div></div><div><figure role=\"group\" aria-label=\"media\"><div><div><div></div></div></div><figcaption><span><span>Credit...</span><span>Illustration by Christoph Niemann</span></span></figcaption></figure></div></div><p>Ito has also found that kids’ impulse to tinker with Minecraft pushes them to master real-world technical skills. One 15-year-old boy I interviewed, Eli, became interested in making “texture packs.” These are the external shells that wrap around 3-D objects in the game, like a drape thrown over a table: Change the pattern on the drape, and you can change what the object looks like. Designing texture packs prompted Eli to develop sophisticated Photoshop skills. He would talk to other texture-&shy;pack designers on Minecraft forums and get them to send him their Photoshop files so he could see how they did things. He also began teaching himself to draw. “I’d be downloading the mod,” he says, “looking at the original texture and saying, ‘O.K., how can I make this a little more cartoony?’&thinsp;” Then he would put his own designs up on the forums to get feedback, which, he discovered, was usually very polite and constructive. “The community,” he says, “is very helpful.”</p><p>While Minecraft rewards this sort of involvement, it can also be frustrating: Mojang updates Minecraft weekly, and sometimes new updates aren’t compatible with an older version. Players complained to me about waking up to discover that their complex contraptions no longer worked. One player spent weeks assembling a giant roller coaster whose carts were powered by redstone tracks only to have an update change the way rails functioned, and the entire roller-&shy;coaster mechanism never worked again. Others ruefully described spending months crafting cities on their own multiplayer servers, only to have a server crash and destroy everything.</p><p>For Ito, this is all a culturally useful part of the experience: Kids become more resilient, both practically and philosophically. “Minecraft is busted, and you’re constantly fixing it,” she says. “It’s that home-brew aesthetic. It’s kind of broken all the time. It’s laggy. The kids get used to the idea that it’s broken and you have to mess with it. You’re not complaining to get the corporate overlord to fix it — you just have to fix it yourself.” This is a useful corrective to other software. “IPhone apps are kind of at the opposite end,” Ito says. “And the way that kids react when things are broken in the Apple ecosystem versus the Minecraft ecosystem is totally different. With [Apple] it’s, ‘Why are they broken?’ Whereas with Minecraft it’s like — ‘Oh, they messed with something again, it’s broken, we have to go figure out what they changed.’ There’s a sort of resignation to that the fact that you’re tinkering all the time.”</p><p>Because Minecraft is now seven years old, Ian Bogost will soon have students at Georgia Tech who grew up playing the game. The prospect intrigues him. “I’m very curious to see what their attitude to technology is,” he says.</p><p><strong>Two years ago</strong>, Ava, a fifth grader who lives on Long Island, whom I met through her aunt, a friend of mine, tried Minecraft for the first time. She started a “survival” world and marveled at the jagged hills receding into the distance. But like most new players, she had no idea what to do. Night fell, mobs arrived and a skeleton staggered toward her. She mistakenly assumed it was friendly. “I was like, Oh, hi, how are you?” Ava says. “And I died after that.”</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>Minecraft is an incredibly complex game, but it’s also — at first — inscrutable. When you begin, no pop-ups explain what to do; there isn’t even a “help” section. You just have to figure things out yourself. (The exceptions are the Xbox and Play&shy;Station versions, which in December added tutorials.) This unwelcoming air contrasts with most large games these days, which tend to come with elaborate training sessions on how to move, how to aim, how to shoot. In Minecraft, nothing explains that skeletons will kill you, or that if you dig deep enough you might hit lava (which will also kill you), or even that you can craft a pickax.</p><p>This “you’re on your own” ethos resulted from early financial limitations: Working alone, Persson had no budget to design tutorials. That omission turned out be an inadvertent stroke of genius, however, because it engendered a significant feature of Minecraft culture, which is that new players have to learn <em>how</em> to play. Minecraft, as the novelist and technology writer Robin Sloan has observed, is “a game about secret knowledge.” So like many modern mysteries, it has inspired extensive information-&shy;&shy;sharing. Players excitedly pass along tips or strategies at school. They post their discoveries in forums and detail them on wikis. (The biggest one, hosted at the site Gamepedia, has nearly 5,000 articles; its entry on Minecraft’s “horses,” for instance, is about 3,600 words long.) Around 2011, publishers began issuing handbooks and strategy guides for the game, which became runaway best sellers; one book on redstone has outsold literary hits like “The Goldfinch,” by Donna Tartt.</p><p>“In Minecraft, knowledge becomes social currency,” says Michael Dezuanni, an associate professor of digital media at Queensland University of Technology in Australia. Dezuanni has studied how middle-&shy;school girls play the game, watching as they engaged in nuanced, Talmudic breakdowns of a particular creation. This is, he realized, a significant part of the game’s draw: It offers many opportunities to display expertise, when you uncover a new technique or strategy and share it with peers.</p><p>The single biggest tool for learning Minecraft lore is YouTube. The site now has more than 70 million Minecraft videos, many of which are explicitly tutorial. To make a video, players use “screencasting” software (some of which is free, some not) that records what’s happening on-screen while they play; they usually narrate their activity in voice-&shy;over. The problems and challenges you face in Minecraft are, as they tend to be in construction or architecture, visual and three-&shy;dimensional. This means, as many players told me, that video demonstrations have a particularly powerful explanatory force: It’s easiest to learn something by seeing someone else do it. In this sense, the game points to the increasing role of video as a rhetorical tool. (“Minecraft” is the second-&shy;most-&shy;searched-&shy;for term on YouTube, after “music.”)</p></div><aside aria-label=\"companion column\"></aside></div><div><div></div><div><figure role=\"group\" aria-label=\"media\"><div><div><div></div></div></div><figcaption><span><span>Credit...</span><span>Illustration by Christoph Niemann</span></span></figcaption></figure></div></div><div><div><p>That includes Ava on Long Island — who, after being killed by skeletons, began watching “survival mode” videos to learn how to stay alive. Soon she had mastered that, and also discovered the huge number of YouTube videos in which players review “minigames,” little challenges that some Minecraft devotees design and load onto servers for others to play. (In one popular minigame, for example, players are shown a sculpture made of blocks and then try to copy it exactly in 30 seconds.) For young Minecraft fans, these videos are a staple of their media diet, crowding out TV. Ava’s mother is genially baffled by this. “I don’t understand it,” she told her daughter when I visited them last fall. “Why are you watching other people play the game? Why don’t you just play?”</p><p>Ava had recently started her own YouTube channel with her friends Aaron and Patrick, where they play and review minigames. Her father set up a high-&shy;quality microphone on a telescoping arm bolted to the computer desk; her sister drew Ava a white sign that says: “RECORDING.” (Its back says: “NOT RECORDING JUST WANT YOU TO BE QUIET.”) As the family’s gray cat wandered around Ava’s keyboard, she dialed up Patrick on a Skype video call.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>When they record a video, they improvise freestyle banter while playing, and simply start all over again if something goes awry. (Which, Patrick said dryly, “happens often.”) So far they have 19 subscribers and have posted 21 videos.</p><p>She played a recent video for me, in which they tried to navigate a difficult map filled with lethal, flowing lava. Their conversation is loose and funny; it’s like listening to two talk-&shy;radio hosts, or perhaps the commentary over a game of basketball — if the commentary were delivered by the athletes themselves, while they play.</p><p>Considered as a genre, YouTube Minecraft videos are quite strange. They take elements of “how to” TV — a cooking show, a home-&shy;renovation show — and blend them with the vocal style of podcasting, while mixing in a dash of TV shows like “Orange County Choppers,” where ingenious mechanics parade their creations.</p><p>“I don’t even know that I know how to properly classify them,” says Ryan Wyatt, the head of gaming content for YouTube. Minecraft videos offer a glimpse of the blurring of the line between consumers and creators. Probably two-thirds of the kids I interviewed had started their own Minecraft channels on YouTube. Most of them were happy when even a handful of friends and family watched their videos.</p><p>Some Minecraft broadcasters have become genuinely famous, though, and earn a good living from their work. These superstars aren’t children, generally; they’re young adults, like Joseph Garrett, known as <a rel=\"noopener noreferrer\" title=\"\" href=\"https://www.youtube.com/channel/UCj5i58mCkAREDqFWlhaQbOw?nohtml5=False\">Stampy Cat</a>, a 25-year-old Briton with seven million YouTube subscribers. One of my children’s favorite Minecraft broadcasters is a user named <a rel=\"noopener noreferrer\" title=\"\" href=\"https://www.youtube.com/user/ThatMumboJumbo?nohtml5=False\">Mumbo Jumbo</a>, another Briton, whose real name is Oliver Brotherhood, known for his instructional videos on using redstone. He is 20 and began posting his videos online when he was 16, he says. At first he did it for fun, until one video — which showcases 20 complex opening-&shy;door devices — became an unexpected hit, netting him one million views. “It’s not the next ‘Gangnam Style,’ but it was pretty good,” Brotherhood says. As more fans found him, he began posting daily and now spends 50 hours a week shooting videos and replying to fans. Brotherhood delivered newspapers while in school, but a year ago his YouTube ad revenue outstripped it.</p><p>“I told my mom, ‘I’m quitting my paper round,’ and she said, ‘Why?’ And I said, ‘I do a YouTube channel, and it’s earning me more.’&thinsp;” When his mother looked at his channel, she saw it had more than 40,000 subscribers and more monthly traffic than the corporate newspaper sites she consults for.</p><p>Next year he plans to study computer science in college. “In the redstone community,” he says, “a lot of people around me are programmers.” Teaching himself coding is much like learning Minecraft, he found; you experiment, ask questions on Internet forums. He described his YouTube channel on his college application, and that, too, “seems to have helped,” he says. The university accepted him without even seeing his final school grades.</p></div><aside aria-label=\"companion column\"></aside></div><div><div></div><div><figure role=\"group\" aria-label=\"media\"><div><div><div></div></div></div><figcaption><span><span>Credit...</span><span>Illustration by Christoph Niemann</span></span></figcaption></figure></div></div><div><div><p><strong>Last year, London, </strong>a 12-year-old in Washington State, set up a server so he could play Minecraft with friends. He left it public, open to anyone — which led to chaos when some strangers logged on one day to start “griefing,” blowing up his and his friends’ creations with TNT. He shut down the server and, a bit wiser now, started a new one with some strict rules. This one included a “whitelist,” so only players pre&shy;approved by London can log in, and a plug-in — a piece of code that changes how the server works — that prevents players from destroying what others have made.</p><p>Most online games don’t require kids to manage the technical aspects of how gamers interact. A hugely popular commercial game like World of Warcraft, for example, is played on a server run by its owner, Blizzard Entertainment. Game companies usually set the rules of what is and isn’t allowed in their games; if you grief others, you might be banned by a corporate overlord. Or the opposite might happen: Abuse might be ignored or policed erratically.</p><p>But Minecraft is unusual because Microsoft doesn’t control all the servers where players gather online. There is no single Minecraft server that everyone around the world logs onto. Sometimes kids log onto a for-&shy;profit server to play mini&shy;games; sometimes they rent a server for themselves and their friends. (Microsoft and Mojang run one such rental service.) Or sometimes they do it free at home: If you and I are in the same room and we both have tablets running Minecraft, I can invite you into my Minecraft world through Wi-Fi.</p><p>What this means is that kids are constantly negotiating what are, at heart, questions of governance. Will their world be a free-for-all, in which everyone can create and destroy everything? What happens if someone breaks the rules? Should they, like London, employ plug-ins to prevent damage, in effect using software to enforce property rights? There are now hundreds of such governance plug-ins.</p><p>Seth Frey, a postdoctoral fellow in computational social science at Dartmouth College, has studied the behavior of thousands of youths on Minecraft servers, and he argues that their interactions are, essentially, teaching civic literacy. “You’ve got these kids, and they’re creating these worlds, and they think they’re just playing a game, but they have to solve some of the hardest problems facing humanity,” Frey says. “They have to solve the tragedy of the commons.” What’s more, they’re often anonymous teenagers who, studies suggest, are almost 90 percent male (online play attracts far fewer girls and women than single-&shy;player mode). That makes them “what I like to think of as possibly the worst human beings around,” Frey adds, only half-&shy;jokingly. “So this shouldn’t work. And the fact that this works is astonishing.”</p><p>Frey is an admirer of Elinor Ostrom, the Nobel Prize-&shy;winning political economist who analyzed the often-&shy;unexpected ways that everyday people govern themselves and manage resources. He sees a reflection of her work in Minecraft: Running a server becomes a crash course in how to compromise, balance one another’s demands and resolve conflict.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>Three years ago, the public library in Darien, Conn., decided to host its own Minecraft server. To play, kids must acquire a library card. More than 900 kids have signed up, according to John Blyberg, the library’s assistant director for innovation and user experience. “The kids are really a community,” he told me. To prevent conflict, the library installed plug-ins that give players a chunk of land in the game that only they can access, unless they explicitly allow someone else to do so. Even so, conflict arises. “I’ll get a call saying, ‘This is Dasher80, and someone has come in and destroyed my house,’&thinsp;” Blyberg says. Sometimes library administrators will step in to adjudicate the dispute. But this is increasingly rare, Blyberg says. “Generally, the self-&shy;governing takes over. I’ll log in, and there’ll be 10 or 15 messages, and it’ll start with, ‘So-and-so stole this,’ and each message is more of this,” he says. “And at the end, it’ll be: ‘It’s O.K., we worked it out! Disregard this message!’&thinsp;”</p><p>Several parents and academics I interviewed think Minecraft servers offer children a crucial “third place” to mature, where they can gather together outside the scrutiny and authority at home and school. Kids have been using social networks like Instagram or Snapchat as a digital third place for some time, but Minecraft imposes different social demands, because kids have to figure out how to respect one another’s virtual space and how to collaborate on real projects.</p><p>“We’re increasingly constraining youth’s ability to move through the world around them,” says Barry Joseph, the associate director for digital learning at the American Museum of Natural History. Joseph is in his 40s. When he was young, he and his friends roamed the neighborhood unattended, where they learned to manage themselves socially. Today’s fearful parents often restrict their children’s wanderings, Joseph notes (himself included, he adds). Minecraft serves as a new free-&shy;ranging realm.</p></div><aside aria-label=\"companion column\"></aside></div><div><div></div></div><div><div><p>Joseph’s son, Akiva, is 9, and before and after school he and his school friend Eliana will meet on a Minecraft server to talk and play. His son, Joseph says, is “at home but still getting to be with a friend using technology, going to a place where they get to use pickaxes and they get to use shovels and they get to do that kind of building. I wonder how much Minecraft is meeting that need — that need that all children have.” In some respects, Minecraft can be as much social network as game.</p><p>Just as Minecraft propels kids to master Photoshop or video-&shy;editing, server life often requires kids to acquire complex technical skills. One 13-year-old girl I interviewed, Lea, was a regular on a server called Total Freedom but became annoyed that its administrators weren’t clamping down on griefing. So she asked if she could become an administrator, and the owners said yes.</p><p>For a few months, Lea worked as a kind of cop on that beat. A software tool called “command spy” let her observe records of what players had done in the game; she teleported miscreants to a sort of virtual “time out” zone. She was eventually promoted to the next rank — “telnet admin,” which allowed her to log directly into the server via telnet, a command-&shy;line tool often used by professionals to manage servers. Being deeply involved in the social world of Minecraft turned Lea into something rather like a professional systems administrator. “I’m supposed to take charge of anybody who’s breaking the rules,” she told me at the time.</p></div><aside aria-label=\"companion column\"></aside></div><div><div><p>Not everyone has found the online world of Minecraft so hospitable. One afternoon while visiting the offices of Mouse, a nonprofit organization in Manhattan that runs high-tech programs for kids, I spoke with Tori. She’s a quiet, dry-&shy;witted 17-year-old who has been playing Minecraft for two years, mostly in single-&shy;player mode; a recent castle-&shy;building competition with her younger sister prompted some bickering after Tori won. But when she decided to try an online server one day, other players — after discovering she was a girl — spelled out “BITCH” in blocks.</p><p>She hasn’t gone back. A group of friends sitting with her in the Mouse offices, all boys, shook their heads in sympathy; they’ve seen this behavior “everywhere,” one said. I have been unable to find solid statistics on how frequently harassment happens in Minecraft. In the broader world of online games, though, there is more evidence: An academic study of online players of Halo, a shoot-’em-up game, found that women were harassed twice as often as men, and in an unscientific poll of 874 self-&shy;described online gamers, 63 percent of women reported “sex-&shy;based taunting, harassment or threats.” Parents are sometimes more fretful than the players; a few told me they didn’t let their daughters play online. Not all girls experience harassment in Minecraft, of course — Lea, for one, told me it has never happened to her — and it is easy to play online without disclosing your gender, age or name. In-game avatars can even be animals.</p><p><strong>How long will Minecraft’s</strong> popularity endure? It depends very much on Microsoft’s stewardship of the game. Company executives have thus far kept a reasonably light hand on the game; they have left major decisions about the game’s development to Mojang and let the team remain in Sweden. But you can imagine how the game’s rich grass-roots culture might fray. Microsoft could, for example, try to broaden the game’s appeal by making it more user-&shy;friendly — which might attenuate its rich tradition of information-&shy;sharing among fans, who enjoy the opacity and mystery. Or a future update could tilt the game in a direction kids don’t like. (The introduction of a new style of combat this spring led to lively debate on forums — some enjoyed the new layer of strategy; others thought it made Minecraft too much like a typical hack-and-slash game.) Or an altogether new game could emerge, out-&shy;Minecrafting Minecraft.</p><p>But for now, its grip is strong. And some are trying to strengthen it further by making it more accessible to lower-&shy;income children. Mimi Ito has found that the kids who acquire real-world skills from the game — learning logic, administering servers, making YouTube channels — tend to be upper middle class. Their parents and after-&shy;school programs help them shift from playing with virtual blocks to, say, writing code. So educators have begun trying to do something similar, bringing Minecraft into the classroom to create lessons on everything from math to history. Many libraries are installing Minecraft on their computers.</p><p>One recent afternoon, I visited the Bronx Library Center, a sleek, recently renovated building in a low-&shy;income part of the borough. A librarian named Katie Fernandez had set up regular Minecraft days for youths, and I watched four boys play together on the library’s server. Fernandez had given them a challenge: Erect a copy of the Arc de Triomphe in Paris in 45 minutes. Three of them began collaborating on one version; a younger boy worked on his own design. The three gently teased one another about their skills. “No, no, stop!” shouted one, when he noticed another building a foot of the Arc too wide. “Ryan, this — like this!” They debated whether command blocks would speed things up. As the 45th minute approached, they hadn’t quite finished their Arc, so they gleefully stuffed the interior with TNT, detonated it and hopped onto different games.</p><p>Over in the corner, the fourth boy continued to labor away at his Arc. He told me he often stays up late playing Minecraft with friends; they have built the Statue of Liberty, 1 World Trade Center and even a copy of the very library he was sitting in. His fingers clicked in a blur as he placed angled steps, upside-&shy;down, to mimic the Arc’s beveled top. He sat back to admire his work. “I haven’t blinked for over — I don’t know how many minutes,” he said. The model was complete, and remarkably realistic.</p><p>“I’m actually pretty proud of that,” he said with a smile.</p></div><aside aria-label=\"companion column\"></aside></div></section><div style=\"font-style:italic;font-size:11px;line-height:12.5px;opacity:0.8;padding-top:12px;padding-bottom:12px;\">© 2020 The New York Times Company.<br><br>
            The content you have chosen to save (which may include videos, articles, 
            images and other copyrighted materials) is intended for your personal, noncommercial use. 
            Such content is owned or controlled by The New York Times Company or the party credited 
            as the content provider. Please refer to nytimes.com and the Terms of Service available 
            on its website for information and restrictions related to the content.</div>
            </div>"
    },
    {
        title: "Horizontal History",
        author: "Tim Urban",
        url: 'waitbutwhy.com',
        full_url: 'https://waitbutwhy.com/2016/01/horizontal-history.html', 
        reading_time: '25 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fwaitbutwhy.com%2Fwp-content%2Fuploads%2F2016%2F01%2FRenaissance-F-1.jpg",
        content: 
            "<div class='article-content'><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fwaitbutwhy.com%2Fwp-content%2Fuploads%2F2016%2F01%2FRenaissance-F-1.jpg\">
            </figure>
            </div><div>
            <div>
            <div><p>Most of us have a pretty terrible understanding of history. Our knowledge is spotty, with large gaps all over the place, and the parts of history we do end up knowing a lot about usually depend on the particular teachers, parents, books, articles, and movies we happen to come across in our lives. Without a foundational, tree-trunk understanding of all parts of history, we often forget the things we do learn, leaving even our favorite parts of history a bit hazy in our heads. Raise your hand if you’d like to go on stage and debate a history buff on the nuances of a historical time period of your choosing. That’s what I thought.</p>
            <p>The reason history is so hard is that it’s so <em>soft</em>. To truly, fully understand a time period, an event, a movement, or an important historical figure, you’d have to be there, and many times over. You’d have to be in the homes of the public living at the time to hear what they’re saying; you’d have to be a fly on the wall in dozens of secret, closed-door meetings and conversations; you’d need to be inside the minds of the key players to know their innermost thoughts and motivations. Even then, you’d be lacking context. To really have the complete truth, you’d need background—the cultural nuances and national psyches of the time, the way each of the key players was raised during childhood and the subtle social dynamics between those players, the impact of what was going on in other parts of the world, and an equally-thorough understanding of the many past centuries that all of these things grew out of.</p>
            <p>That’s why not only can’t even the most perfect history buff fully understand history, but the key people involved <em>at the time</em> can’t ever know the full story. History is a giant collective tangle of thousands of interwoven stories involving millions of characters, countless chapters, and many, many narrators.</p>
            <p>And you know humans—that’s not how they like things. The human brain really, really likes to <em>simplify </em>things. History provides the context of our world and our lives, because each of us is a character in this grand story—and the last thing we want to believe is that the story is too complicated and mysterious for us to understand.</p>
            <p>Fairy tales are satisfying, because the plot is crystal clear—there are good guys and there are bad guys and there’s only one side of the story. Children identify with the good guys—the <em>us </em>guys—and they detest the bad guys—the <em>them</em> guys<em>—</em>and everyone’s happy. Stories written for adults aren’t that different—you loved <em>Shawshank </em>and <em>Braveheart</em> and <em>Star Wars</em>, right?</p>
            <p>So when it comes to the story we’re all a <em>part </em>of, we most certainly want to feel the same way. We want history to be simple and clear, with good guys and bad guys, and we’d like to make sure that our ancestors, our ethnic group, our nation, and all the other tribes we belong to are Aladdin in the story—not Jafar.</p>
            <p>The problem with this is that not everyone can be Aladdin. <em>Someone </em>has to be Jafar, right? Well, no. Not if there are many different story-tellers. Since no one is ever telling anything close to the full, real, <em>complete </em>story, in all its complexity—as we said, no one even <em>knows </em>the full story—each historian, each ruler, and each society creates their own fairy tale version of what went down in the past. When things are unsatisfyingly multi-faceted, we pick the facet we like best. When there are knowledge gaps, we make things up. When there are questions of motive, we pick one that fits nicely into the narrative.</p>
            <p>This leaves us with plenty of tools to leave every story with a proper Aladdin and a proper Jafar and allows us to make sure that Aladdin is exactly who we want him to be.</p>
            <p>The US is a good example. A huge number of people in today’s world have been told a story of the US in which the US is Aladdin, and a huge other number of people have heard the same story with the US as Jafar. Some people will claim to have a more nuanced view, but deep in their heart, when they see an American flag, they see either a good guy flag or a bad guy flag. (One of the major political divides in the US stems from liberals thinking conservatives over-Aladdinize the US and conservatives thinking liberals over-Jafarify the US while Aladdinizing the other side.)</p>
            <p>This is the same phenomenon behind the stark opinion divide around Israel and Palestine. Hordes of people on both sides of what is an <em>insanely</em> complicated story are red in the face with ire at the other side, completely positive that their side is Aladdin and incensed that anyone could ever call the other side Aladdin and their side Jafar. Only with the stark clarity of a fairy tale could people ever feel so unshakingly sure.</p>
            <p>Of course, it’s not that there are no good guys or bad guys in history. History is a pretty ugly story—what else would you expect from a species of primitive biological animals—and accountability for that ugliness isn’t spread out evenly amongst all people. To an extent, the definition of words like good and bad, right and wrong, and hero and villain lie in the eye of the beholder—but there’s also plenty of human behavior that qualifies as objectively good or bad.</p>
            <p>So it’s not that there are never objective Aladdins and there are never objective Jafars—it’s that almost none of us has any idea what the fuck we’re talking about. Point to a historical event and tell me that there was a true Aladdin and Jafar going on, and I’ll acknowledge that that might be true. Tell me that you know who was who, and in most cases I’ll shake my head.</p>
            <p>Which brings me to me. Blogging about history is asking for trouble. Portray nearly any story or person as an Aladdin or a Jafar and you’ll feel the wrath of both the people who believe the opposite situation and the people who think you’ve oversimplified the situation. Portray something in a nuanced and balanced way and you’ll get yelled at by people who believe <em>both</em> of the one-sided views. Nothing brings people’s tribal fires to the table like history. I’ve learned this <a href=\"https://waitbutwhy.com/2014/09/muhammad-isis-iraqs-full-story.html\">from</a> <a href=\"https://waitbutwhy.com/2014/05/absurdly-famous-people-probably-dont-know-enough.html\">experience</a>.</p>
            <p>This doesn’t make me any less excited to write about history—but it makes me want to research the shit out of a part of history before I write about it. Only by reading a bunch of varying accounts and opinions can you start to form a clear picture of what we know and don’t know.</p>
            <p>So that’s why for this post, I’m not gonna tell you shit. Rather than dive into the weeds of what happened when, and why, I’m going to focus on one of the rare elements of history that’s indisputably black-and-white—<em>who </em>happened when.</p>
            <p>Because before we can responsibly start arguing with each other about Aladdins and Jafars, we need to get the basic timeline and characters of the story clear.</p>
            <p>But I’m going to lay things out a little differently than you’re probably used to.</p>
            <p>Normally, we learn about history’s storylines in isolation. We might have a strong sense of the history of physics breakthroughs or the progression of western philosophical thought or the succession of French rulers—but we’re not as clear on how each of these storylines relate to each <em>other</em>. If you think of history like a tangle of vines growing upwards through time, studying one type of history at a time is like following the path of one particular vine while ignoring the other vines around it. It’s understanding history in a <em>vertical </em>sense.</p>
            <p>And while vertical history has its merits, it doesn’t leave you with an especially <em>complete</em> picture of any one time. An econ buff in the year 2500 might know all about the Great Depression that happened in the early 20th century and the major recession that happened about 80 years later, but that same person might mistake the two world wars for happening in the 1800s or the 2200s if they’re a little hazy on the history of wars. So while an econ buff, that person would have a pretty poor understanding of what our modern times are all <em>about. </em></p>
            <p>Likewise, I might know that Copernicus began writing his seminal work <i>On the Revolutions of the Celestial Spheres </i>in Poland in the early 1510s, but by learning that right around that same time in Italy, Michelangelo painted the ceiling of the Sistine Chapel, I get a better picture of the <em>times</em>. By learning that it was right while both of these things were happening that Henry VIII married Catherine of Aragon in England, the 1510s suddenly begins to take on a distinct <em>personality</em>. These three facts, when put together, allow me to see a more three-dimensional picture of the 1510s—it allows me to see the 1510s <em>horizontally</em>, like cutting out a complete segment of the vine tangle and examining it all together.</p>
            <p>A blog post is limited in its ability to examine all of history horizontally. But I’ve taken two separate cracks below that I think can work together nicely to help us take a horizontal view of different times. Both involve a lot of names.</p>
            <p>Which leads me to the inevitable disclaimer about <em>who </em>I chose to include. I tried to remove my own biases by gathering the names from a handful of lists by publications like <em>Time</em>. I searched the internet for things like “most influential people in history” and “most important people in the Middle Ages” and “most famous people of the 19th century” and “most powerful Chinese emperors” and ended up with a big pile of names, some of whom I’m familiar with, others I’m not. That said, between the fact that the lists I used were by publications targeting English-speaking people and that I inevitably leaned more towards people I had heard of, the group of names will skew America- and Euro-centric, with places like Africa, the Middle East, and Southeast Asia probably underrepresented. This isn’t entirely by accident, though—this post is only useful if you’ve heard of the people, and I intentionally chose names I thought a large portion of Wait But Why readers would know. In other words, merit wasn’t the only criteria—household fame mattered too. And yes, I missed a lot of people—with limited space on the screen, the names had to be a sampling, not an exhaustive list.</p>
            <p><span><strong>Horizontal History—First Crack:</strong></span></p>
            <p>For my first crack, I present to you a big pile of famous names, organized by birth decade—kind of a “2,600 Under 2,600” list. The purpose is to help orient ourselves on when people lived, especially <em>in relation to each other</em>.</p>
            <p>Having a clear picture of generations is very easy when you think about currently-living people. For example, I know that Mark Zuckerberg is around my age while Vladimir Putin is about the age of my parents and George H.W. Bush is about the age of my grandparents. On the other side of things, Prince George—the one world-famous baby—is the age of my kids if I had kids. I know this without having to think about it:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FMy-generations.png\">
            </figure>
            </div>
            <p>If I list people by birth decade instead of generation, it still makes sense. People born in the 70s and 60s feel older than me but not as old as my parents, and people born in the 30s and 40s feel older than my parents but younger than my grandparents:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_3\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FMy-generations-decades.png\">
            </figure>
            </div>
            <p>But this is much harder for generations that aren’t currently alive, and it gets less and less clear the farther back you go. Quick! Name the oldest member and youngest member of this group: Nietzsche, Darwin, Freud, Marx, Gandhi, Tolstoy, Twain. Not that easy, right? And that’s only going back 200 years. But by laying them out by birth decade, you can get oriented:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_4\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FOlder-decades.png\">
            </figure>
            </div>
            <p>Since a generation is typically about 30 years, you can move three or six lines down from a name to see who they viewed as their parents’ or grandparents’ ages during their lifetimes, and you can go the same distance up to see who they viewed as their kids’ or grandkids’ age. So Darwin would have seen Twain as some young kid and he would have shaken his old man fist at Gandhi from the rocking chair on his porch. Meanwhile, Nietzsche would have seen Marx as a guy his dad’s age and Freud as a contemporary, though a bit younger.</p>
            <p>Two people more than seven or eight lines away from each other on the list probably were not ever alive at the same time, which means they were likely not that clear about each others’ generation, in the way I’m not really clear on whether Hemingway was in my great-grandparents’ generation, my great-great-grandparents’ generation, or some other age.</p>
            <p>Using this decade list tool, let’s look at a whole group of famous historical figures to see who was the same age as whom, who shook their old man fist at whom, and who was and wasn’t alive at the same time. The decade colors are in a three-way cycle, so you can jump to rows of the same color above and below to quickly go up and down by generations (i.e. if you take a name on a green line, one green line down is their parents’ age, three green lines up is their great-grandchildren’s age, etc.). For people alive today and in the past century, I couldn’t come close to including every famous person, so I just picked a sampling.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_5\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FHorizontal-History-Generations-4.jpg\">
            </figure>
            </div>
            <p>Okay how did that go? Fun? Icky? I can’t quite tell. In any case, let’s move on.</p>
            <p><span><strong>Horizontal History—Second Crack:</strong></span></p>
            <p>For my second crack at depicting history horizontally, I present to you A Psychotic Jumble of Colorful Vertical Bars That Might Be Awesome or Maybe Not I Can’t Tell Here Either But I Think It Might Be Fun.</p>
            <p>This time, I got more specific than birth decade and actually identified the exact birth year <em>and </em>the death year of each name, using a bar to depict their lifespan. While the above chart simplifies who lived when, the diagram below allows you to follow a single horizontal path along any year and see who was and wasn’t alive at that time.</p>
            <p>As I made the diagram, I ran into a big problem, which is that it looked like an upside-down L with <em>way </em>more names at the top than the bottom (because there are a lot more household names who were alive in the last 200 years than in previous centuries). Crunching all those recent names into blog width made the font tiny—so I solved the problem by cutting out about half of the recent names. <em>But</em>, before I did, I broke the complete list of 1800–2016 names into two groups by category and here they are below:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_6\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FHorizontal-History-Lifespans-Recent-1-2.jpg\">
            </figure>
            </div>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_7\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FHorizontal-History-Lifespans-Recent-2-2.jpg\">
            </figure>
            </div>
            <p>Okay now that that’s out of the way, here’s the big list (with only half of the total 1800–2016 names included). It goes back to 1450. Trace a horizontal line across to get a feel for what was going on during that particular time.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_8\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FHorizontal-History-Lifespans-4.jpg\">
            </figure>
            </div>
            [<strong>Note: </strong>A number of people have requested a sideways version of this diagram. <a href=\"https://28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com/wp-content/uploads/2016/01/Horizontal-History-Lifespans-Sideways.jpg\">Here it is.</a>]
            <p>Some overall thoughts:</p>
            <ul><li><strong>Lifespans are unfair.</strong> Looking at people’s lives visually really makes it clear when two people are born around the same time but then one of them randomly dies 30 years before the other.</li>
            </ul><ul><li><strong>Murder is dickish.</strong> Another thing this diagram highlights. How not okay is it to cut someone <em>else’s </em>bar short? JFK might have been on his way to a nice 85-year bar when this other guy just took a scissors and <em>snipped </em>his bar.</li>
            </ul><ul><li><b>On the other hand, short lives were appreciated during the making of this diagram. </b>This was a nightmare of a puzzle, especially at the top, and while trying to fit a lot of bars into a small space, there were times I found myself saying, out loud, “Oh nice,” when I’d look up someone’s life dates and realize that they were murdered at a young age. Likewise, one factor that led to a number of the recent people being cut from the big diagram was living too long. Frank Lloyd Wright’s a cool dude, but not 2.5 inches of diagram cool.</li>
            </ul><ul><li><strong>Some people aren’t easily categorized<em>. </em></strong>I tried my best. You try putting Ben Franklin into a category.</li>
            </ul><ul><li><strong>Yeah, yeah, I said the whole “Aladdin and Jafar are in the eye of the beholder” thing and then I created a category for people who I deemed dicks.<em> </em></strong>I know. But it was fun to label certain people as dicks. Ya know?</li>
            </ul><p>Each little part of this diagram tells a story. Let’s go through a few examples:</p>
            <p>I mentioned in a box in the first chart that Mozart wrote his <em>Requiem</em> the same year the US forefathers were writing the Bill of Rights and that Beethoven had a love-hate relationship with Napoleon—but using the lifespan diagram, you can see both of these stories visually.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_9\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2F1780.jpg\">
            </figure>
            </div>
            <p>I also mentioned the major Shakespeare, Galileo, Tokugawa, and John Smith events that all happened right around the year 1610.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_10\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2F1610.png\">
            </figure>
            </div>
            <p>And in the intro, I referenced Copernicus’s seminal work happening right when Michelangelo was painting the Sistine Chapel ceiling and Henry VIII was marrying Catherine of Aragon.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_11\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2F1510.png\">
            </figure>
            </div>
            <p>But if you look at the other stuff the diagram shows going on around that same time, it tells a bigger story:</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_12\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2FRenaissance-2.jpg\">
            </figure>
            </div>
            <p>While Copernicus, Michelangelo, and Henry VIII were happening, it was also the golden age of European exploration—look at all those light purple explorers!—and the precursor age to the oncoming Age of Imperialism. Simultaneously, you can see the Protestant Reformation brewing with the presence of all those dark blue religious figures. The one dark blue exception is Guru Nanak, who was over in Asia being the founding prophet of Sikhism (today’s 5th biggest religion). Meanwhile, Michelangelo was part of something larger, as the other pink bars—and Machiavelli—remind us that the Italian Renaissance was in full swing.</p>
            <p>Every time I look at the lifespan diagram, a new interesting horizontal pops out to me. Here’s one more: People in the US associate the 1860s with Lincoln and the Civil War. But what we overlook is that the 1860s was one of history’s greatest literary decades. In the ten years between 1859 and 1869, Darwin published his world-changing <em>On the Origin of Species </em>(1859), Dickens published <em>A Tale of Two Cities </em>(1859) and <em>Great Expectations </em>(1861), Lewis Carroll published <em>Alice in Wonderland </em>(1865), Dostoyevsky published <em>Crime and Punishment</em> (1866), and Tolstoy capped things off with <em>War and Peace</em> (1869). These guys were all in their primes <em>at the same time</em>. So was Lincoln, before some cock snipped his bar off at the worst time possible.</p>
            <div class=\"RIL_IMG\" id=\"RIL_IMG_13\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2F28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2016%2F01%2F1860s-2.jpg\">
            </figure>
            </div>
            <p>So there’s some horizontal history for you. Now go brush up so we can all be oriented the next time we yell at each other about fairy tales.</p>
            <p>If you’re into Wait But Why, sign up for the <strong><a href=\"http://eepurl.com/Ffj9D\">Wait But Why email list</a></strong> and we’ll send you the new posts right when they come out. Better than having to check the site and wonder!</p>
            <p>If you’d like to support Wait But Why, <strong><a href=\"https://www.patreon.com/waitbutwhy\">here’s our Patreon</a></strong>.</p>
            </div></div></div></div>"
    },
    {
        title: "Why Black Hole Interiors Grow (Almost) Forever",
        author: "Natalie Wolchover",
        url: 'www.quantamagazine.org',
        full_url: 'https://www.quantamagazine.org/why-black-hole-interiors-grow-forever-20181206/#:~:text=According%20to%20general%20relativity%2C%20the,stretches%20toward%20the%20center%20point.', 
        reading_time: '5 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f74aa316602b.jpg",
        content: 
            "<div class='article-content'><h2>The renowned physicist Leonard Susskind has identified a possible quantum origin for the ever-growing volume of black holes.</h2><article><article><div><div><section><div><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2F5f74aa316602b.jpg\">
            </figure>
            </div></div><p><a href=\"https://sitp.stanford.edu/people/leonard-susskind\">Leonard Susskind</a>, a pioneer of <a href=\"https://www.quantamagazine.org/tag/string-theory/\">string theory</a>, the holographic principle and other big physics ideas spanning the past half-century, has&nbsp;<a href=\"https://arxiv.org/abs/1810.11563\">proposed a solution</a>&nbsp;to an important puzzle about <a href=\"https://www.quantamagazine.org/tag/black-holes/\">black holes</a>. The problem is that even though these mysterious, invisible spheres appear to stay a constant size as viewed from the outside, their interiors keep growing in volume essentially forever. How is this possible?
            </p><p>
            In a series of recent papers and talks, the 78-year-old Stanford University professor and his collaborators conjecture that black holes grow in volume because they are steadily increasing in complexity — an idea that, while unproven, is fueling new thinking about the quantum nature of gravity inside black holes.
            </p><p>
            Black holes are spherical regions of such extreme gravity that not even light can escape. First discovered a century ago as shocking solutions to the equations of Albert Einstein’s general theory of relativity, they’ve since been detected throughout the universe. (They typically form from the inward gravitational collapse of dead stars.)&nbsp;Einstein’s theory equates the force of gravity with curves in space-time, the four-dimensional fabric of the universe, but gravity becomes so strong in black holes that the space-time fabric bends toward its breaking point — the infinitely dense “singularity” at the black hole’s center.
            </p><p>
            According to general relativity, the inward gravitational collapse never stops. Even though, from the outside, the black hole appears to stay a constant size, expanding slightly only when new things fall into it, its interior volume grows bigger and bigger all the time as space stretches toward the center point. For a simplified picture of this eternal growth, imagine a black hole as a funnel extending downward from a two-dimensional sheet representing the fabric of space-time. The funnel gets deeper and deeper, so that infalling things never quite reach the mysterious singularity at the bottom. In reality, a black hole is a funnel that stretches inward from all three spatial directions. A spherical boundary surrounds it called the “event horizon,” marking the point of no return.
            </p><p>
            Since at least the 1970s, physicists have recognized that black holes must really be quantum systems of some kind — just like everything else in the universe. What Einstein’s theory describes as warped space-time in the interior is presumably really a collective state of vast numbers of gravity particles called “gravitons,” described by the true <a href=\"https://www.quantamagazine.org/tag/quantum-gravity/\">quantum theory of gravity</a>. In that case, all the known properties of a black hole should trace to properties of this quantum system.
            </p><p>
            Indeed, in 1972, the Israeli physicist Jacob Bekenstein&nbsp;<a href=\"https://journals.aps.org/prd/abstract/10.1103/PhysRevD.7.2333\">figured out</a> that the area of the spherical event horizon of a black hole corresponds to its “entropy.” This is the number of different possible microscopic arrangements of all the particles inside the black hole, or, as modern theorists would describe it, the black hole’s storage capacity for information.
            </p><p>
            Bekenstein’s insight led <a href=\"https://www.quantamagazine.org/stephen-hawkings-black-hole-paradox-keeps-physicists-puzzled-20180314/\">Stephen Hawking</a> to&nbsp;<a href=\"https://www.nature.com/articles/248030a0\">realize</a>&nbsp;two years later that black holes have temperatures, and that they therefore radiate heat. This radiation causes black holes to slowly evaporate away, giving rise to the much-discussed “black hole information paradox,” which asks what happens to information that falls into black holes. Quantum mechanics says the universe preserves all information about the past. But how does information about infalling stuff, which seems to slide forever toward the central singularity, also evaporate out?
            </p><p>
            The relationship between a black hole’s surface area and its information content has kept quantum gravity researchers busy for decades. But one might also ask: What does the growing volume of its interior correspond to, in quantum terms? “For whatever reason, nobody, including myself for a number of years, really thought very much about what that means,” said Susskind. “What is the thing which is growing? That should have been one of the leading puzzles of black hole physics.”
            </p><p>
            In recent years, with the rise of quantum computing, physicists have been gaining new insights about physical systems like black holes by studying their information-processing abilities — as if they were quantum computers. This angle led Susskind and his collaborators to identify a candidate for the evolving quantum property of black holes that underlies their growing volume. What’s changing, the theorists say, is the “complexity” of the black hole — roughly a measure of the number of computations that would be needed to recover the black hole’s initial quantum state, at the moment it formed. After its formation, as particles inside the black hole interact with one another, the information about their initial state becomes ever more scrambled. Consequently, their complexity continuously grows.
            </p><p>
            Using <a href=\"https://www.quantamagazine.org/albert-einstein-holograms-and-quantum-gravity-20181114/\">toy models that represent black holes as holograms</a>, Susskind and his collaborators have shown that the complexity and volume of black holes both grow at the same rate, supporting the idea that the one might underlie the other.&nbsp;And, whereas Bekenstein calculated that black holes store the maximum possible amount of information given their surface area, Susskind’s findings suggest that they also grow in complexity at the fastest possible rate allowed by physical laws.
            </p><p><a href=\"http://www.theory.caltech.edu/people/preskill/\">John Preskill</a>, a theoretical physicist at the California Institute of Technology who also studies black holes using quantum information theory, finds Susskind’s idea very interesting.&nbsp;“That’s really cool that this notion of computational complexity, which is very much something that a computer scientist might think of and is not part of the usual physicist’s bag of tricks,” Preskill said, “could correspond to something which is very natural for someone who knows general relativity to think about,” namely the growth of black hole interiors.
            </p><p>
            Researchers are still puzzling over the implications of Susskind’s thesis. <a href=\"https://sitp.stanford.edu/people/aron-wall\">Aron Wall</a>, a theorist at Stanford (soon moving to the University of Cambridge), said, “The proposal, while exciting, is still rather speculative and may not be correct.”&nbsp;One challenge is defining complexity in the context of black holes, Wall said, in order to clarify how the complexity of quantum interactions might give rise to spatial volume.
            </p><p>
            A potential lesson, according to <a href=\"https://www.ias.edu/scholars/douglas-stanford\">Douglas Stanford</a>, a black hole specialist at the Institute for &nbsp;Advanced Study in Princeton, New Jersey, “is that black holes have a type of internal clock that keeps time for a very long time. For an ordinary quantum system,” he said, “this is the complexity of the state. For a black hole, it is the size of the region behind the horizon.”
            </p><p>
            If complexity does underlie spatial volume in black holes, Susskind envisions consequences for our understanding of cosmology in general. “It’s not only black hole interiors that grow with time. The space of cosmology grows with time,” he said. “I think it’s a very, very interesting question whether the cosmological growth of space is connected to the growth of some kind of complexity. And whether the cosmic clock, the evolution of the universe, is connected with the evolution of complexity. There, I don’t know the answer.”
            </p><p><i>Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences.</i></p></section></div></div></article></article></div>"
    },
    {
        title: "Mental Models I Find Repeatedly Useful",
        author: "Gabriel Weinberg",
        url: 'medium.com/@yegg',
        full_url: 'https://medium.com/@yegg/mental-models-i-find-repeatedly-useful-936f1cc405d', 
        reading_time: '5 min',
        featured: false, 
        # cover_img: "https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmiro.medium.com%2Fmax%2F3332%2F1*C1DE6dVyfkEdFIvmFZC_4Q.jpeg",
        content: 
            "<div class='article-content'><div class=\"RIL_IMG\" id=\"RIL_IMG_1\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmiro.medium.com%2Fmax%2F3332%2F1*C1DE6dVyfkEdFIvmFZC_4Q.jpeg\">
            </figure>
            </div><div><h1>Mental Models I Find Repeatedly Useful</h1><div><div><div><div><a href=\"https://medium.com/@yegg?source=post_page-----936f1cc405d--------------------------------\" rel=\"noopener\"><div class=\"RIL_IMG\" id=\"RIL_IMG_2\">
            <figure>
            <img alt=\"\" src=\"https://pocket-image-cache.com//filters:no_upscale()/https%3A%2F%2Fmiro.medium.com%2Ffit%2Fc%2F56%2F56%2F1*4idmo7xd7H02knpQ7BGCRw.jpeg\">
            </figure>
            </div></a></div><div><div><div><span><a href=\"https://medium.com/@yegg?source=post_page-----936f1cc405d--------------------------------\" rel=\"noopener\"><p>Gabriel Weinberg</p></a></span></div></div><span></span></div></div><div></div></div></div></div><p><strong><em>2019 UPDATE: Since this post came out, I co-authored a book about it called</em></strong><a rel=\"noopener nofollow\" href=\"https://superthinking.com/\"><strong><em> Super Thinking: The Big Book of Mental Models</em></strong></a><strong><em>. You can order it now from </em></strong><a rel=\"noopener nofollow\" href=\"https://amzn.to/2PqmeZD\"><strong><em>Amazon</em></strong></a><strong><em>, </em></strong><a rel=\"noopener nofollow\" href=\"https://www.barnesandnoble.com/w/books/1130204238?ean=9780525533580#/\"><strong><em>Barnes &amp; Noble</em></strong></a><strong><em>, or </em></strong><a rel=\"noopener nofollow\" href=\"https://www.indiebound.org/book/9780525533580\"><strong><em>Indiebound</em></strong></a><strong><em>.</em></strong></p><p>Around 2003 I came across <a rel=\"noopener nofollow\" href=\"https://en.wikipedia.org/wiki/Charlie_Munger\">Charlie Munger’s</a> 1995 speech, <a rel=\"noopener nofollow\" href=\"http://web.archive.org/web/20151004200748/http:/law.indiana.edu/instruction/profession/doc/16_1.pdf\">The Psychology of Human Misjudgment</a>, which introduced me to how behavioral economics can be applied in business and investing. More profoundly, though, it opened my mind to the power of seeking out and applying mental models across a wide array of disciplines.</p><p>A mental model <span><span>i</span></span>s just a concept you can use to help try to explain things (e.g. <a rel=\"noopener nofollow\" href=\"https://en.wikipedia.org/wiki/Hanlon%27s_razor\">Hanlon’s Razor</a> — “never attribute to malice that which is adequately explained by carelessness.”). There are tens of thousands of mental models, and every discipline has their own set that you can learn through coursework, mentorship, or first-hand experience.</p><p>There is a much smaller set of concepts, however, that come up repeatedly in day-to-day decision making, problem solving, and truth seeking. As <a rel=\"noopener nofollow\" href=\"https://25iq.com/2015/08/22/a-dozen-things-ive-learned-from-charlie-munger-about-mental-models-and-worldly-wisdom/\">Munger says</a>, “80 or 90 important models will carry about 90% of the freight in making you a worldly‑wise person.”</p><p>This post is my attempt to enumerate the mental models that are repeatedly useful to me. This set is clearly biased from my own experience and surely incomplete. I hope to continue to revise it as I remember and learn more.</p><p><strong>How-to Use This List</strong></p><p>I find mental models are useful to try to make sense of things and to help generate ideas. To actually be useful, however, you have to apply them in the right context at the right time. And for that to happen naturally, you have to know them well and practice using them.</p><p>Therefore, here are two suggestions for using this list:</p><ol><li>For mental models you don’t know or don’t know well, you can use this list as a jumping off point to study them. I’ve provided links (mainly to Wikipedia) to start that process.</li><li>When you have a particular problem in front of you, you can go down this list, and see if any of the models could possibly apply.</li></ol><p><strong>Notes</strong></p><ul><li>Most of the mental models on this list are here because they are useful outside of their specific discipline. For example, use of the mental model “peak oil” isn’t restricted to an energy context. Most references to “peak x” are an invocation of this model. Similarly, inflation as a concept applies outside of economics, e.g. grade inflation and expectations inflation.</li><li>I roughly grouped the mental models by discipline, but as noted, this grouping is not to be taken as an assertion that they only apply within that dicipline. The best ideas often arise when going cross-dicipline.</li><li>I realize my definition of mental model differs from some others, with mine being more broadly defined as any concept that helps explain, analyze, or navigate the world. I prefer this broader definition because it allows me to assemble a more wide-ranging list of useful concepts that may not be mental models under other definitions, but I nevertheless find on relatively equal footing in terms of usefulness in the real world.</li><li>The numbers next to each mental model reflect the frequency with which they come up:  <br>(1) — Frequently (63 models)  <br>(2) — Occasionally (43 models)  <br>(3) — Rarely, though still repeatedly (83 models)</li><li>If studying new models, I’d start with the lower numbers first. The quotes next to each concept are meant to be a basic definition to remind you what it is, and not a teaching tool. Follow the link to learn more.</li><li>I am not endorsing any of these concepts as normatively good; I’m just saying they have repeatedly helped me explain and navigate the world.</li><li>I wish I had learned many of these years earlier. In fact, the proximate cause for posting this was so I could more effectively answer the question I frequently get from people I work with: “what should I learn next?” If you’re trying to be generally effective, my best advice is to start with the things on this list.</li></ul></div>"
    },
]

User.create(username: "demo", email: "demo@email.com", password: "apple123")

Article.create(articles);

file_names= [
    "pg.jpg",
    "munger.jpg",
    "odin.png",
    "algos.jpg",
    "brick.jpg",
    "sapiens.jpg",
    "art.jpg",
    "stripe.png",
    "hash.png",
    "systems.png",
    "pg-2.jpg",
    "startup.jpg",
    "compass.png",
    "taco.jpg",
    "dog.jpg",
    "brain.png",
    "quantum.png",
    "fantasy.jpg",
    "minecraft.png",
    "history.jpg",
    "gravity.jpg",
    "mind.jpg"
]

# articles = Article.all

# file = open('https://rocket--kb-dev.s3-us-west-1.amazonaws.com/dancer.jpg')
# Article.first.cover_img.attach(io: file, filename: 'dancer.jpg')
# Article.all[1].cover_img.attach(io: file, filename: 'dancer.jpg')

articles = Article.all

articles.each_with_index do |article, i|
    file = open("https://rocket--kb-dev.s3-us-west-1.amazonaws.com/#{file_names[i]}")
    article.cover_img.attach(io: file, filename: file_names[i])
end